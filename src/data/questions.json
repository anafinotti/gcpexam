[
  {
    "id": 1,
    "source": "examtopics",
    "question": "You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?",
    "options": {
      "A": "Deployment Manager",
      "B": "Cloud Composer",
      "C": "Managed Instance Group",
      "D": "Unmanaged Instance Group"
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 3Â years ago\nSelected Answer: A\nThe correct answer is Option A - Deployment Manager. Deployment Manager is a configuration management tool that allows you to define and deploy a set of resources, including Compute Engine VMs, in a declarative manner. You can use it to specify the exact specifications of your VMs in a configuration file, and Deployment Manager will create and manage those VMs for you. Deployment Manager is recommended by Google as a way to automate and manage the deployment of resources on the Google Cloud Platform.\nhttps://cloud.google.com/deployment-manager/docs/ jogoldberg 4Â months, 2Â weeks ago\nThe correct answer (Infrastructure Manager) is missing.\nDeployment Manager USED to be the correct answer, however Deployment Manager goes End of Life on 31-Dec-2025, and will be fully deprecated on 31-Mar-2026."
      },
      {
        "index": 2,
        "text": "Anonymous: shreymath9999 Highly Voted 2Â years, 2Â months ago\nThe question says - \"Dynamic way of provision VMs on Compute Engine\" which both the Managed Instance group and the Deployment Manager does, but for different purposes. So here the answer can be more of Deployment Manager as it covers the scope of question, while the Managed Instance group can also dynamically provision VMs based on configuration file but only for auto-healing or horizontal scaling purposes, the answer could have been C if the question was asked as \"Dynamic way of provision VMs on Compute Engine for horizontal scaling/auto healing\""
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: A\nDeployment manager tale care of defining and deploying the resources."
      },
      {
        "index": 4,
        "text": "Anonymous: URYC 7Â months, 1Â week ago\nSelected Answer: A\nThe correct answer is Option A - Deployment Manager. jogoldberg 4Â months, 2Â weeks ago\nThe correct answer (Infrastructure Manager) is missing.\nDeployment Manager USED to be the correct answer, however Deployment Manager goes End of Life on 31-Dec-2025, and will be fully deprecated on 31-Mar-2026."
      },
      {
        "index": 5,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA. As its google recommended best practice - although will be deprecated December 2025 and you will need to use Infrastructure manager instead. You specify a dedicated configuration yaml file. It outlines in a declarative way the GCP resources you need. You can preview before you deploy to check correct."
      },
      {
        "index": 6,
        "text": "Anonymous: harsh5kalsait 1Â year, 5Â months ago\nA - due to provisioning VMs on Compute Engine - exact specifications will be in a dedicated configuration file."
      },
      {
        "index": 7,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: C\nReference:\nhttps://cloud.google.com/compute/docs/instances/"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 9,
        "text": "Anonymous: guicane 2Â years, 2Â months ago\nSelected Answer: A\nC makes no sense, it's A Nikki2424 1Â year, 8Â months ago\nWhy does C make no sense?"
      },
      {
        "index": 10,
        "text": "Anonymous: Evan7557 2Â years, 3Â months ago\nA deployment manager"
      }
    ]
  },
  {
    "id": 2,
    "source": "examtopics",
    "question": "You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do?",
    "options": {
      "A": "Use kubectl app deploy <dockerfilename>.",
      "B": "Use gcloud app deploy <dockerfilename>.",
      "C": "Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.",
      "D": "Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 9Â months ago\nC is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 3Â years ago\nSelected Answer: C\nThe correct answer is Option C. To deploy a Docker container on Kubernetes Engine, you should first create a Docker image from the Dockerfile and push it to Container Registry, which is a fully-managed Docker container registry that makes it easy for you to store, manage, and deploy Docker container images. Then, you can create a Deployment YAML file that specifies the image to use and other desired deployment options, and use the kubectl command-line tool to create the deployment based on the YAML file.\nOption A is incorrect because kubectl app deploy is not a valid command.\nOption B is incorrect because gcloud app deploy is used to deploy applications to App Engine, not Kubernetes Engine.\nOption D is incorrect because it involves storing the image in Cloud Storage rather than Container Registry.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/deploying-a-container ast3citos 2Â years, 11Â months ago\nThe link you provided has stopped working :_( jogoldberg 4Â months, 2Â weeks ago\nContainer Registry is deprecated. Effective March 18, 2025, Container Registry is shut down and writing images to Container Registry is unavailable. The solution is now Artifact Registry.\nhttps://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: C\nFirst you would require to create a docker image from DockerFile then push into registry. After that you can create deployment file which refer to that specific image."
      },
      {
        "index": 4,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: C\nC as you need to create an container image from the dockerfile first and put in artifact registry before you can access to deploy to your cluster."
      },
      {
        "index": 5,
        "text": "Anonymous: Cloudmoh 11Â months, 1Â week ago\nSelected Answer: C\nThe file image should be created from the Dockerfile and pushed to the Registry and from there it should be deployed once it's YAML structure is in place."
      },
      {
        "index": 6,
        "text": "Anonymous: madzzzz 1Â year, 3Â months ago\nSelected Answer: C\nC is correct jogoldberg 4Â months, 2Â weeks ago\nIt isn't, though. None of the answers are correct. C is just the least incorrect :D\nContainer Registry is deprecated. Effective March 18, 2025, Container Registry is shut down and writing images to Container Registry is unavailable. The solution is now Artifact Registry.\nhttps://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr"
      },
      {
        "index": 7,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: C\nReference -\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe answer is C."
      },
      {
        "index": 9,
        "text": "Anonymous: Evan7557 2Â years, 3Â months ago\nC is Correct Answer"
      },
      {
        "index": 10,
        "text": "Anonymous: dookiecloud 2Â years, 4Â months ago\nC is correct.\nWe need to build docker image then push to Container Registry and setup yaml deployment in GKE to pull our registry image."
      }
    ]
  },
  {
    "id": 3,
    "source": "examtopics",
    "question": "Your development team needs a new Jenkins server for their project. You need to deploy the server using the fewest steps possible. What should you do?",
    "options": {
      "A": "Download and deploy the Jenkins Java WAR to App Engine Standard.",
      "B": "Create a new Compute Engine instance and install Jenkins through the command line interface.",
      "C": "Create a Kubernetes cluster on Compute Engine and create a deployment with the Jenkins Docker image.",
      "D": "Use GCP Marketplace to launch the Jenkins solution."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 9Â months ago\nD is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 3Â years ago\nSelected Answer: D\nThe correct answer is Option D. By using GCP Marketplace to launch the Jenkins solution, you can quickly deploy a Jenkins server with minimal steps.\nOption A involves deploying the Jenkins Java WAR to App Engine Standard, which requires more steps and may not be suitable for your requirements.\nOption B involves creating a new Compute Engine instance and manually installing Jenkins, which also requires more steps.\nOption C involves creating a Kubernetes cluster and creating a deployment with the Jenkins Docker image, which again involves more steps and may not be the most efficient solution."
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: D\nD is correct. However for most coorporate users this will be disabled. As will be dockerhub which wil lhave a containerised Jenkins server. So for coorp users you will have to install yourself on a fresh instance."
      },
      {
        "index": 4,
        "text": "Anonymous: madzzzz 1Â year, 3Â months ago\nSelected Answer: B\nmaybe b"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe answer is D."
      },
      {
        "index": 6,
        "text": "Anonymous: Evan7557 2Â years, 3Â months ago\nD Answer"
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 2Â years, 3Â months ago\nThe correct answer is D\nThis is the fastest and easiest way to deploy a Jenkins server on GCP. GCP Marketplace provides pre-configured and pre-packaged applications that you can launch with just a few clicks.\nTo deploy Jenkins from GCP Marketplace, follow these steps:\n->Go to the GCP Marketplace.\n->Search for \"Jenkins\".\n->Click the \"Jenkins\" app listing.\n->Click the \"Launch\" button.\n->Configure the Jenkins deployment options, such as the machine type and region.\n->Click the \"Deploy\" button.\nGCP Marketplace will deploy a Jenkins server to your GCP project. Once the deployment is complete, you can access the Jenkins web UI at the URL provided in the deployment details."
      },
      {
        "index": 8,
        "text": "Anonymous: elviskimutai 2Â years, 4Â months ago\nD. Use GCP Marketplace to launch the Jenkins solution: This is the most straightforward and efficient method to deploy Jenkins on Google Cloud Platform"
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nD is the correct answer , fewest steps possible"
      },
      {
        "index": 10,
        "text": "Anonymous: vivekvj 2Â years, 9Â months ago\nSelected Answer: D\nD IS CORRECT"
      }
    ]
  },
  {
    "id": 4,
    "source": "examtopics",
    "question": "You need to update a deployment in Deployment Manager without any resource downtime in the deployment. Which command should you use?",
    "options": {
      "A": "gcloud deployment-manager deployments create --config <deployment-config-path>",
      "B": "gcloud deployment-manager deployments update --config <deployment-config-path>",
      "C": "gcloud deployment-manager resources create --config <deployment-config-path>",
      "D": "gcloud deployment-manager resources update --config <deployment-config-path>"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Agents89 Highly Voted 4Â years, 9Â months ago\nB is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 2Â years ago\nSelected Answer: B\nThe correct answer is Option B: `gcloud deployment-manager deployments update --config <deployment-config-path>`. This command updates an existing deployment with the configuration specified in the `deployment-config-path` file. It allows you to make changes to the deployment without any downtime in the resources.\nhttps://cloud.google.com/sdk/gcloud/reference/deployment-manager/"
      },
      {
        "index": 3,
        "text": "Anonymous: jogoldberg Most Recent 4Â months, 2Â weeks ago\nSelected Answer: B\nThis question should be replaced with one about Infra Manager. Deployment manager is going EOL on 31-December-2025 and Deprecated on 31-March-2026."
      },
      {
        "index": 4,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: B\nB. B deployment manager uses a config file to update a change to existing infrastructure - as the question asked to update. A. would create a new deployment.C and D resources keywork only lists and describes cannot use to update as required."
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is B"
      },
      {
        "index": 6,
        "text": "Anonymous: Evan7557 1Â year, 3Â months ago\nAnswer B is Correct"
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: B\nThe correct answer is B.\nThis command updates an existing deployment with the configuration specified in the deployment-config-path file. Deployment Manager automatically determines whether resources need to be created, updated, or deleted in order to apply the changes.\nHere is an example of how to use the gcloud deployment-manager deployments update command to update a deployment without any resource downtime:\ngcloud deployment-manager deployments update my-deployment --config deployment.yaml\nThis command will update the my-deployment deployment with the configuration specified in the deployment.yaml file. Deployment Manager will automatically determine whether resources need to be created, updated, or deleted in order to apply the changes."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nb is the answer, clearly"
      },
      {
        "index": 9,
        "text": "Anonymous: sakdip66 1Â year, 9Â months ago\nI agree B is the ðŸ”‘"
      },
      {
        "index": 10,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: B\ngcloud deployment-manager deployments update --config is correct"
      }
    ]
  },
  {
    "id": 5,
    "source": "examtopics",
    "question": "You need to run an important query in BigQuery but expect it to return a lot of records. You want to find out how much it will cost to run the query. You are using on-demand pricing. What should you do?",
    "options": {
      "A": "Arrange to switch to Flat-Rate pricing for this query, then move back to on-demand.",
      "B": "Use the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator.",
      "C": "Use the command line to run a dry run query to estimate the number of bytes returned. Then convert that bytes estimate to dollars using the Pricing Calculator.",
      "D": "Run a select count (*) to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 4Â months ago\nCorrect Answers is (B):\nOn-demand pricing\nUnder on-demand pricing, BigQuery charges for queries by using one metric: the number of bytes processed (also referred to as bytes read). You are charged for the number of bytes processed whether the data is stored in BigQuery or in an external data source such as Cloud Storage, Drive, or Cloud Bigtable. On-demand pricing is based solely on usage.\nhttps://cloud.google.com/bigquery/pricing#on_demand_pricing"
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 4Â years, 8Â months ago\nB is Correct"
      },
      {
        "index": 3,
        "text": "Anonymous: yomi95 Most Recent 1Â year ago\nSelected Answer: B\nA dry run query in BigQuery allows you to estimate the amount of data scanned by your query without actually running it. This is especially useful for queries expected to process a large amount of data.\nYou can use the estimated bytes read to calculate the cost using BigQuery's on-demand pricing model, where costs are based on the amount of data processed"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is B"
      },
      {
        "index": 5,
        "text": "Anonymous: Evan7557 1Â year, 3Â months ago\nAnswer B"
      },
      {
        "index": 6,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: B\nThe correct answer is B.\nThis option is the most accurate way to estimate the cost of your query, because it takes into account the actual number of bytes that will be processed by BigQuery.\nHere is an example of how to run a dry run query in BigQuery:\nbq dry run --query \"SELECT * FROM <dataset>.<table> WHERE <condition>\"\nThis command will print the estimated number of bytes that will be processed by BigQuery. You can then use the Pricing Calculator to convert this bytes estimate to dollars.\nOnce you have estimated the cost of your query, you can decide whether or not to proceed with running it. If you decide to proceed, you can monitor the cost of your query using the BigQuery Monitoring Console."
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nyes B is the correct answer, biq query charges for queries by using one metric"
      },
      {
        "index": 8,
        "text": "Anonymous: certified28 1Â year, 7Â months ago\nSelected Answer: B\nB is Correct"
      },
      {
        "index": 9,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: B\nCalculation should be on bytes read"
      },
      {
        "index": 10,
        "text": "Anonymous: BlueJay20 1Â year, 11Â months ago\nSelected Answer: B\nCalculation on bytes read."
      }
    ]
  },
  {
    "id": 6,
    "source": "examtopics",
    "question": "You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?",
    "options": {
      "A": "Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.",
      "B": "Create an instance template, and use the template in a managed instance group with autoscaling configured.",
      "C": "Create an instance template, and use the template in a managed instance group that scales up and down based on the time of day.",
      "D": "Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: mohdafiuddin Highly Voted 5Â years ago\nI'll take a simple and logical approach for answering this.\nLet's first break down the question into key requirements -"
      },
      {
        "index": 1,
        "text": "automatically scale the application based on underlying infrastructure CPU usage."
      },
      {
        "index": 2,
        "text": "use virtual machines directly.\nA. Not feasible because VMs are not used directly here.\nB. This is the correct answer.\nC. Time of Day... Easy elimination because this does not scale on CPU usage and time of day is mentioned NOWHERE.\nD. Third Party Tools.... Nobody would use GCP if they needed third party tools to do something as simple as scaling based on CPU usage. all popular cloud providers have native solutions for this including GCP. kopper2019 4Â years, 9Â months ago\nand also D is out because why would I use a third party tool when is a GCP exam RMO000 4Â years, 3Â months ago\nIf the resource/solution is not available. It's a possibility."
      },
      {
        "index": 2,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\ncorrect is B as you have to use VM instances directly."
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: B\nB. As instance Template Defines the VM configuration (machine type, startup script, disk, etc.). Managed Instance Group: Uses the template to create and manage multiple VM instances. Supports autoscaling based on CPU usage or other metrics natively. Provides rolling updates and self-healing VMs automatically.Fast scaling solution using built-in Google Cloud features."
      },
      {
        "index": 4,
        "text": "Anonymous: Cloudmoh 11Â months, 1Â week ago\nSelected Answer: B\nBased on the logic behind this question right, A managed instance group (MIG) with autoscaling is the most operationally efficient way to scale virtual machine instances directly based on CPU usage"
      },
      {
        "index": 5,
        "text": "Anonymous: warbon 12Â months ago\nSelected Answer: B\nA managed instance group (MIG) with autoscaling is the most operationally efficient way to scale virtual machine instances directly based on CPU usage. The instance template defines the VM configuration, and the autoscaling feature ensures that the application scales quickly and efficiently based on resource utilization, meeting the organization's requirements. Kubernetes and third-party tools are unnecessary for this scenario, and scaling based on time is less dynamic and not tied to actual usage."
      },
      {
        "index": 6,
        "text": "Anonymous: sh00001 1Â year, 6Â months ago\nThe correct option is B as it manges the VMs"
      },
      {
        "index": 7,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is B"
      },
      {
        "index": 8,
        "text": "Anonymous: tlopsm 2Â years, 2Â months ago\nB is the answers you can autoscale based on CPU Usage. D is wrong as it suggests that the triggering is time of day"
      }
    ]
  },
  {
    "id": 7,
    "source": "examtopics",
    "question": "You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?",
    "options": {
      "A": "Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.",
      "B": "Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.",
      "C": "Export your transactions to a local file, and perform analysis with a desktop tool.",
      "D": "Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: mohdafiuddin Highly Voted 4Â years, 6Â months ago\nSolving this by first eliminating the options that don't suit us. By breaking down the question into the key requirements-"
      },
      {
        "index": 1,
        "text": "Analyzing Google Cloud Platform service costs from three separate projects."
      },
      {
        "index": 2,
        "text": "Using standard query syntax. -> (Relational data and SQL)\nA. 'Cloud Storage bucket'........'Cloud Bigtable'. Not feasible, mainly because cloud BigTable is not good for Structured Data (or Relational Data on which we can run SQL queries as per the question's requirements). BigTable is better suited for Semi Structured data and NoSQL data.\nB. 'Cloud Storage bucket'.....'Google Sheets'. Not Feasible because there is no use of SQL in this option, which is one of the requirements.\nC. Local file, external tools... this is automatically eliminated because the operation we need is simple, and there has to be a GCP native solution for this. We shouldn't need to rely on going out of the cloud for such a simple thing.\nD. 'BigQuery'.....'SQL queries' -> This is the right answer. ryumada 2Â years, 11Â months ago\nCloud billing data can only be exported to a JSON local file and to Bigquery. So, using Cloud Storage to export cloud billing data is not possible to do.\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "index": 2,
        "text": "Anonymous: cesar7816 Highly Voted 5Â years, 4Â months ago\nAgreed, BigQuery yurstev 4Â years, 6Â months ago\nthe key is standard query syntax"
      },
      {
        "index": 3,
        "text": "Anonymous: Raghav2650 Most Recent 10Â months ago\nSelected Answer: D\nYou need a scalable and efficient way to query cost data across multiple projects.\nBigQuery is the best choice because it supports large-scale data analysis using SQL.\nBigQuery allows you to run time-based SQL queries to analyze trends and forecast future costs.\nou can write queries using window functions (e.g., DATE_TRUNC(), INTERVAL) to break costs down by day or month."
      },
      {
        "index": 4,
        "text": "Anonymous: SAMBIT 1Â year, 5Â months ago\nTry it here: https://lookerstudio.google.com/reporting/0B7GT7ZlyzUmCZHFhNDlKVENHYmc/page/tLtE"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is D"
      },
      {
        "index": 6,
        "text": "Anonymous: Evan7557 1Â year, 9Â months ago\nD is the Answer"
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 10Â months ago\nSelected Answer: D\nThe correct answer is D.\nBigQuery is a fully-managed, petabyte-scale analytics data warehouse that enables businesses to analyze all their data very quickly. It is also a good choice for analyzing cost data because it can handle large amounts of data and can perform complex queries quickly.\nTime window-based SQL queries allow you to analyze data over a specific time period. For example, you could write a query to calculate the total cost of a particular service for each day of the month.\nHere is an example of a BigQuery query that you could use to calculate the total cost of a particular service for each day of the month:\nsql\nSELECT\ncost,\nDATE(usage_start_time) AS date\nFROM\n`[PROJECT_ID].billing.dataset`\nWHERE\nservice_id = 'YOUR_SERVICE_ID'\nGROUP BY\ndate\nORDER BY\ndate\nThis query will return a table with two columns: `cost` and `date`. The `cost` column will contain the total cost of the service for each day of the month. The `date` column will contain the date for each row."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nyes D, is the correct answer, because we only use bigquery for semi or structured data"
      }
    ]
  },
  {
    "id": 8,
    "source": "examtopics",
    "question": "You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?",
    "options": {
      "A": "Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365 ×’â‚¬\" 90)",
      "B": "Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.",
      "C": "Use gsutil rewrite and set the Delete action to 275 days (365-90).",
      "D": "Use gsutil rewrite and set the Delete action to 365 days."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Sammigbo Highly Voted 5Â years, 7Â months ago\nAnswer is B. There should be no reason to recalculate the time needed to delete after a year. JKRowlings 5Â years, 1Â month ago\nThe correct ans is A. yvinisiupacuando 4Â years, 8Â months ago\nRight answer is clearly B, \"A\" does not make any sense."
      },
      {
        "index": 2,
        "text": "Anonymous: cloudenthu01 Highly Voted 5Â years, 6Â months ago\nCorrect is B.\nYou only re-calculate expiry date when objects are re-written using re-write option to another storage class in which case creation date is rest.\nBut in this case objects is moveed to Coldline class after 90 days and then we want to delete the object after 365 days. T_T_M 5Â years, 4Â months ago\nYou can change the storage class of an existing object either by rewriting the object or by using Object Lifecycle Management...Since Object Life cycle management was used there was no need to recalculate the expiration date and delete action still remains 365 days.\nhttps://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: B\nB. Its basic operation of cloud storage object lifecycle management. Not A as actions are run from measures from object creation time - not time duration after another action happened."
      },
      {
        "index": 4,
        "text": "Anonymous: vaclavbenes1 11Â months, 2Â weeks ago\nSelected Answer: B\nage\nThe age condition is satisfied when a resource reaches the specified age (in days). Age is measured from the resource's creation time.\nhttps://cloud.google.com/storage/docs/lifecycle#age"
      },
      {
        "index": 5,
        "text": "Anonymous: devanshgoyal12 1Â year ago\nSelected Answer: D\nWhy answer is not D"
      },
      {
        "index": 6,
        "text": "Anonymous: peddyua 1Â year ago\nSelected Answer: B\n{\n\"rule\": [\n{\n\"action\": { \"type\": \"SetStorageClass\", \"storageClass\": \"COLDLINE\" },\n\"condition\": { \"age\": 90 }\n},\n{\n\"action\": { \"type\": \"Delete\" },\n\"condition\": { \"age\": 365 }\n}\n]\n}"
      },
      {
        "index": 7,
        "text": "Anonymous: narop 1Â year ago\nSelected Answer: A\nCloud Storage Object Lifecycle Management allows you to define conditions and actions for objects based on their age, creation time, or other attributes.\nTo meet the requirements:\nMove to Coldline after 90 days: This is achieved by setting a SetStorageClass action with an Age condition of 90 days.\nDelete after 1 year (365 days): Since the object would have been moved to Coldline after 90 days, the remaining time for deletion is 365 - 90 = 275 days. Set a Delete action with an Age condition of 275 days."
      },
      {
        "index": 8,
        "text": "Anonymous: user263263 1Â year, 1Â month ago\nSelected Answer: B\nA is not correct. age is defined based on creation time. SetStorageClass only affects the modification time, not the creation time. So the 90 days are not relevant for the deletion rule."
      },
      {
        "index": 9,
        "text": "Anonymous: nubelukita45852 1Â year, 4Â months ago\nSelected Answer: B\nLa gestiÃ³n del ciclo de vida de objetos en Google Cloud Storage permite automatizar el cambio de la clase de almacenamiento y la eliminaciÃ³n de los objetos en funciÃ³n de su antigÃ¼edad. Para este caso:\nDespuÃ©s de 90 dÃ­as, se debe cambiar la clase de almacenamiento a Coldline, lo que se hace usando la acciÃ³n SetStorageClass.\nLuego, despuÃ©s de 365 dÃ­as (1 aÃ±o), los objetos deben ser eliminados usando la acciÃ³n Delete.\nA es incorrecta porque sugiere eliminar los objetos a los 275 dÃ­as, lo cual no coincide con el requisito de eliminar los videos despuÃ©s de un aÃ±o.\nC y D son incorrectas porque gsutil rewrite no es la herramienta correcta para gestionar polÃ­ticas de ciclo de vida."
      },
      {
        "index": 10,
        "text": "Anonymous: JackSkeletonCoder 1Â year, 4Â months ago\nSelected Answer: B\noption A can be misguiding but you don't have to specify 275 days since the rule would be implemented based on the creation of object not after the effect of the previous rule. Hence option B"
      }
    ]
  },
  {
    "id": 9,
    "source": "examtopics",
    "question": "You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do?",
    "options": {
      "A": "When creating the VM via the web console, specify the service account under the 'Identity and API Access' section.",
      "B": "Download a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service- account.",
      "C": "Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine- service-account.",
      "D": "Download a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service- account.json."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 8Â months ago\nA is correct ready2rock 4Â years, 7Â months ago\nHow can this be? It says you HAVE a VM, meaning it's already created. A cannot be the solution. jiniguez 4Â years, 1Â month ago\nAs the comment says:\n\"To change an instance's service account and access scopes, the instance must be temporarily stopped ... After changing the service account or access scopes, remember to restart the instance.\" So we can stop the instance, change the service account, then start it up again. boof 4Â years, 3Â months ago\nA seems legit, the answer is worded poorly but is the most correct.\n---\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes\n---\n\"To change an instance's service account and access scopes, the instance must be temporarily stopped ... After changing the service account or access scopes, remember to restart the instance.\" So we can stop the instance, change the service account, then start it up again. ashrafh 4Â years, 5Â months ago\nI vote A\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances\nChanging the service account and access scopes for an instance\nIf you want to run the VM as a different identity, or you determine that the instance needs a different set of scopes to call the required APIs, you can change the service account and the access scopes of an existing instance. For example, you can change access scopes to grant access to a new API, or change an instance so that it runs as a service account that you created, instead of the Compute Engine default service account. However, Google recommends that you use the fine-grained IAM policies instead of relying on access scopes to control resource access for the service account.\nTo change an instance's service account and access scopes, the instance must be temporarily stopped. To stop your instance, read the documentation for Stopping an instance. After changing the service account or access scopes, remember to restart the instance. Use one of the following methods to the change service account or access scopes of the stopped instance.\nHope this helps :)"
      },
      {
        "index": 2,
        "text": "Anonymous: jabrrJ68w02ond1 Highly Voted 4Â years, 1Â month ago\nEither the question or the answers are wrong. The question says that we HAVE a Linux VM, so we should strike all the answers that include \"when creating the VM..\" - on the other hand, adding JSON Tokens to VM metadata is terrible because it's readable in clear-text for everyone. So, what do we need to do here?"
      },
      {
        "index": 3,
        "text": "Anonymous: Joseph_Covaro Most Recent 11Â months, 1Â week ago\nSelected Answer: A\nWhile A doesn't really work, given that the VM is already created, all the other answers are absolutely unacceptable. You should never store private keys as raw text, such as .json."
      },
      {
        "index": 4,
        "text": "Anonymous: warbon 12Â months ago\nSelected Answer: A\nThe correct way to assign a specific service account to a VM is by specifying it during VM creation in the 'Identity and API Access' section. This ensures the VM uses the specified service account for authentication when accessing Google Cloud services. The other options, which involve manually downloading and managing JSON keys, are not recommended due to security risks and Google's best practices for managing service accounts."
      },
      {
        "index": 5,
        "text": "Anonymous: peddyua 1Â year ago\nSelected Answer: C\nseems like A. doesn't fit. As it's ONLY valid only during the VM creation process. It allows you to assign a custom service account instead of the default one.\nHowever, if the VM is already created, you cannot use this method.\nC. correct answer\ngcloud compute instances stop VM_NAME --zone ZONE\ngcloud compute instances set-service-account VM_NAME \\\n--zone ZONE \\\n--service-account SERVICE_ACCOUNT_EMAIL\ngcloud compute instances start VM_NAME --zone ZONE"
      },
      {
        "index": 6,
        "text": "Anonymous: Sanjai_I 1Â year ago\nSelected Answer: C\nRecommended approach: Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-service-account.\nThis will allow the VM to use the specified service account for accessing Cloud SQL, without relying on the default Compute Engine service account.\nThe other options are not suitable:\nOption A is for specifying a service account when creating a VM via the web console, but it's not applicable for an existing VM.\nOption B is incorrect, as the compute-engine-service-account key is not a valid key for storing a service account JSON key file.\nOption D is not the correct approach, as saving the JSON key file to ~/.gcloud/compute-engine-service-account.json is not a standard way to configure the service account for a VM."
      },
      {
        "index": 7,
        "text": "Anonymous: errorfetch 1Â year, 4Â months ago\nSelected Answer: A\nA is correct because the easiest solution is specifying the service account while creating the vm. if you dont specify the default compute engine account is chosen."
      },
      {
        "index": 8,
        "text": "Anonymous: nubelukita45852 1Â year, 4Â months ago\nSelected Answer: A\nPara asegurarse de que la mÃ¡quina virtual utilice una cuenta de servicio especÃ­fica en lugar de la predeterminada de Compute Engine, debes especificar la cuenta de servicio correcta al crear la mÃ¡quina virtual. En la secciÃ³n \"Identidad y acceso a API\", puedes seleccionar la cuenta de servicio adecuada para garantizar que las solicitudes y accesos a otros servicios, como Cloud SQL, se realicen usando los permisos asociados a esa cuenta de servicio.\nB, C y D implican el uso de claves privadas JSON, lo cual no es una prÃ¡ctica recomendada debido a los riesgos de seguridad asociados al manejo de claves manualmente."
      },
      {
        "index": 9,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: A\nA is correct."
      },
      {
        "index": 10,
        "text": "Anonymous: hmd2910 1Â year, 7Â months ago\nThe question implies that a Linux VM already exists and needs to be configured to use a specific service account instead of the default Compute Engine service account. This is crucial because it eliminates option A, which focuses on setting the service account during VM creation.\nWhy Option C is Correct:\nCustom Metadata : Custom metadata is designed for VM-specific configuration. It's the ideal place to store service account credentials.\ncompute-engine-service-account : This is the specific metadata key used to tell the VM which service account to use.\nJSON Private Key : This is the standard format for storing service account credentials."
      }
    ]
  },
  {
    "id": 10,
    "source": "examtopics",
    "question": "You created an instance of SQL Server 2017 on Compute Engine to test features in the new version. You want to connect to this instance using the fewest number of steps. What should you do?",
    "options": {
      "A": "Install a RDP client on your desktop. Verify that a firewall rule for port 3389 exists.",
      "B": "Install a RDP client in your desktop. Set a Windows username and password in the GCP Console. Use the credentials to log in to the instance.",
      "C": "Set a Windows password in the GCP Console. Verify that a firewall rule for port 22 exists. Click the RDP button in the GCP Console and supply the credentials to log in.",
      "D": "Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: vnxt Highly Voted 5Â years, 8Â months ago\nI would say B is correct. RDP is enabled by default when you crate a Windows instance (no need to chek on it). Just make sure you install an RDP client ( chrome ext or RDP) and set windows password. ryumada 3Â years, 5Â months ago\nThe firewall rule for port 3389 is created by default if you create windows server on Compute Engine. So, no need to verify it.\nhttps://cloud.google.com/compute/docs/instances/connecting-to-windows#before-you-begin eledu1985 3Â years, 2Â months ago\nIn the link you provided the first step is to verify firewall rule was created, even if it is the default option!, so D is the most accurate even by the link. bubidubi 1Â year, 11Â months ago\nI disagree. The Q says that you have just created this VM, which means the FW rules for RDP has also been created. I believe the documentations says it a bit wonky as it is something to check if things aren't working after you've done everything right, as someone could have deleted that rule if this server has been up for a while, rather than having to check this right after you've made a new VM.\nI'd say B is better answer than D. Gregwaw 2Â years, 4Â months ago\nYou need firewall ingress rule for port 3398 in your VPC. It is not created by default. pas77 4Â years, 6Â months ago\nObviously, B is not the answer because you have to install an RDP client which is an extra step. D is the answer because you can connect directly using the RDP button in the GCP console. jabrrJ68w02ond1 4Â years, 1Â month ago\nTested it myself. At least on my Machines, I was asked to First install a RDP Client. UtsavDM 4Â years, 4Â months ago\nNo, we can't connect using RDP directly in the GCP console. When we click on it, it asks us to install RDP client. So ultimately, B is more accurate."
      },
      {
        "index": 2,
        "text": "Anonymous: ankit89 Highly Voted 5Â years, 8Â months ago\nD seems more correct obeythefist 3Â years, 10Â months ago\nI tested this on on Compute Engine today by deploying a new instance. D is not correct. When you click the RDP button, you are asked to install a client or use the Windows RDP client if you are running Windows. There is no option to enter credentials or get an RDP session through the web interface."
      },
      {
        "index": 3,
        "text": "Anonymous: Mobadarin Most Recent 9Â months, 3Â weeks ago\nSelected Answer: D\nyou need firewall ingress rule for port 3398 in your VPC. It is not created by default."
      },
      {
        "index": 4,
        "text": "Anonymous: d5dd70c 1Â year ago\nSelected Answer: D\nD looks good"
      },
      {
        "index": 5,
        "text": "Anonymous: Arghadeep 1Â year ago\nSelected Answer: D\nThere is no additional need of installing rdp client which makes and b incorrect, you can directly log in from console. And even the firewall rule although it's default should be verified before establishing connections. D is the absolute answer"
      },
      {
        "index": 6,
        "text": "Anonymous: modaknarayan 1Â year, 1Â month ago\nSelected Answer: D\nD. Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in.\nExplanation: For connecting to a SQL Server 2017 instance on Google Cloud Compute Engine using the fewest steps, you can use the RDP (Remote Desktop Protocol) connection, which is supported by default for Windows instances. The steps involved include:\nSetting a Windows username and password: You need to specify these credentials in the GCP Console to access the instance via RDP.\nVerifying the firewall rule for port 3389: Port 3389 is the default RDP port, so you need to ensure the appropriate firewall rule is in place.\nClicking the RDP button in the GCP Console: Once these settings are configured, you can use the RDP button in the console to connect to the instance. You'll then supply the credentials you set in the console."
      },
      {
        "index": 7,
        "text": "Anonymous: zAbuQasen 1Â year, 1Â month ago\nSelected Answer: D\nOption D correctly includes:\nSetting the Windows username and password in the GCP Console.\nVerifying the firewall rule for port 3389.\nUsing the \"RDP\" button in the GCP Console to log in, ensuring the fewest number of steps."
      },
      {
        "index": 8,
        "text": "Anonymous: mnasruul 1Â year, 1Â month ago\nSelected Answer: B\nB is Correct, because not option RDP button at Console GCP https://cloud.google.com/compute/docs/instances/connecting-to-windows#other mnasruul 1Â year, 1Â month ago\nCurrently GCP support RDP Button at VM GCP https://www.google.com/search?sca_esv=636238c710bc0fc7&sxsrf=ADLYWIIE4h1x6wxfcCsUII3wGOJeLC5P5g:1733916710025&q=RDP+button+in+the+GCP+Console&udm=2&fbs=AEQNm0D7Nkus73mDfRiM-qGSEUWUGRlQvLHQV1vCo9BTpxTWdXf_Rth04xNqa7aHoD5yv9G64fmPfTi2VEH1A-4DF7EBxIPcUtslRrL9UyKNDbAjJZ4ogfz78wrk6C3LLbEGPWofK58Izae8SKteGXNQrtJbqmcwjyEKkFlr9j0j3HzRXs7H1iRv67Sx8M7um58mJG883G2H7e0gr16XyhQdWNKiT3YFzA&sa=X&ved=2ahUKEwj00siMz5-KAxUWRmcHHfmvOtYQtKgLegQIFhAB&biw=1920&bih=940&dpr=1.5#vhid=tU0HqiUlxKFSYM&vssid=mosaic"
      },
      {
        "index": 9,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nSelected Answer: D\nD is correct because B doesn't mention *using the RDP button*, making the task less efficient. B assumes that you will be forced to implement the second task (install RDP)"
      },
      {
        "index": 10,
        "text": "Anonymous: Makar 1Â year, 2Â months ago\nD is correct\nFor connecting to a SQL Server instance (which is running on a Windows Server on Google Compute Engine), using Remote Desktop Protocol (RDP) is the standard method."
      }
    ]
  },
  {
    "id": 11,
    "source": "examtopics",
    "question": "You need to host an application on a Compute Engine instance in a project shared with other teams. You want to prevent the other teams from accidentally causing downtime on that application. Which feature should you use?",
    "options": {
      "A": "Use a Shielded VM.",
      "B": "Use a Preemptible VM.",
      "C": "Use a sole-tenant node.",
      "D": "Enable deletion protection on the instance."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 4Â months ago\nCorrect Answer is (D):\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation.\nAs part of your workload, there might be certain VM instances that are critical to running your application or services, such as an instance running a SQL server, a server used as a license manager, and so on. These VM instances might need to stay running indefinitely so you need a way to protect these VMs from being deleted.\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletionProtection flag, the request fails. Only a user that has been granted a role with compute.instances.create permission can reset the flag to allow the resource to be deleted.\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      },
      {
        "index": 2,
        "text": "Anonymous: professor Highly Voted 5Â years, 6Â months ago\nAgree with D\nYou can enabale Termination protection"
      },
      {
        "index": 3,
        "text": "Anonymous: iooj Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nIt does't say that someone can delete it and we should prevent deletion."
      },
      {
        "index": 4,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: D\nThe correct answer is D."
      },
      {
        "index": 5,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: C\nC. Question says accidental downtime - this could be caused by many other reasons other than just straight deleting things.\n\"Use a sole-tenant node\" allows you to have dedicated hardware for your VM instances, providing isolation from other workloads. This isolation can help prevent other teams' actions from impacting your application's availability."
      },
      {
        "index": 6,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: D\n(Use a sole-tenant node) is a method to ensure that your VMs run on a physical host dedicated to your project (related to licences etc), but it doesn't specifically prevent accidental downtime caused by other teams in a shared environment."
      },
      {
        "index": 7,
        "text": "Anonymous: JB28 2Â years ago\nTo prevent accidental deletion of a Compute Engine instance, you can enable deletion protection on the instance. This feature prevents the instance from being deleted by any user until deletion protection is disabled.\nSo, the correct answer is: D. Enable deletion protection on the instance."
      },
      {
        "index": 8,
        "text": "Anonymous: kelliot 2Â years, 1Â month ago\nSelected Answer: D\nfor me, is D"
      },
      {
        "index": 9,
        "text": "Anonymous: NHarshada12345 2Â years, 1Â month ago\nSelected Answer: D\nANS is D:\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletion\nProtection flag, the request fails. Only a user that has been granted a role with compute."
      },
      {
        "index": 10,
        "text": "Anonymous: EmW9117 2Â years, 2Â months ago\nSelected Answer: D\nC is incorrect as sole-tenant is project-based. The other users in the same project can still cause accidentally deletion of the VM even if using a sole-tenant node."
      }
    ]
  },
  {
    "id": 12,
    "source": "examtopics",
    "question": "Your organization needs to grant users access to query datasets in BigQuery but prevent them from accidentally deleting the datasets. You want a solution that follows Google-recommended practices. What should you do?",
    "options": {
      "A": "Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner.",
      "B": "Add users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner.",
      "C": "Create a custom role by removing delete permissions, and add users to that role only.",
      "D": "Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: someoneinthecloud Highly Voted 5Â years, 6Â months ago\nI believe the key part is the \"following Google Best Practices\" phrase.\nA - Works, but doesn't follow GCP best practices\nB - Doesn't work as the role grants permission to delete datasets\nC - Works, but is more complicated than A and doesn't follow Google best practices\nD - Correct, more complicated than A, but it follows Google Best Practices. TAvenger 4Â years, 10Â months ago\nRead description carefully \"prevent from accidentally deleting the datasets\". Not tables, datasets! option B does not allow to delete datesets either.\nCheck dateset permissions in the roles/bigquery.dataEditor:\nbigquery.datasets.create\nbigquery.datasets.get\nbigquery.datasets.getIamPolicy\nbigquery.datasets.updateTag\nYou CANNOT delete dataset with option \"B\" Bableves 3Â years, 9Â months ago\nNetiher with A. afooh 3Â years, 5Â months ago\nBut it means you will have to add the users one by one which doesn't follow Google best practices... YuvarajK 4Â years, 7Â months ago\nI think A is the Answer and it follow GCP best practices.\nhttps://cloud.google.com/iam/docs/understanding-roles#bigquery-roles\nWe do have the role - BigQuery User which does the below permissions\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project.\nbigquery.datasets.create\nbigquery.datasets.get\nbigquery.datasets.getIamPolicy Abhi00754 2Â years, 9Â months ago\nbigquery.datasets.create allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets so he can delete these created datasets Abhi00754 2Â years, 9Â months ago\nD seems correct kyo 4Â years, 5Â months ago\nI don't think A works properly.\nroles/bigquery.user has bigquery.datasets.create. And the documentation states:\n> Additional, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets.\nIf bigquery.user creates a new dataset, it's likely that bigquery.user will get permission to delete that dataset. This means that bigquery.user may have permission to delete data.\nhttps://cloud.google.com/bigquery/docs/access-control Bossam 3Â years, 5Â months ago\nSee the question carefully \"accidentally deleting the datasets\" it is saying not to delete \"the\" datasets which means original dataset which existed before his creation .So answer is A. Zina12 3Â years, 1Â month ago\nThe only way a user can accidentally delete a dataset is if they have the delete permission anyway. So brvinod and kyo's points still stand brvinod 3Â years, 11Â months ago\nA bigquery.user will get a \"data owner\" role on the datasets he creates. That means he can delete those data sets he created. In that sense A fails to that extent."
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (D):\nThe proper answer regarding to bigquery roles is the listed in the options, the proper rol that resolve this requirement is: roles/bigquery.dataViewer\nhttps://cloud.google.com/bigquery/docs/access-control#custom_roles\non the other hand, the question explicitly is asking to use the GCP best practices on IAM :\nGCP Best Practices explain clearly these rules:\nPolicy management\nâ‘ Set organization-level IAM policies to grant access to all projects in your organization.\nâ‘ Grant roles to a Google group instead of individual users when possible. It is easier to add members to and remove members from a Google group instead of updating an IAM policy to add or remove users.\nâ‘ If you need to grant multiple roles to allow a particular task, create a Google group, grant the roles to that group, and then add users to that group.\nhttps://cloud.google.com/iam/docs/using-iam-securely#policy_management JackGlemins 4Â years, 11Â months ago\nOther best practice is use predefine roles over custom roles. Maybe A is correct JackGlemins 4Â years, 11Â months ago\nI correct myself: https://cloud.google.com/iam/docs/understanding-custom-roles\nKey Point: Custom roles enable you to enforce the principle of least privilege, ensuring that the user and service accounts in your organization have only the permissions essential to performing their intended functions. prashuG 5Â years, 4Â months ago\nAnswer is A: roles/bigquery.user is a BigQuery User role which when applied to a project provides the ability to run jobs, including queries, within the project. A member with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project.\nRef: https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles j1c4b 4Â years, 9Â months ago\nyou can create data set with bigquery.user role because it has bigquery.datasets.create permissions. And if a user has bigquery.datasets.create permissions, when that user creates a dataset, they are granted bigquery.dataOwner access to it. So A is NOT a choice"
      },
      {
        "index": 3,
        "text": "Anonymous: Vismaya Most Recent 2Â weeks, 3Â days ago\nSelected Answer: D\nD is the answer"
      },
      {
        "index": 4,
        "text": "Anonymous: Mogamal 5Â months, 1Â week ago\nSelected Answer: B\nA. roles/bigquery user: This role only provides permissions to run jobs, list datasets, and view metadata. It does not grant permission to query data, which is a key requirement."
      },
      {
        "index": 5,
        "text": "Anonymous: Anji14 1Â year, 3Â months ago\nA is correct because the key point is.. users can query the dataset but not delete. For querying, jobs create role required which comes under bigquery user role"
      },
      {
        "index": 6,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: D\nD. follows Google Best Practice. Others do not or are flat out wrong."
      },
      {
        "index": 7,
        "text": "Anonymous: omunoz 1Â year, 8Â months ago\nSelected Answer: D\nGCP Best Practices is to create a group of Users and assign it a custom role with the required permissions (least privilege)."
      },
      {
        "index": 8,
        "text": "Anonymous: rahulsahni849 1Â year, 10Â months ago\nSelected Answer: A\nA IS THE ANWER"
      },
      {
        "index": 9,
        "text": "Anonymous: blackBeard33 1Â year, 10Â months ago\nSelected Answer: D\nD is solves the problem and follow best practices. With A you will be able to delete data sets you create."
      },
      {
        "index": 10,
        "text": "Anonymous: Aks14 1Â year, 11Â months ago\nSelected Answer: A\nYou can create a custom role at the project or organization level. Since users are added to role, it should be A. https://cloud.google.com/iam/docs/creating-custom-roles"
      }
    ]
  },
  {
    "id": 13,
    "source": "examtopics",
    "question": "You have a developer laptop with the Cloud SDK installed on Ubuntu. The Cloud SDK was installed from the Google Cloud Ubuntu package repository. You want to test your application locally on your laptop with Cloud Datastore. What should you do?",
    "options": {
      "A": "Export Cloud Datastore data using gcloud datastore export.",
      "B": "Create a Cloud Datastore index using gcloud datastore indexes create.",
      "C": "Install the google-cloud-sdk-datastore-emulator component using the apt get install command.",
      "D": "Install the cloud-datastore-emulator component using the gcloud components install command."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: someoneinthecloud Highly Voted 5Â years, 6Â months ago\nI believe answer is C\nhttps://cloud.google.com/sdk/docs/downloads-apt-get\nThe question is not about the datastore command itself but from where we should run the update command on the Ubuntu to install the component. XRiddlerX 5Â years, 5Â months ago\nI agree with this comment. The answer is C.\nIf you installed the SDK from the Ubuntu repo and try to do the following:\n$ gcloud components install cloud-datastore-emulator\nYou will receive this message:\nERROR: (gcloud.components.install)\nYou cannot perform this action because the Cloud SDK component manager\nis disabled for this installation. You can run the following command\nto achieve the same result for this installation:\nsudo apt-get install google-cloud-sdk-datastore-emulator stepkurniawan 5Â years, 4Â months ago\nit says that in your Ubuntu, you have Cloud SDK installed already. So it should be able to run the command in D Ale1973 5Â years, 4Â months ago\nYes, but it says that \"The Cloud SDK was installed from the Google Cloud Ubuntu package repository\", then to install datastore emulator you should use the command in Option C. Ale1973 5Â years, 4Â months ago\nWOW!!! Today I have learned a new and interesting thing thanks to you... myuniquename 4Â years, 3Â months ago\nabsolutely insane if that question comes up during the associate exam, who on earth would know that off the top of their heads? Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: professor Highly Voted 5Â years, 6Â months ago\nAns is D\nhttps://cloud.google.com/datastore/docs/tools/datastore-emulator Eshkrkrkr 5Â years, 2Â months ago\nWrong! The answer is C! When you install SDK using apt Cloud SDK Component Manager is disabled and you need to install extra packages again using apt.\nhttps://cloud.google.com/sdk/docs/components#managing_cloud_sdk_components\nNote: These instructions will not work if you have installed Cloud SDK using a package manager such as APT or yum because Cloud SDK Component Manager is disabled when using that method of installation. SWObaby 5Â years, 1Â month ago\nI believe the answer is C...\nIt is a tricky question!! The question states, \"The Cloud SDK was installed from the Google Cloud Ubuntu package repository.\" For those, who aren't that familiar with Debian/Ubuntu, D seems like an attractive answer. It works as a way to install Datastore...but it does NOT fit the context of the question.\nI recommend looking back to G Cloud SDK installation (Debian/Ubuntu): https://cloud.google.com/sdk/docs/install#deb\nRead the \"Installation Steps\" in the documentation. In Step 3, \"sudo apt-get update && sudo apt-get install google-cloud-sdk\". Then, Step 4 is additionally adding other components, such as \"sudo apt-get install google-cloud-sdk-datastore-emulator\".\nProving C the correct answer. ShakthiGCP 4Â years, 10Â months ago\nGo With 'C' ... just tried creating a ubuntu server and verified these.. Dont worry about any other options. https://cloud.google.com/sdk/docs/quickstart#deb check this link ."
      },
      {
        "index": 3,
        "text": "Anonymous: Vismaya Most Recent 2Â weeks, 3Â days ago\nSelected Answer: D\ngcloud components install is the correct one"
      },
      {
        "index": 4,
        "text": "Anonymous: AdelElagawany 3Â months, 3Â weeks ago\nSelected Answer: C\nThe correct Answer is C\nI managed to reproduce it as below:\n- When using : gcloud components install cloud-datastore-emulator, I got the below error:\nERROR: (gcloud.components.install)\nYou cannot perform this action because the Google Cloud CLI component manager\nis disabled for this installation. You can run the following command\nto achieve the same result for this installation: apt-get install google-cloud-cli-datastore-emulator\n- When using: sudo apt-get install google-cloud-sdk-datastore-emulator, It works."
      },
      {
        "index": 5,
        "text": "Anonymous: RushinthJohn 7Â months, 1Â week ago\nSelected Answer: D\nYou can't directly install the Datastore emulator with apt-get install google-cloud-sdk-datastore-emulator. The Datastore emulator is a component of the Google Cloud SDK (gcloud CLI), and it's installed using the gcloud components install command."
      },
      {
        "index": 6,
        "text": "Anonymous: yodaforce 10Â months ago\nSelected Answer: C\nExplanation:\nSince you installed the Cloud SDK from the Google Cloud Ubuntu package repository, you should also install additional components (like the Cloud Datastore emulator) using the same package manager (apt-get) rather than gcloud components install.\nTo install the Datastore emulator, run:\nsudo apt-get install google-cloud-sdk-datastore-emulator\nWhy Not the Others?\nA. gcloud datastore export â†’ Exports data but does not install the emulator.\nB. gcloud datastore indexes create â†’ Creates Datastore indexes but does not provide local testing.\nD. gcloud components install cloud-datastore-emulator â†’ This command only works if you installed the SDK via the tarball or ZIP method. However, since your installation is from the Google Cloud Ubuntu package repository, you must use apt-get instead."
      },
      {
        "index": 7,
        "text": "Anonymous: peddyua 12Â months ago\nSelected Answer: C\nworks: apt-get install google-cloud-sdk-datastore-emulator"
      },
      {
        "index": 8,
        "text": "Anonymous: 09bd94b 1Â year, 1Â month ago\nSelected Answer: C\nIt is installed using the same procedure as the Cloud SDK"
      },
      {
        "index": 9,
        "text": "Anonymous: juankloyd 1Â year, 2Â months ago\nSelected Answer: C\nC for me."
      },
      {
        "index": 10,
        "text": "Anonymous: deskj 1Â year, 3Â months ago\nC is correct because the datastore emulator is installed using apt and not gcloud.\nD is incorrect because the \"cloud-datastore-emulator\" component is a legacy component and is not recommended for use. The correct component to install is \"google-cloud-sdk-datastore-emulator\".\nhttps://cloud.google.com/datastore/docs/tools/datastore-emulator"
      }
    ]
  },
  {
    "id": 14,
    "source": "examtopics",
    "question": "Your company set up a complex organizational structure on Google Cloud. The structure includes hundreds of folders and projects. Only a few team members should be able to view the hierarchical structure. You need to assign minimum permissions to these team members, and you want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Add the users to roles/browser role.",
      "B": "Add the users to roles/iam.roleViewer role.",
      "C": "Add the users to a group, and add this group to roles/browser.",
      "D": "Add the users to a group, and add this group to roles/iam.roleViewer role."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (C):\nWe need to apply the GCP Best practices.\nroles/browser Browser Read access to browse the hierarchy for a project, including the folder, organization, and IAM policy. This role doesn't include permission to view resources in the project.\nhttps://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "index": 2,
        "text": "Anonymous: SIX Highly Voted 5Â years, 7Â months ago\nC is the better answer."
      },
      {
        "index": 3,
        "text": "Anonymous: Ciupaz Most Recent 1Â year, 2Â months ago\nSelected Answer: A\nDirectly assigning the roles/browser role to users (answer A) is a simpler and more immediate solution, without the need to manage groups, which reduces administrative complexity."
      },
      {
        "index": 4,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: C\n(roles/browser)\nRead access to browse the hierarchy for a project, including the folder, organization, and allow policy. This role doesn't include permission to view resources in the project.\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#browser\nTo view, only, \"view the hierarchical structure\", C. But with browser role they will not be able to view resources, only the structure."
      },
      {
        "index": 5,
        "text": "Anonymous: zangetsu2 1Â year, 11Â months ago\nSelected Answer: C\nhttps://cloud.google.com/iam/docs/understanding-roles#browser"
      },
      {
        "index": 6,
        "text": "Anonymous: AndyMandy 2Â years ago\nSelected Answer: D\nhttps://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "index": 7,
        "text": "Anonymous: nathanBK 2Â years, 1Â month ago\nSelected Answer: C\nThe correct answer is D. Add the users to a group, and add this group to roles/iam.roleViewer role.\nThis approach follows Google-recommended practices by utilizing IAM groups to manage user permissions and ensure granular control over access to the organizational structure. By adding users to a group and assigning the roles/iam.roleViewer role to the group, you can effectively grant the necessary permissions to view the hierarchical structure while minimizing the scope of access."
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe answer is D"
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: C\nC is the correct answer.\nhttps://cloud.google.com/iam/docs/understanding-roles#browser"
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC is the correct answer"
      }
    ]
  },
  {
    "id": 15,
    "source": "examtopics",
    "question": "Your company has a single sign-on (SSO) identity provider that supports Security Assertion Markup Language (SAML) integration with service providers. Your company has users in Cloud Identity. You would like users to authenticate using your company's SSO provider. What should you do?",
    "options": {
      "A": "In Cloud Identity, set up SSO with Google as an identity provider to access custom SAML apps.",
      "B": "In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider.",
      "C": "Obtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Mobile & Desktop Apps.",
      "D": "Obtain OAuth 2.0 credentials, configure the user consent screen, and set up OAuth 2.0 for Web Server Applications."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: PhilipAWS Highly Voted 4Â years ago\nOnly option B make sense to me as per - https://support.google.com/cloudidentity/answer/6262987?hl=en&ref_topic=7558767 nitinz 3Â years, 11Â months ago\nyou nailed it. B is correct."
      },
      {
        "index": 2,
        "text": "Anonymous: poogcp Highly Voted 4Â years, 7Â months ago\nFor me its B option"
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 3Â weeks ago\nSelected Answer: B\nCloud Identity and Google Workspace support Security Assertion Markup Language (SAML) 2.0 for single sign-on. SAML is an open standard for exchanging authentication and authorization data between a SAML IdP and SAML service providers. When you use SSO for Cloud Identity or Google Workspace, your external IdP is the SAML IdP and Google is the SAML service provider.\nREF: https://cloud.google.com/architecture/identity/single-sign-on#single_sign-on_process"
      },
      {
        "index": 4,
        "text": "Anonymous: kelliot 1Â year, 1Â month ago\nSelected Answer: B\nFor me its B"
      },
      {
        "index": 5,
        "text": "Anonymous: walker1988 1Â year, 2Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nOption B is makes sense"
      },
      {
        "index": 7,
        "text": "Anonymous: ekta25 1Â year, 3Â months ago\nB. In Cloud Identity, set up SSO with a third-party identity provider with Google as a service provider."
      },
      {
        "index": 8,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: B\nWhen you use SSO for Cloud Identity or Google Workspace, your external IdP is the SAML IdP and Google is the SAML service provider.\nhttps://cloud.google.com/architecture/identity/single-sign-on#single_sign-on_process"
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB is the correct answer as c and d are not SAML"
      },
      {
        "index": 10,
        "text": "Anonymous: goshubh 1Â year, 4Â months ago\nChat GPT has suggested option A, here's the reason it gave for why B is not the right option-\nsetting up SSO with a third-party identity provider with Google as a service provider, is typically used when your organization wants to use an external SSO IdP, not Google Cloud Identity, to authenticate users."
      }
    ]
  },
  {
    "id": 16,
    "source": "examtopics",
    "question": "Your organization has a dedicated person who creates and manages all service accounts for Google Cloud projects. You need to assign this person the minimum role for projects. What should you do?",
    "options": {
      "A": "Add the user to roles/iam.roleAdmin role.",
      "B": "Add the user to roles/iam.securityAdmin role.",
      "C": "Add the user to roles/iam.serviceAccountUser role.",
      "D": "Add the user to roles/iam.serviceAccountAdmin role."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: PhilipAWS Highly Voted 5Â years ago\nWhoever say C is right answer, please read the question 1000000000 times if not understand - \"Your organization has a dedicated person who creates and manages all service accounts for Google Cloud projects. \" Dedicated person who creates and manages all service... Now read below;\nTo allow a user to manage service accounts, grant one of the following roles:\nService Account User (roles/iam.serviceAccountUser): Includes permissions to list service accounts, get details about a service account, and impersonate a service account.\nService Account Admin (roles/iam.serviceAccountAdmin): Includes permissions to list service accounts and get details about a service account. Also includes permissions to create, update, and delete service accounts, and to view or change the IAM policy on a service account.\nNow look in which role mentioned \"CREATE\"?\nObviously - roles/iam.serviceAccountAdmin....... So Answer is????\n1M% - D only Jhelum 4Â years ago\nCalm down Jamal, don't pull out the knife... creativenets 2Â years, 7Â months ago\nHe's right. Calm down. Save that bomb for later. Romio2023 2Â years, 1Â month ago\nno, i want to choose C iooj 1Â year, 4Â months ago\nobviously C - 1B%"
      },
      {
        "index": 2,
        "text": "Anonymous: SIX Highly Voted 5Â years, 7Â months ago\nThe right answer is D."
      },
      {
        "index": 3,
        "text": "Anonymous: kelliot Most Recent 2Â years, 1Â month ago\nSelected Answer: D\nNo doubt, is D"
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nSelected Answer: D\nD\nAs per the documentation:\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is D"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: D\nroles/iam.serviceAccountAdmin --> Create and manage service accounts.\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin\nroles/iam.serviceAccountUser --> Run operations as the service account.\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD is the right answer, as the persnon needs to create also"
      },
      {
        "index": 8,
        "text": "Anonymous: Neha_Pallavi 2Â years, 4Â months ago\nD.Service Account Admin (roles/iam.serviceAccountAdmin): Includes permissions to list service accounts and get details about a service account. Also includes permissions to create, update, and delete service accounts, and to view or change the IAM policy on a service account."
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years, 6Â months ago\ndedicated person who creates and manages all service accounts is key world makes me select D as right answer."
      },
      {
        "index": 10,
        "text": "Anonymous: haroldbenites 3Â years, 7Â months ago\nGo for D"
      }
    ]
  },
  {
    "id": 17,
    "source": "examtopics",
    "question": "You are building an archival solution for your data warehouse and have selected Cloud Storage to archive your data. Your users need to be able to access this archived data once a quarter for some regulatory requirements. You want to select a cost-efficient option. Which storage option should you use?",
    "options": {
      "A": "Cold Storage",
      "B": "Nearline Storage",
      "C": "Regional Storage",
      "D": "Multi-Regional Storage"
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Teegongkia Highly Voted 5Â years, 5Â months ago\nTook ACE last week and the exact question came out. I go with B as i felt A is a trick answer. There is no Cold Storage in GCP. ssankar 5Â years, 5Â months ago\nHello Teegongkia ,\nis the questions are still valid ??\nThanks BenAji 4Â years, 9Â months ago\nCold data tiering refers to the storage of less frequently, or sporadically accessed data in low cost media such as HDFS (Hadoop Distributed File System) and cloud storage options including Amazon Web Services (AWS), Google Cloud Platform (GCP), and Azure Data Lake Storage (ADLS) that are managed separately from the SAP HANA database, but still accessible at any time. blogs.sap.com/2018/12/03/what-is-sap-hana-cold-data-tiering/"
      },
      {
        "index": 2,
        "text": "Anonymous: droogie Highly Voted 5Â years, 7Â months ago\nThis one is confusing. First, there's no 'Cold' storage. It's Coldline.\nNearline Storage is ideal for data you plan to read or modify on average once per month or less.Coldline Storage is ideal for data you plan to read or modify at most once a quarter.\nhttps://cloud.google.com/storage/docs/storage-classes\nSo with the misspelling of 'Cold' and these guys accessing it every 90 days, I'm leaning towards Nearline TAvenger 4Â years, 11Â months ago\nI believe the question is old, when Regional and Multi-Regional were also storage classes of the GCS.\nBefore changes: (Multi-Region, Regional, Nearline, Coldline)\nAfter recent changes we have\n- Storage Classes (Standard, Nearline, Coldline, Archive)\n- Storage Locations (Regional, Dual-region, Multi-Region)\nIt's tricky for exam because we don't to answer according to old version or new version.\nFor the latest version, costs for 1Gb for storing (3 month) + retrieval\nNearline: 0.01 * 3 + 0.01 = 0.04\nColdline: 0.004 * 3 + 0.02 = 0.032\nColdline is more cost effective.\nIf \"Cold\" means Coldline (not Archive) the asnwer is A\nIf \"Cold\" means Archive the answer is B\nI hope that \"Cold\" means Coldline. I would try wirh A ri_unhou119 4Â years, 8Â months ago\nAï¼š\nGoogle Cloud doc:\nhttps://cloud.google.com/storage/docs/storage-classes#coldline obeythefist 3Â years, 10Â months ago\nYes, but the question says the data will be accessed once per quarter, Google's documentation tells us that Coldline is most suitable for data accessed less than once per quarter. This direct part of the question tells us how we must answer. Eshkrkrkr 5Â years, 2Â months ago\nIt's a typo. Google wouldn't force to consume knowledge that is a non-best practice from Google. Asnwer is A. lxgywil 4Â years, 8Â months ago\nFor Google, these exams are just another business. DickDastardly 4Â years, 10Â months ago\n\"Cold\" is not a typo. I took the exam today and the answers appeared exactly as listed here. sarahf 5Â years, 1Â month ago\nAt the page for data archiving (https://cloud.google.com/storage/archival) the first paragraph says: \"Coldline is also ideal for cold storageâ€”data your business expects to touch less than once a quarter.\"\nSo there is such thing as Cold storage according to Google.\nAlso at (https://cloud.google.com/storage/docs/storage-classes#archive) they talk about Cold storage: \"Cold data storage - Archived data, such as data stored for legal or regulatory reasons, can be stored at low cost as Archive Storage, yet still be available if you need it.\" sanhoo 4Â years, 7Â months ago\nThanks for bringing this up. Really helpfull. sanhoo 4Â years, 7Â months ago\nThis line indicates that cold storage term is used for - archival / coldline\n\"With low latency and a consistent API across Cloud Storage, Archive and Coldline introduce cold storage you can actually use\" https://cloud.google.com/storage/archival"
      },
      {
        "index": 3,
        "text": "Anonymous: guaose Most Recent 2Â months, 2Â weeks ago\nSelected Answer: B\nYour users need to be able to access this archived data once a quarter for some regulatory requirements --> more than 1 time in a quarter --> nearline"
      },
      {
        "index": 4,
        "text": "Anonymous: yodaforce 10Â months ago\nSelected Answer: A\nOnce a Quarter. Coldline"
      },
      {
        "index": 5,
        "text": "Anonymous: velrisan 1Â year ago\nSelected Answer: A\nIs A. Why is not B?. Because is meant for data that is accessed about once a month."
      },
      {
        "index": 6,
        "text": "Anonymous: Moin23 1Â year, 1Â month ago\nSelected Answer: A\nCold data storage - Archived data, such as data stored for legal or regulatory reasons, can be stored at low cost as Archive storage, yet still be available if you need it.\nDisaster recovery - In the event of a disaster recovery event, recovery time is key. Cloud Storage provides low latency access to data stored as Archive storage.\nhttps://cloud.google.com/storage/docs/storage-classes#:~:text=Cold%20data%20storage%20%2D%20Archived%20data,available%20if%20you%20need%20it."
      },
      {
        "index": 7,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nThere is no such thing as Cold Storage, only Coldline storage.\nNearline storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice.\nhttps://cloud.google.com/storage/docs/storage-classes"
      },
      {
        "index": 8,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: B\nFor data accessed less than once a year, Archive\nOk, once a quarter, is 4 times per year, so Cold Storage is not correct for this"
      },
      {
        "index": 9,
        "text": "Anonymous: RVSubbareddy 1Â year, 8Â months ago\nSelected Answer: A\nwe have to consider it as coldline storage"
      },
      {
        "index": 10,
        "text": "Anonymous: Jonassamr 1Â year, 9Â months ago\nSelected Answer: B\ni think it's B"
      }
    ]
  },
  {
    "id": 18,
    "source": "examtopics",
    "question": "A team of data scientists infrequently needs to use a Google Kubernetes Engine (GKE) cluster that you manage. They require GPUs for some long-running, non- restartable jobs. You want to minimize cost. What should you do?",
    "options": {
      "A": "Enable node auto-provisioning on the GKE cluster.",
      "B": "Create a VerticalPodAutscaler for those workloads.",
      "C": "Create a node pool with preemptible VMs and GPUs attached to those VMs.",
      "D": "Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Polok Highly Voted 5Â years, 7Â months ago\nIf you need something for long-running, non- restartable jobs you dont use preemptible VMs\nThink answer is D."
      },
      {
        "index": 2,
        "text": "Anonymous: [Removed] Highly Voted 4Â years, 9Â months ago\nIncorrect options are\nB. VerticalPodAutscaler scales PODS based on the app you deploy.\nFor handle infrequently GPU access, you need infrequently GPU nodes\nVerticalAutscaler Pod deployed on a non GPU node it useless,\n[We cant have the node always have GPU for infrequent requests]\nC. Preemptible VMs cant last long\nD. For infrequent access, you don't want to have a permanent homogenous cluster.\nThe correct option is \"A\"\nauto-provisioning = Attaches and deletes node pools to cluster based on the requirements.\nHence creating a GPU node pool, and auto-scaling would be better\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning kimharsh 4Â years, 1Â month ago\nA is not correct because you can't add a GPU node to an existing GKE cluster\nLimitations\nBefore using GPUs on GKE, keep in mind the following limitations:\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\nGPUs are only supported with general-purpose N1 machine types.\nGPUs are not supported in Windows Server node pools\nREF: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#limitations\nSo the answer should be D rachee 4Â years, 1Â month ago\nYour reference says existing \"node pools\" not GKE cluster. Auto-provisioning creates new \"node pools\": https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning Ridhanya 4Â years, 1Â month ago\nbut node pools are homogenous, so how can we be sure that option A will create a GPU node pool wjtb 3Â years, 7Â months ago\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning\nNode auto-provisioning creates node pools based on the following information:\nCPU, memory and ephemeral storage resource requests.\nGPU requests\nPending Pods' node affinities and label selectors.\nPending Pods' node taints and tolerations."
      },
      {
        "index": 3,
        "text": "Anonymous: Vismaya Most Recent 2Â weeks, 3Â days ago\nSelected Answer: D\nD is the answer"
      },
      {
        "index": 4,
        "text": "Anonymous: Moin23 1Â year, 1Â month ago\nSelected Answer: D\nno one is talking about non-restartable jobs , it should be D then"
      },
      {
        "index": 5,
        "text": "Anonymous: RKS_2021 1Â year, 4Â months ago\nSelected Answer: A\nChanging the answer"
      },
      {
        "index": 6,
        "text": "Anonymous: RKS_2021 1Â year, 4Â months ago\nSelected Answer: D\nD is right answer"
      },
      {
        "index": 7,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: A\nIt's A. Shit gets only auto-provisioned when your devs actually deploy something that requires a GPU. It doesn't run permanently by default thus saves costs since it only gets provisioned when neededn."
      },
      {
        "index": 8,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: A\nA is correct. If the application requires a GPU then auto-provisioning will provision a vm with a GPU"
      },
      {
        "index": 9,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: D\n\"Enable node auto-provisioning\" with GPU will not works due to limitation \"You cannot add GPUs to existing node pools\""
      },
      {
        "index": 10,
        "text": "Anonymous: Sheqos 1Â year, 10Â months ago\nSelected Answer: D\nSelected Answer: D"
      }
    ]
  },
  {
    "id": 19,
    "source": "examtopics",
    "question": "Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do?",
    "options": {
      "A": "Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.",
      "B": "Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity.",
      "C": "Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.",
      "D": "Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: professor Highly Voted 4Â years, 6Â months ago\nAns is A\nhttps://tools.google.com/dlpage/dirsync/"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer (A):\nDirectory Sync\nGoogle Cloud Directory Sync enables administrators to synchronize users, groups and other data from an Active Directory/LDAP service to their Google Cloud domain directory\nhttps://tools.google.com/dlpage/dirsync/"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nhttps://support.google.com/a/answer/106368?hl=en\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-accounts\nhttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct answer as it help you to sybchronize users"
      },
      {
        "index": 6,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nA. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity."
      },
      {
        "index": 7,
        "text": "Anonymous: Nazz1977 2Â years, 1Â month ago\nSelected Answer: A\nA....is right"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nA is right, this is part of Tutorials Dojo practice test"
      },
      {
        "index": 9,
        "text": "Anonymous: haroldbenites 2Â years, 7Â months ago\nGo for A"
      },
      {
        "index": 10,
        "text": "Anonymous: crisyeb 2Â years, 10Â months ago\nSelected Answer: A\nA is correct"
      }
    ]
  },
  {
    "id": 20,
    "source": "examtopics",
    "question": "You have successfully created a development environment in a project for an application. This application uses Compute Engine and Cloud SQL. Now you need to create a production environment for this application. The security team has forbidden the existence of network routes between these 2 environments and has asked you to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment.",
      "B": "Create a new production subnet in the existing VPC and a new production Cloud SQL instance in your existing project, and deploy your application using those resources.",
      "C": "Create a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC.",
      "D": "Ask the security team to grant you the Project Editor role in an existing production project used by another division of your company. Once they grant you that role, replicate the setup you have in the development environment in that project."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 4Â years, 7Â months ago\nCorrect answer is A. pYWORLD 3Â years, 5Â months ago\nCorrect answer!"
      },
      {
        "index": 2,
        "text": "Anonymous: JieHeng Highly Voted 3Â years, 6Â months ago\nShould be A\nit's a best practice \"to have one project per application per environment.\" - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#project-structure"
      },
      {
        "index": 3,
        "text": "Anonymous: kelliot Most Recent 1Â year, 1Â month ago\nSelected Answer: A\nA\nGoogle's best practices says \"create a new project for each environment."
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nAccording to Google recommended practices, you should create a separate project for different environments (dev, test, and prod). Also, the question has forbidden the existence of these environments so shared VPC cannot be used."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct answer, as it satisy tyhe requirement of security team , no commiunicatiojn , as option c allows coummnication"
      },
      {
        "index": 7,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nA. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment."
      },
      {
        "index": 8,
        "text": "Anonymous: diasporabro 2Â years, 3Â months ago\nSelected Answer: A\nSatisfies requirements by the security team"
      },
      {
        "index": 9,
        "text": "Anonymous: anolive 2Â years, 3Â months ago\nSelected Answer: A\nmake sense"
      },
      {
        "index": 10,
        "text": "Anonymous: alexandercamachop 2Â years, 5Â months ago\nSelected Answer: A\nA is definitely the answer."
      }
    ]
  },
  {
    "id": 21,
    "source": "examtopics",
    "question": "Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called\nDomain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?",
    "options": {
      "A": "Ask the auditor for their Google account, and give them the Viewer role on the project.",
      "B": "Ask the auditor for their Google account, and give them the Security Reviewer role on the project.",
      "C": "Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.",
      "D": "Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 5Â years, 7Â months ago\nC - https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors spudleymcdudley 5Â years, 6Â months ago\nThis guy is right!"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (C):\nroles/viewer Read access to all resources. Get and list access for all resources.\nUsing primitive roles\nThe following table lists the primitive roles that you can grant to access a project, the description of what the role does, and the permissions bundled within that role. Avoid using primitive roles except when absolutely necessary. These roles are very powerful, and include a large number of permissions across all Google Cloud services. For more details on when you should use primitive roles, see the Identity and Access Management FAQ.\nIAM predefined roles are much more granular, and allow you to carefully manage the set of permissions that your users have access to. See Understanding Roles for a list of roles that can be granted at the project level. Creating custom roles can further increase the control you have over user permissions.\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#using_primitive_roles"
      },
      {
        "index": 3,
        "text": "Anonymous: kayceeec Most Recent 1Â year, 7Â months ago\nSelected Answer: C\nthe key word is \"organisation Policy called Domain Restricted sharing.\" his external google account wont work"
      },
      {
        "index": 4,
        "text": "Anonymous: Ankit_EC_ran 1Â year, 10Â months ago\nSelected Answer: C\nCORRECT ANSWER IS C"
      },
      {
        "index": 5,
        "text": "Anonymous: ogerber 2Â years, 1Â month ago\nSelected Answer: C\nDomain Restricted Sharing: Since your organization has the Domain Restricted Sharing policy enabled, sharing resources with accounts outside your Cloud Identity domain isn't allowed. Therefore, options A and B, which involve using the auditor's Google account, aren't feasible."
      },
      {
        "index": 6,
        "text": "Anonymous: kelliot 2Â years, 1Â month ago\nC, without doubt"
      },
      {
        "index": 7,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nSelected Answer: D\nD\nAs per the documentation, Security Reviewer is more narrow role than the basic Viewer role:\nhttps://cloud.google.com/iam/docs/understanding-roles#iam.securityReviewer\nhttps://cloud.google.com/iam/docs/understanding-roles#viewer"
      },
      {
        "index": 8,
        "text": "Anonymous: Rahaf99 2Â years, 2Â months ago\nSelected Answer: C\nIt could be A, But C is more practical and you don't have to give the auditor extra 3 seconds of work, and yourself for deleting him after he finishes"
      },
      {
        "index": 9,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is C"
      },
      {
        "index": 10,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: C\nThe Resource Manager provides a domain restriction constraint that can be used in organization policies to limit resource sharing based on domain or organization resource. This constraint allows you to restrict the set of identities that are allowed to be used in Identity and Access Management policies.\nOrganization policies can use this constraint to limit resource sharing to identities that belong to a particular organization resource.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      }
    ]
  },
  {
    "id": 22,
    "source": "examtopics",
    "question": "You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Create a Cloud Function to create an instance template.",
      "B": "Create a snapshot schedule for the disk using the desired interval.",
      "C": "Create a cron job to create a new disk from the disk using gcloud.",
      "D": "Create a Cloud Task to create an image and export it to Cloud Storage."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer (B):\nBest practices for persistent disk snapshots\nYou can create persistent disk snapshots at any time, but you can create snapshots more quickly and with greater reliability if you use the following best practices.\nCreating frequent snapshots efficiently\nUse snapshots to manage your data efficiently.\nCreate a snapshot of your data on a regular schedule to minimize data loss due to unexpected failure.\nImprove performance by eliminating excessive snapshot downloads and by creating an image and reusing it.\nSet your snapshot schedule to off-peak hours to reduce snapshot time.\nSnapshot frequency limits\nCreating snapshots from persistent disks\nYou can snapshot your disks at most once every 10 minutes. If you want to issue a burst of requests to snapshot your disks, you can issue at most 6 requests in 60 minutes.\nIf the limit is exceeded, the operation fails and returns the following error:\nhttps://cloud.google.com/compute/docs/disks/snapshot-best-practices"
      },
      {
        "index": 2,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years ago\nB is correct for this question stepkurniawan 4Â years, 10Â months ago\nQuestion: One cannot delete the old disk when using snapshot, right? Ale1973 4Â years, 10Â months ago\nSnapshots and disks are independent objects con GCP, you could create a snapshot form disk and then delete the disk, the snapshot will stay in place. Actually, you could use this snapshot to create a new disk, assign to another VM, mount it, and use it (all the information that the original disk had at the time of the snapshot will still be there). Ridhanya 3Â years, 7Â months ago\nIn snapshot schedule, there is autodelete and you can specify the days after which auto delete can happen"
      },
      {
        "index": 3,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\n2020 is B. Now, 2024, better solution es backup&dr"
      },
      {
        "index": 4,
        "text": "Anonymous: Ankit_EC_ran 1Â year, 4Â months ago\nSelected Answer: B\nThe correct answer is B. Automatic snapshot and deletion as per the need."
      },
      {
        "index": 5,
        "text": "Anonymous: kelliot 1Â year, 7Â months ago\nSelected Answer: B\nB\nthe others make no sense at all"
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is B"
      },
      {
        "index": 7,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\ncreate a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads.\nA snapshot retention policy defines how long you want to keep your snapshots.\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots#retention_policy"
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct Answer, as you can create the snapshot as per your requirment"
      },
      {
        "index": 9,
        "text": "Anonymous: abirroy 2Â years, 11Â months ago\nSelected Answer: B\nCreate a snapshot schedule for the disk using the desired interval."
      },
      {
        "index": 10,
        "text": "Anonymous: csrazdan 3Â years ago\nSelected Answer: B\nSnapshot is a better option because they are incremental and you can configure them to consolidate and delete snapshots that are not required for recovery. Image can also provide this functionality but the image is full backup which is inefficient in cases where the content of the file system is changing frequently."
      }
    ]
  },
  {
    "id": 23,
    "source": "examtopics",
    "question": "You need to assign a Cloud Identity and Access Management (Cloud IAM) role to an external auditor. The auditor needs to have permissions to review your\nGoogle Cloud Platform (GCP) Audit Logs and also to review your Data Access logs. What should you do?",
    "options": {
      "A": "Assign the auditor the IAM role roles/logging.privateLogViewer. Perform the export of logs to Cloud Storage.",
      "B": "Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy.",
      "C": "Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Perform the export of logs to Cloud Storage.",
      "D": "Assign the auditor's IAM user to a custom role that has logging.privateLogEntries.list permission. Direct the auditor to also review the logs for changes to Cloud IAM policy."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 10Â months ago\nCorrect Answer is (B):\nBackground\nGoogle Cloud provides Cloud Audit Logs, which is an integral part of Cloud Logging. It consists of two log streams for each project: Admin Activity and Data Access.\nAdmin Activity logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity logs are always enabled. There is no charge for your Admin Activity audit logs.\nData Access logs record API calls that create, modify, or read user-provided data. Data Access audit logs are disabled by default because they can be large.\nlogging.viewer: The logging.viewer role gives the security admin team the ability to view the Admin Activity logs.\nlogging.privateLogViewer : The logging.privateLogViewer role gives the ability to view the Data Access logs. ESP_SAP 4Â years, 10Â months ago\nCorrect Answer is (B): (Continuation).\nScenario: External auditors\nIn this scenario, audit logs for an organization are aggregated and exported to a central sink location. A third-party auditor is granted access several\ntimes a year to review the organization's audit logs. The auditor is not authorized to view PII data in the Admin Activity logs.\nDuring normal access, the auditors' Google group is only granted access to view the historic logs stored in BigQuery. If any anomalies are discovered,\nthe group is granted permission to view the actual Cloud Logging Admin Activity logs via the dashboard's elevated access mode. At the end of each audit period,\nthe group's access is then revoked.\nData is redacted using Cloud DLP before being made accessible for viewing via the dashboard application. ESP_SAP 4Â years, 10Â months ago\nCorrect Answer is (B): (Continuation).\nThe table below explains IAM logging roles that an Organization Administrator can grant to the service account used by the dashboard,\nas well as the resource level at which the role is granted:\nlogging.viewer Organization Dashboard service account The logging.viewer role permits the service account to read the Admin Activity logs in Cloud Logging.\nbigquery.dataViewer BigQuery dataset Dashboard service account The bigquery.dataViewer role permits the service account used by the dashboard application\nto read the exported Admin Activity logs."
      },
      {
        "index": 2,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years ago\nfor me B is the correct answer.. Eshkrkrkr 4Â years, 8Â months ago\nYes, B is correct because:\n1) Question doesn't ask us to export and store logs for any long period of time.\n2) Custom role with only logging.privateLogEntries.list permission won't let the auditor to access Log Exporer at all (https://cloud.google.com/logging/docs/access-control#console_permissions - Minimal read-only access: logging.logEntries.list)"
      },
      {
        "index": 3,
        "text": "Anonymous: MANGANDA Most Recent 1Â year ago\nSelected Answer: D\nn this scenario, virtual machines in the web-applications project need access to BigQuery datasets in the crm-databases-proj project."
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: B\nThere is no need to export logs to Cloud Storage for the auditor to review them unless there's a specific requirement or preference for reviewing them outside the GCP environment. The Logging service provides the necessary tools for log viewing and querying within the console.\nDirecting the auditor to review logs for changes to Cloud IAM policy is part of their duties to ensure that the IAM policies have been correctly managed and modified. This does not require a separate permission as the privateLogViewer role already provides the necessary access."
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is B"
      },
      {
        "index": 6,
        "text": "Anonymous: ziomek666 1Â year, 9Â months ago\nNo logs in cloud storage since reviewer won't have access to it"
      },
      {
        "index": 7,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\n- The Logs Viewer role (roles/logging.viewer) gives you read-only access to Admin Activity, Policy Denied, and System Event audit logs. If you have just this role, you cannot view Data Access audit logs that are in the _Default bucket.\n- The Private Logs Viewer role(roles/logging.privateLogViewer) includes the permissions contained in roles/logging.viewer, plus the ability to read Data Access audit logs in the _Default bucket.\nTherefore, no need to export logs to Cloud storage explicitly, the _Default bucket sink access is already provided from the above role.\nhttps://cloud.google.com/iam/docs/audit-logging#audit_log_permissions"
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nb is the correect answer"
      },
      {
        "index": 9,
        "text": "Anonymous: Neha_Pallavi 1Â year, 10Â months ago\nB. Assign the auditor the IAM role roles/logging.privateLogViewer. Direct the auditor to also review the logs for changes to Cloud IAM policy."
      },
      {
        "index": 10,
        "text": "Anonymous: MilanRajGupta 1Â year, 11Â months ago\nThis answer is similar to answer choice B, but it suggests creating a custom role for the auditor that includes the \"logging.privateLogEntries.list\" permission. While this would provide the auditor with access to the necessary logs, directing them to also review Cloud IAM policy logs is not relevant to their request. Therefore, this answer is also not correct."
      }
    ]
  },
  {
    "id": 24,
    "source": "examtopics",
    "question": "You are managing several Google Cloud Platform (GCP) projects and need access to all logs for the past 60 days. You want to be able to explore and quickly analyze the log contents. You want to follow Google-recommended practices to obtain the combined logs for all projects. What should you do?",
    "options": {
      "A": "Navigate to Stackdriver Logging and select resource.labels.project_id=\"*\"",
      "B": "Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.",
      "C": "Create a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days.",
      "D": "Configure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Verve Highly Voted 4Â years, 11Â months ago\nIts B."
      },
      {
        "index": 2,
        "text": "Anonymous: dttncl Highly Voted 3Â years, 9Â months ago\nI believe B is the answer.\nAll that matters in this scenario is the logs for the past 60 days.\nWe can use BigQuery to analyze contents so C is incorrect. We need to configure a BQ as the sink for the logs export so we can query and analyze log data in the future. Therefore D is incorrect.\nhttps://cloud.google.com/logging/docs/audit/best-practices#export-best-practices\nSince we only care about the logs within 60 days, we can set the expiration time to 60 to retain only the logs within that time frame. Once data is beyond 60 days old, it wouldn't be included in future analyzations.\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time ryzior 3Â years, 4Â months ago\nI think here we have the case described in details:\nhttps://cloud.google.com/architecture/exporting-stackdriver-logging-for-security-and-access-analytics"
      },
      {
        "index": 3,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\n2024, there is not \"Stackdriver Logging Export, but for 2020 it is B"
      },
      {
        "index": 4,
        "text": "Anonymous: IshwarChandra 1Â year, 3Â months ago\nresource.labels.project_id=\"*\" is not a correct query because \"*\" returns 0 records so option A is not a correct answer"
      },
      {
        "index": 5,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: B\nWhen it comes to log data, you're typically dealing with high-volume time-series data that is partitioned by time (e.g., by day). In such cases, setting a partition expiration is often more appropriate because it ensures that you're continuously retaining a rolling window of log data (for example, the last 60 days' worth) and automatically purging older data, rather than deleting the entire table at once after a certain period. Cynthia2023 1Â year, 6Â months ago\nIn BigQuery, setting an expiration time for tables can be applied in two contexts:\nTable Expiration:\nWhen you set a table expiration time at the table level, it applies to the entire table. This means that the entire table will be deleted once the specified expiration time has elapsed since the table's creation time.\nPartition Expiration:\nFor partitioned tables, you can set a partition expiration time, which applies to individual partitions within the table. Each partition's data will be deleted once the specified expiration time has elapsed since the creation of that specific partition.\nThis is particularly useful for time-series data, like logs, where you might want to only keep recent data and allow older data to be automatically purged."
      },
      {
        "index": 6,
        "text": "Anonymous: Romio2023 1Â year, 7Â months ago\nI dont get the options"
      },
      {
        "index": 7,
        "text": "Anonymous: kelliot 1Â year, 7Â months ago\nSelected Answer: B\nI guess it's B"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is B"
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nProvides storage of log entries in BigQuery datasets. You can use big data analysis capabilities on the stored logs. Logging sinks stream logging data into BigQuery in small batches, which lets you query data without running a load job.\nYou can set a default table expiration time at the dataset level, or you can set a table's expiration time when the table is created. A table's expiration time is often referred to as \"time to live\" or TTL. When a table expires, it is deleted along with all of the data it contains.\nhttps://cloud.google.com/logging/docs/export/configure_export_v2#overview\nhttps://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time"
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is thecorrect answer, we can use bq to get 60 days logs and analyse"
      }
    ]
  },
  {
    "id": 25,
    "source": "examtopics",
    "question": "You need to reduce GCP service costs for a division of your company using the fewest possible steps. You need to turn off all configured services in an existing\nGCP project. What should you do?",
    "options": {
      "A": "1. Verify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID.",
      "B": "1. Verify that you are assigned the Project Owners IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them.",
      "C": "1. Verify that you are assigned the Organizational Administrator IAM role for this project. 2. Locate the project in the GCP console, enter the project ID and then click Shut down.",
      "D": "1. Verify that you are assigned the Organizational Administrators IAM role for this project. 2. Switch to the project in the GCP console, locate the resources and delete them."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: DarioFama23 Highly Voted 4Â years, 6Â months ago\nfor me is A the correct answer"
      },
      {
        "index": 2,
        "text": "Anonymous: shafiqeee1 Highly Voted 4Â years, 6Â months ago\nA - I reproduced in my project"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nhttps://cloud.google.com/resource-manager/docs/access-control-proj#permissions\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\na is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nVerify that you are assigned the Project Owners IAM role for this project. 2. Locate the project in the GCP console, click Shut down and then enter the project ID."
      },
      {
        "index": 7,
        "text": "Anonymous: vinodthakur49 1Â year, 6Â months ago\nSelected Answer: A\nA is the correct answer"
      },
      {
        "index": 8,
        "text": "Anonymous: Shenannigan 1Â year, 8Â months ago\nSelected Answer: A\nAnswer is A\nhttps://support.google.com/googleapi/answer/6251787?hl=en#zippy=%2Cshut-down-a-project"
      },
      {
        "index": 9,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: A\noption A"
      },
      {
        "index": 10,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\ni believe it's option A.\nroles/owner Owner All Editor permissions and permissions for the following actions:\nManage roles and permissions for a project and all resources within the project.\nSet up billing for a project."
      }
    ]
  },
  {
    "id": 26,
    "source": "examtopics",
    "question": "You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in crm-databases-proj. You want to follow Google-recommended practices to give access to the service account in the web-applications project. What should you do?",
    "options": {
      "A": "Give ×’â‚¬project owner×’â‚¬ for web-applications appropriate roles to crm-databases-proj.",
      "B": "Give ×’â‚¬project owner×’â‚¬ role to crm-databases-proj and the web-applications project.",
      "C": "Give ×’â‚¬project owner×’â‚¬ role to crm-databases-proj and bigquery.dataViewer role to web-applications.",
      "D": "Give bigquery.dataViewer role to crm-databases-proj and appropriate roles to web-applications."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ezat Highly Voted 5Â years, 6Â months ago\nD cuz u just need read for DB at the other project DarioFama23 5Â years, 6Â months ago\nU re right, D is the correct answee tavva_prudhvi 4Â years, 9Â months ago\nSee the option correctly, as the web app needs access to the big query datasets we have to give access to the web app the data viewer role to only read the datasets! Hence, C"
      },
      {
        "index": 2,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years, 6Â months ago\nC is correct.. BigQuery 3Â years, 10Â months ago\nTHAT SO DUM Romio2023 2Â years, 1Â month ago\nI meet BigQuery the first time ever personly GCPACE2020 4Â years, 6Â months ago\nBut why giving project owner role to crm-databases-proj ?"
      },
      {
        "index": 3,
        "text": "Anonymous: jlocke Most Recent 11Â months, 1Â week ago\nSelected Answer: D\nHereâ€™s how to interpret the wording:\n\"Give bigquery.dataViewer role to crm-databases-proj\" means that you update the IAM policy for the crm-databases-proj project (or specifically the BigQuery datasets within that project) so that the service account (which is part of the web-applications project) gets the BigQuery Data Viewer role."
      },
      {
        "index": 4,
        "text": "Anonymous: Izzy55555 11Â months, 1Â week ago\nSelected Answer: C\nI find this question confusing, in Google Cloud IAM, roles are granted to members, not directly to projects. Why are all the options to give roles to projects?"
      },
      {
        "index": 5,
        "text": "Anonymous: guidbem 1Â year, 3Â months ago\nIt is D. Tested and approved that you do not need BigQuery permissions on the web-app project to access data on the bq tables stored in the crm-dbs project. You do need bq permissions for the SA on the crm project and compute permissions for the same SA on the web-app project. Then, using this SA on a VM on the web-app server, you can access data from bq on the crm-dbs project"
      },
      {
        "index": 6,
        "text": "Anonymous: Namik 1Â year, 5Â months ago\nSelected Answer: C\nExplanation:\nLeast Privilege Principle: This approach adheres to the principle of least privilege by granting the minimum necessary permissions.\nProject Owner: The crm-databases-proj project needs full control to manage its resources.\nbigquery.dataViewer: The web-applications project only needs read access to BigQuery datasets in the crm-databases-proj project.\nWhy other options are less suitable:\nA: Giving project owner to web-applications provides unnecessary permissions.\nB: Giving project owner to both projects grants excessive permissions.\nD: Giving bigquery.dataViewer to crm-databases-proj is incorrect as this project needs full control over its resources.\nBy following option C, you ensure that the web-applications project has the required access to BigQuery datasets without compromising security."
      },
      {
        "index": 7,
        "text": "Anonymous: nish2288 1Â year, 6Â months ago\nSelected Answer: D\nLet's analyze the options:\nA & B: Granting \"project owner\" gives excessive permissions, violating the least privilege principle.\nC: Granting \"project owner\" to crm-databases-proj is unnecessary.\nD: Granting \"bigquery.dataViewer\" to crm-databases-proj allows the VM access to datasets and aligns with least privilege. Granting appropriate roles to web-applications secures the web application itself (not shown in this scenario).\nTherefore, option D is the recommended approach."
      },
      {
        "index": 8,
        "text": "Anonymous: abhi2704 1Â year, 10Â months ago\nProject owner role is not required here, so that leaves us with only Option D"
      },
      {
        "index": 9,
        "text": "Anonymous: Bagibo 2Â years ago\nSelected Answer: D\nA, b & c is wrong. Keywords is configuring aervice account. A,b & c concerns user account. Correct answer is D"
      },
      {
        "index": 10,
        "text": "Anonymous: Cynthia2023 2Â years ago\nNone of the options is correct. As for D:\nThis option is unclear and potentially misleading. The bigquery.dataViewer role should be assigned specifically to the service account in the web-applications project, not to the crm-databases-proj project. Cynthia2023 2Â years ago\nThe ideal approach (not listed in the options) would be:\nCreate a service account in the web-applications project specifically for accessing the BigQuery datasets.\nGrant this service account the bigquery.dataViewer role (or another more specific role if different access is needed) on the crm-databases-proj project's BigQuery datasets.\nUse this service account in your VMs in the web-applications project."
      }
    ]
  },
  {
    "id": 27,
    "source": "examtopics",
    "question": "An employee was terminated, but their access to Google Cloud was not removed until 2 weeks later. You need to find out if this employee accessed any sensitive customer information after their termination. What should you do?",
    "options": {
      "A": "View System Event Logs in Cloud Logging. Search for the user's email as the principal.",
      "B": "View System Event Logs in Cloud Logging. Search for the service account associated with the user.",
      "C": "View Data Access audit logs in Cloud Logging. Search for the user's email as the principal.",
      "D": "View the Admin Activity log in Cloud Logging. Search for the service account associated with the user."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: shubhi_1 1Â year, 9Â months ago\nOption C is more correct"
      },
      {
        "index": 2,
        "text": "Anonymous: idk_4 2Â years ago\nGuys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you BuenaCloudDE 1Â year, 6Â months ago\nShould use discussion to find out correct answer. Also usually you can find fine explain for question"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is C"
      },
      {
        "index": 4,
        "text": "Anonymous: cooldude26 2Â years, 2Â months ago\nC. View Data Access audit logs in Cloud Logging. Search for the user's email as the principal.\nData Access audit logs provide detailed information about accesses to your Google Cloud resources. By searching for the terminated employee's email address as the principal in the Data Access audit logs, you can track their access to sensitive customer information after their termination. This approach allows you to specifically focus on data access, which is crucial for identifying any unauthorized or suspicious activities related to customer data."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nOption C is more correct , as data access logs contain API , from this you can check for it"
      },
      {
        "index": 6,
        "text": "Anonymous: sabrinakloud 2Â years, 9Â months ago\nSelected Answer: C\nI think option C is correct :\nData Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data."
      },
      {
        "index": 7,
        "text": "Anonymous: Bobbybash 2Â years, 11Â months ago\nSelected Answer: C\nC. View Data Access audit logs in Cloud Logging. Search for the user's email as the principal.\nData Access audit logs record all activity related to accessing or modifying data, including reading, writing, and deleting operations. By searching for the terminated employee's email as the principal, you can see if they accessed any sensitive customer information after their termination. System Event Logs and Admin Activity logs may not have the details of the data accessed, so Data Access audit logs are the most appropriate option in this scenario."
      },
      {
        "index": 8,
        "text": "Anonymous: mrvergara 3Â years ago\nSelected Answer: A\nhttps://cloud.google.com/logging/docs/audit.\nData Access audit logs are disabled by default"
      },
      {
        "index": 9,
        "text": "Anonymous: Cornholio_LMC 3Â years, 3Â months ago\nhad this question today"
      },
      {
        "index": 10,
        "text": "Anonymous: abirroy 3Â years, 5Â months ago\nSelected Answer: C\nView Data Access audit logs in Cloud Logging. Search for the user's email as the principal"
      }
    ]
  },
  {
    "id": 28,
    "source": "examtopics",
    "question": "You need to create a custom IAM role for use with a GCP service. All permissions in the role must be suitable for production use. You also want to clearly share with your organization the status of the custom role. This will be the first version of the custom role. What should you do?",
    "options": {
      "A": "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.",
      "B": "Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to BETA while testing the role permissions.",
      "C": "Use permissions in your role that use the 'testing' support level for role permissions. Set the role stage to ALPHA while testing the role permissions.",
      "D": "Use permissions in your role that use the 'testing' support level for role permissions. Set the role stage to BETA while testing the role permissions."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: raksteer Highly Voted 5Â years, 6Â months ago\nYou need a custom role with permissions supported in prod and you want to publish the status of the role.\nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support\nSUPPORTED The permission is fully supported in custom roles.\nTESTING The permission is being tested to check its compatibility with custom roles. You can include the permission in custom roles, but you might see unexpected behavior. Not recommended for production use.\nNOT_SUPPORTED The permission is not supported in custom roles.\nYou can't use TESTING as it is not good for prod. And you need first version which should be ALPHA. Answer should be A. passmepls 5Â years, 5Â months ago\ngood job BigQuery 3Â years, 10Â months ago\nWAY TO GO. VERY CLEAR EXP INDEED"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 4Â months ago\nCorrect Answer is (A):\nTesting and deploying\nCustom roles include a launch stage, which is stored in the stage property for the role. The launch stage is informational; it helps you keep track of whether each role is ready for widespread use.\nEach custom role can have one of the following launch stages:\nLaunch stages\nALPHA The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.\nBETA The role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.\nGA The role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available. ESP_SAP 5Â years, 4Â months ago\nCorrect Answer is (A): Continuation\nSupport levels for permissions in custom roles\nYou can include many, but not all, Identity and Access Management (IAM) permissions in custom roles. Each permission has one of the following support levels:\nSupport level Description\nSUPPORTED The permission is fully supported in custom roles.\nTESTING The permission is being tested to check its compatibility with custom roles. You can include the permission in custom roles, but you might see unexpected behavior. Not recommended for production use.\nNOT_SUPPORTED The permission is not supported in custom roles.\nThe first version of the Custom Role is ALPHA then suitable to productions all permissions in \"Supported\"... GCP_Student1 4Â years, 10Â months ago\nESP_SAP\nThere is a discrepancy between your first post and the second post. Compare these two sentences;\n1st POST - ALPHA The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.\n2nd POST - SUPPORTED The permission is fully supported in custom roles.\nAre you still going to go with A ? learn_GCP 3Â years, 3Â months ago\nHere ALPHA is for Google cloud feature, only informational. given to identify whether the feature is fully available as a service.\nand SUPPORTED -- is for a custom role which is supported by Google cloud, meaning any support is provided by Google cloud"
      },
      {
        "index": 3,
        "text": "Anonymous: nish2288 Most Recent 1Â year, 6Â months ago\nSelected Answer: A\nLet's break down the options:\nA & B: \"supported\" is the ideal choice for production roles as they are well-tested and documented. BETA is for pre-release features, not initial testing.\nC: \"testing\" permissions are unstable and not suited for production. ALPHA stage is for internal testing before even BETA.\nD: Same issue as option C - \"testing\" permissions are not for production, and ALPHA is an earlier stage than BETA.\nTherefore, the most suitable approach is:\nA. Use permissions in your role that use the 'supported' support level for role permissions. Set the role stage to ALPHA while testing the role permissions."
      },
      {
        "index": 4,
        "text": "Anonymous: Rayuga1 2Â years, 1Â month ago\nIts A cause in first place it need to be supported not in testing phase because the question is asking it to be in ready phase secondly then needed to be shared in testing phase"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nI will go with A"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: A\nA is correct answer.\nhttps://cloud.google.com/iam/docs/roles-overview#custom-role-supported-permissions\nhttps://cloud.google.com/iam/docs/roles-overview#custom-role-testing-deploying"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nAnswer is A, as you need for production and you dont neeed testing for it and you need first version , so it will be ALPHA , not beta"
      },
      {
        "index": 8,
        "text": "Anonymous: Neha_Pallavi 2Â years, 4Â months ago\nYou need a custom role with permissions supported in prod and you want to publish the status of the role.\nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support\nSUPPORTED The permission is fully supported in custom roles."
      },
      {
        "index": 9,
        "text": "Anonymous: Tofer2022 3Â years, 2Â months ago\nwhy not B? temple1305 2Â years, 10Â months ago\nBecause ...FIRST VERSION... is ALPHA."
      },
      {
        "index": 10,
        "text": "Anonymous: theBestStudent 3Â years, 5Â months ago\nSelected Answer: A\nIt must be suitable for production so Supported permissions only. Plus, it is your first version of the custom role, so you need to check if all is good, then ALPHA."
      }
    ]
  },
  {
    "id": 29,
    "source": "examtopics",
    "question": "Your company has a large quantity of unstructured data in different file formats. You want to perform ETL transformations on the data. You need to make the data accessible on Google Cloud so it can be processed by a Dataflow job. What should you do?",
    "options": {
      "A": "Upload the data to BigQuery using the bq command line tool.",
      "B": "Upload the data to Cloud Storage using the gsutil command line tool.",
      "C": "Upload the data into Cloud SQL using the import function in the console.",
      "D": "Upload the data into Cloud Spanner using the import function in the console."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years ago\nB looks correct. Key work unstructured data obeythefist 3Â years, 4Â months ago\nAlso \"different\" file formats, this further supports B as the correct choice."
      },
      {
        "index": 2,
        "text": "Anonymous: cooldude26 Highly Voted 1Â year, 8Â months ago\nSelected Answer: B\nGoogle Cloud Storage (GCS) is the recommended service for storing unstructured data like files, images, and backups. If you have large quantities of unstructured data in different file formats that need to be processed with ETL (Extract, Transform, Load) transformations and then processed by a Dataflow job, the typical workflow is to store the raw data in Cloud Storage.\nOnce the data is in Cloud Storage, you can easily access it and perform ETL transformations using Google idk_4 1Â year, 6Â months ago\nGuys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you"
      },
      {
        "index": 3,
        "text": "Anonymous: idk_4 Most Recent 1Â year, 6Â months ago\nGuys, do you know which answers are the correct answers on this website? The website's answers or the most voted ones? I'm preparing for the exam and I need a quick and certain response. Thank you"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is B"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nKey term is \"unstructured data in different file formats\". Except B, remaining options are suitable for structured data. So, correct answer is B."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nB is correct for Unstructurd DAta its Cloud storage"
      },
      {
        "index": 7,
        "text": "Anonymous: shivampriya11 1Â year, 11Â months ago\nwhy this page ask for contributer access.. i can not access whole questions"
      },
      {
        "index": 8,
        "text": "Anonymous: Nxt_007 1Â year, 11Â months ago\nSelected Answer: B\nOption B is correct\nCloud Storage is a scalable and cost-effective object storage service that can hold unstructured data of various file formats. Before performing ETL (Extract, Transform, Load) transformations, it's often beneficial to store the raw data in a centralized location, like Cloud Storage."
      },
      {
        "index": 9,
        "text": "Anonymous: sabrinakloud 2Â years, 3Â months ago\nSelected Answer: B\nAnswer B"
      },
      {
        "index": 10,
        "text": "Anonymous: Untamables 2Â years, 9Â months ago\nSelected Answer: B\nCloud Storage as a datalake"
      }
    ]
  },
  {
    "id": 30,
    "source": "examtopics",
    "question": "You need to manage multiple Google Cloud projects in the fewest steps possible. You want to configure the Google Cloud SDK command line interface (CLI) so that you can easily manage multiple projects. What should you do?",
    "options": {
      "A": "1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.",
      "B": "1. Create a configuration for each project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project",
      "C": "1. Use the default configuration for one project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects.",
      "D": "1. Use the default configuration for one project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SSunny Highly Voted 3Â years, 10Â months ago\nA\nCloud SDK comes with a default configuration. To create multiple configurations, use gcloud config configurations create, and gcloud config configurations activate to switch between them.\nhttps://cloud.google.com/sdk/gcloud/reference/config/set"
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 3Â years, 10Â months ago\nA. 1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects."
      },
      {
        "index": 3,
        "text": "Anonymous: aalllkk Most Recent 1Â year, 1Â month ago\nThe correct answer is A"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nThe gcloud config command group lets you set, view and unset properties used by Google Cloud CLI. A configuration is a set of properties that govern the behavior of gcloud and other Google Cloud CLI tools. The initial default configuration is set when gcloud init is run. You can create additional named configurations using gcloud init or gcloud config configurations create. To switch between configurations, use gcloud config configurations activate.\nhttps://cloud.google.com/sdk/gcloud/reference/config\nhttps://cloud.google.com/sdk/gcloud/reference/config/configurations/activate"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct answer, as it comes with the default cofiguration and you dont need to update it"
      },
      {
        "index": 7,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nA. 1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects. kINDLY SHARE COMPLETE QUESTION"
      },
      {
        "index": 8,
        "text": "Anonymous: xaqanik 1Â year, 11Â months ago\nSelected Answer: A\nGoogle Cloud SDK allows you to create multiple configurations for different projects, and you can easily switch between these configurations as needed. To manage multiple projects efficiently, you can create a separate configuration for each project and activate the appropriate configuration when you work with each assigned project. The gcloud config configurations create and gcloud config configurations activate commands allow you to create and activate different configurations. By using different configurations, you can ensure that your CLI commands are always executed in the correct context and against the correct project, without the need to manually change the configuration each time you switch projects."
      },
      {
        "index": 9,
        "text": "Anonymous: VietmanOfficiel 2Â years, 4Â months ago\nSelected Answer: A"
      },
      {
        "index": 1,
        "text": "Generate your configurations with \"gcloud config configurations create <config_id> ...\" then activate the one you need according to the project you are working on with \"gcloud config activate <config_id>\""
      }
    ]
  },
  {
    "id": 31,
    "source": "examtopics",
    "question": "Your managed instance group raised an alert stating that new instance creation has failed to create new instances. You need to maintain the number of running instances specified by the template to be able to process expected application traffic. What should you do?",
    "options": {
      "A": "Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names.",
      "B": "Create an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template.",
      "C": "Verify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template.",
      "D": "Delete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: TAvenger Highly Voted 4Â years, 11Â months ago\nReally tricky question.\nIdeal scenario would be"
      },
      {
        "index": 1,
        "text": "create new template, while creating ensure that in the new template disks.autoDelete=true, 3. delete existing persistent disks, 4. make rolling update ...\nIn order to switch to new template we need \"Rolling update\". Unfortunately, it is not mentioned.\nWith current options\nC - not correct, we cannot update existing template\nD - not correct, we cannot delete existing template when it is in use (just checked in GCP) (We need rolling update)\nB - will not solve our problem without Rolling update\nA - This is the only option (I know that it can be temporary) that will work without Rolling update according to\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-migs ShakthiGCP 4Â years, 10Â months ago\nWell reasoned. I'm also going with A. FunkyB 2Â years, 11Â months ago\nThank you for providing the link. ovokpus 2Â years, 3Â months ago\nC - you were not told to update existing template. You were told to verify the syntax is correct."
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (C):\nYour instance template has set the disks.autoDelete option to false for boot persistent disks so that when a VM has been deleted (for example, because of autohealing), the persistent disk was not deleted. When the managed instance group attempted to recreate the VM with the same name, it ran into the same issue where a persistent disk already exists with the same name. Delete the existing persistent disk to resolve the immediate problem and update the instance template to set the disks.autoDelete to true if you would like boot persistent disks to be deleted alongside the instance\nhttps://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#troubleshooting magistrum 5Â years ago\nCan't update instance templates, see below JackGlemins 4Â years, 10Â months ago\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      },
      {
        "index": 3,
        "text": "Anonymous: guaose Most Recent 2Â months, 2Â weeks ago\nSelected Answer: D\nWhy the other options are less ideal:\nA and B: They donâ€™t mention updating the instance template or setting autoDelete, which is crucial to prevent future disk name conflicts.\nC: While it includes autoDelete, it doesnâ€™t mention updating the template or verifying name uniqueness, which is necessary to resolve the issue."
      },
      {
        "index": 4,
        "text": "Anonymous: dead1407 4Â months, 2Â weeks ago\nSelected Answer: C\nInstance creation can fail if there are naming conflicts between instance names and persistent disk names, or if disks are not set to auto-delete and already exist. Verifying the template, deleting conflicting disks, and ensuring disks.autoDelete is set to true will help maintain the correct number of running instances as specified by the template."
      },
      {
        "index": 5,
        "text": "Anonymous: Comunidad 7Â months, 2Â weeks ago\nSelected Answer: C\nOpciÃ³n AcciÃ³n Comentario\nA Crear un nuevo instance template con sintaxis vÃ¡lida y eliminar discos persistentes con el mismo nombre que las instancias. Correcto, pero incompleto: no aborda el problema recurrente, solo lo evita momentÃ¡neamente.\nC Verificar que el template actual tiene sintaxis vÃ¡lida, eliminar discos persistentes conflictivos, y configurar disks.autoDelete = true. âœ… SoluciÃ³n completa: evita el problema ahora y previene que ocurra de nuevo."
      },
      {
        "index": 6,
        "text": "Anonymous: sabbella 10Â months ago\nSelected Answer: C\nIf you donâ€™t set autoDelete: true, disks from deleted VMs will stick around\nGoogle recommends setting autoDelete: true in MIG templates to avoid this exact scenario"
      },
      {
        "index": 7,
        "text": "Anonymous: yomi95 1Â year, 2Â months ago\nSelected Answer: A\nFrom my understanding this VM creation issue is based on boot disk from a previous failed VM exists within the MIG making the MIG unable to create a New VM for that failed VM (Auto healing). all the answers are based on this solution. (There might be other issues as per below document which needs other solutions).\nBut the document states below for boot disk issue scenario in a MIG,\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-migs\n\"The boot disk already exists\nBy default, a new boot persistent disk is created when you create an instance. The name of the boot disk matches the name of the VM. If you name a VM my-instance, the disk is also named my-instance. If a persistent disk already exists with that name, the request fails. To resolve this issue, you can optionally take a snapshot, and then delete the existing persistent disk.\""
      },
      {
        "index": 8,
        "text": "Anonymous: master9 1Â year, 4Â months ago\nSelected Answer: C\nEnsure the template is valid.\nPersistent disk naming: Avoid naming disks the same as instances.\nAutomatic deletion: Set disks.autoDelete to true in the template for automatic disk removal."
      },
      {
        "index": 9,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nIs not C\n\"Instance templates are designed to create instances with identical configurations. So you cannot update an existing instance template or change an instance template after you create it.\"\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates"
      }
    ]
  },
  {
    "id": 32,
    "source": "examtopics",
    "question": "Your company is moving from an on-premises environment to Google Cloud. You have multiple development teams that use Cassandra environments as backend databases. They all need a development environment that is isolated from other Cassandra instances. You want to move to Google Cloud quickly and with minimal support effort. What should you do?",
    "options": {
      "A": "1. Build an instruction guide to install Cassandra on Google Cloud. 2. Make the instruction guide accessible to your developers.",
      "B": "1. Advise your developers to go to Cloud Marketplace. 2. Ask the developers to launch a Cassandra image for their development work.",
      "C": "1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Use the snapshot to create instances for your developers.",
      "D": "1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Upload the snapshot to Cloud Storage and make it accessible to your developers. 3. Build instructions to create a Compute Engine instance from the snapshot so that developers can do it themselves."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years ago\nB is correct for me.. launch a solution from marketplace"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (B):\nhttps://medium.com/google-cloud/how-to-deploy-cassandra-and-connect-on-google-cloud-platform-with-a-few-clicks-11ee3d7001d1 nightflyer 4Â years, 7Â months ago\nBut we are moving from on premises to gcp xtian2900 4Â years, 10Â months ago\nthanks, i always look for your insight"
      },
      {
        "index": 3,
        "text": "Anonymous: bubidubi Most Recent 1Â year, 5Â months ago\nFor me the answer is B for no other reason than it mentions Marketplace, which I imagine google wants you to think about and it's what the majority had selected as well."
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 1Â year, 8Â months ago\nSelected Answer: B\nB\nWith minimal support - a click away:\nhttps://console.cloud.google.com/marketplace/product/bitnami-launchpad/cassandra?project=fast-art-401415"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nKey term is \"move to Google Cloud quickly and with minimal support effort\". Right away you can think of Google Cloud Marketplace in such situations."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct answer as , it requires the minimal effoort"
      },
      {
        "index": 7,
        "text": "Anonymous: creativenets 2Â years, 1Â month ago\nSelected Answer: D\nD is the right answer not B. Remember the question states moving from on premise to gcloud. Meaning we already have servers build. We would need those image or snapshot so devs can create their own insance.\nA and B - start from scratch\nC - Doesnt give permissions to devs so you'll have to create the instance (more support effort) geeroylenkins 2Â years ago\nnah - D is not the right answer. D says build a Cassandra CE instance. The answer does not refer to taking a snapshot of the on-prem instances, it refers to a snapshot of a Compute Engine VM which you'd have to manually build.\nB is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: Untamables 2Â years, 9Â months ago\nSelected Answer: B\nB\nhttps://cloud.google.com/blog/products/databases/open-source-cassandra-now-managed-on-google-cloud"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nB absolutely correct, there is no need of manual installs."
      },
      {
        "index": 10,
        "text": "Anonymous: pfabio 3Â years, 2Â months ago\nSelected Answer: B\nB is correct: You want to move to Google Cloud quickly and with minimal support effort.\nhttps://cloud.google.com/marketplace"
      }
    ]
  },
  {
    "id": 33,
    "source": "examtopics",
    "question": "You have a Compute Engine instance hosting a production application. You want to receive an email if the instance consumes more than 90% of its CPU resources for more than 15 minutes. You want to use Google services. What should you do?",
    "options": {
      "A": "1. Create a consumer Gmail account. 2. Write a script that monitors the CPU usage. 3. When the CPU usage exceeds the threshold, have that script send an email using the Gmail account and smtp.gmail.com on port 25 as SMTP server.",
      "B": "1. Create a Cloud Monitoring Workspace and associate your Google Cloud Platform (GCP) project with it. 2. Create a Cloud Monitoring Alerting Policy that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.",
      "C": "1. Create a Cloud Monitoring Workspace and associate your GCP project with it. 2. Write a script that monitors the CPU usage and sends it as a custom metric to Cloud Monitoring. 3. Create an uptime check for the instance in Cloud Monitoring.",
      "D": "1. In Cloud Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})% 2. In Cloud Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Bobbybash Highly Voted 2Â years, 5Â months ago\nSelected Answer: B\nB\nBy setting up a Cloud Monitoring Alerting Policy and configuring the notification channel to an email address, you can receive email alerts when the CPU usage exceeds the threshold that you set. This can be done by using Google Cloud's built-in monitoring and alerting features, which can be more reliable and easier to manage than setting up a custom script or using external email services."
      },
      {
        "index": 2,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\nnow, 2024, I think is not necessary to create workspace in monitoring... ccpmad 1Â year, 1Â month ago\nCloud Monitoring metric scope is now"
      },
      {
        "index": 3,
        "text": "Anonymous: dgmon174 1Â year, 6Â months ago\nWhat is up with blatantly wrong options being marked as the \"correct answer\"??"
      },
      {
        "index": 4,
        "text": "Anonymous: cooldude26 1Â year, 8Â months ago\nSelected Answer: B\nThe most appropriate and efficient method to set up an alert for high CPU usage on a Google Compute Engine instance using Google's services would be:\nB."
      },
      {
        "index": 1,
        "text": "Create a Cloud Monitoring Workspace and associate your Google Cloud Platform (GCP) project with it."
      },
      {
        "index": 2,
        "text": "Create a Cloud Monitoring Alerting Policy that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.\nThis option utilizes Google Cloud's built-in Cloud Monitoring service to track the CPU usage and send alerts based on predefined conditions without the need for scripting or managing an email server."
      },
      {
        "index": 5,
        "text": "Anonymous: LDAP_Anand 2Â years, 6Â months ago\nD. 1. In Cloud Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})% 2. In Cloud Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel."
      },
      {
        "index": 6,
        "text": "Anonymous: u422628 2Â years, 6Â months ago\nSelected Answer: B\nAgree, B works"
      },
      {
        "index": 7,
        "text": "Anonymous: fragment137 2Â years, 7Â months ago\nSelected Answer: B\nAgree it's B if we're talking about StackDriver."
      },
      {
        "index": 8,
        "text": "Anonymous: anolive 2Â years, 9Â months ago\nSelected Answer: B\nis correct"
      }
    ]
  },
  {
    "id": 34,
    "source": "examtopics",
    "question": "You have an application that uses Cloud Spanner as a backend database. The application has a very predictable traffic pattern. You want to automatically scale up or down the number of Spanner nodes depending on traffic. What should you do?",
    "options": {
      "A": "Create a cron job that runs on a scheduled basis to review Cloud Monitoring metrics, and then resize the Spanner instance accordingly.",
      "B": "Create a Cloud Monitoring alerting policy to send an alert to oncall SRE emails when Cloud Spanner CPU exceeds the threshold. SREs would scale resources up or down accordingly.",
      "C": "Create a Cloud Monitoring alerting policy to send an alert to Google Cloud Support email when Cloud Spanner CPU exceeds your threshold. Google support would scale resources up or down accordingly.",
      "D": "Create a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 10Â months ago\nD. Create a Cloud Monitoring alerting policy to send an alert to webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly."
      },
      {
        "index": 2,
        "text": "Anonymous: theBestStudent Highly Voted 3Â years, 5Â months ago\nSelected Answer: D\nWithout knowing that much, you can discard easily B,C as they don't make any sense. Automation should be a key in this answer. Also you should discard \"A\" as with a CronJob you won't spann on time as it will be a fixed time checking. So the only one that is left is D, as just creating an alert and sending it to \"something else\" (in this case a webhook) in an automated way, should be the common sense way of handling this. FeaRoX 2Â years, 11Â months ago\nIsn't \"fixed time chacking\" appropriate for quote : very predictable traffic pattern? space_cadet 2Â years, 10Â months ago\nCrossed my mind too, but why check every time when you can trigger a response when it happens.\nPredictability can also be used to determine the threshold."
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE Most Recent 1Â year, 6Â months ago\nWhy not A if traffic very predictable?"
      },
      {
        "index": 4,
        "text": "Anonymous: cooldude26 2Â years, 2Â months ago\nSelected Answer: D\nThe most suitable approach to automatically scale the number of Cloud Spanner nodes based on predictable traffic patterns is:\nD. Create a Cloud Monitoring alerting policy to send an alert to a webhook when Cloud Spanner CPU is over or under your threshold. Create a Cloud Function that listens to HTTP and resizes Spanner resources accordingly.\nThis option utilizes Cloud Monitoring alerts and Cloud Functions to dynamically scale Cloud Spanner resources based on CPU thresholds, providing an automated and responsive solution."
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: D\nCorrect answer is D."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD is the correct Answer as B or C does not do it automatically, and a doesnot use for long spanning"
      },
      {
        "index": 7,
        "text": "Anonymous: temppp 3Â years ago\nWhy not A is correct as question suggested specific time where as D is like an unpredectiable time? jrisl1991 2Â years, 11Â months ago\nBecause even though the traffic has a clear pattern, if the traffic changes one day (like a special holiday for ecommerce websites), you wouldn't be able to serve accordingly. It's never a good practice use fixed jobs for time-based traffic issues."
      },
      {
        "index": 8,
        "text": "Anonymous: roaming_panda 3Â years, 1Â month ago\nD is definitely correct .. people"
      },
      {
        "index": 9,
        "text": "Anonymous: abirroy 3Â years, 5Â months ago\nSelected Answer: D\nCorrect answer is D"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years, 6Â months ago\nD is correct, It is part of Tutorials Dojo practice test"
      }
    ]
  },
  {
    "id": 35,
    "source": "examtopics",
    "question": "Your company publishes large files on an Apache web server that runs on a Compute Engine instance. The Apache web server is not the only application running in the project. You want to receive an email when the egress network costs for the server exceed 100 dollars for the current month as measured by Google Cloud.\nWhat should you do?",
    "options": {
      "A": "Set up a budget alert on the project with an amount of 100 dollars, a threshold of 100%, and notification type of ×’â‚¬email.×’â‚¬",
      "B": "Set up a budget alert on the billing account with an amount of 100 dollars, a threshold of 100%, and notification type of ×’â‚¬email.×’â‚¬",
      "C": "Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.",
      "D": "Use the Cloud Logging Agent to export the Apache web server logs to Cloud Logging. Create a Cloud Function that uses BigQuery to parse the HTTP response log data in Cloud Logging for the current month and sends an email if the size of all HTTP responses, multiplied by current Google Cloud egress prices, totals over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nC. Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly."
      },
      {
        "index": 2,
        "text": "Anonymous: MyName_ Highly Voted 4Â years, 3Â months ago\n[C]\nI think the keyword here is \"as measured by Google Cloud\". In Answer D you calculate the price yourself, in C you use the billing provided by GCP. Thus I think the Answer is C."
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: C\nSince the Apache server is not the only application in the project, a project- or billing-account-wide budget alert would not be specific to the server. Exporting billing data to BigQuery allows you to filter and sum egress costs specifically for the Compute Engine instance running Apache. Automating this with a Cloud Function and Cloud Scheduler ensures you get notified when the threshold is exceeded."
      },
      {
        "index": 4,
        "text": "Anonymous: bubidubi 1Â year, 5Â months ago\nI would never had guessed C or D. I thought it's A, as it's the most straight forward. C or D are ridiculously complex to me. omunoz 1Â year, 2Â months ago\nYou need to filter only Network egress costs, so C make more sense..."
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: C\nCorrect answer is C."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC is the correct answer , as it gives you everything the question wants"
      },
      {
        "index": 7,
        "text": "Anonymous: Shenannigan 2Â years, 2Â months ago\nSelected Answer: A\nAnswer is A\nFrom this Link:\nhttps://cloud.google.com/load-balancing/docs/ssl\nit states this:\nExternal SSL proxy load balancers are intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use an external HTTP(S) load balancer.\n443 is HTTPS traffic\nfor those saying 443 isn't https\nhttps://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=443 Shenannigan 2Â years, 2Â months ago\nDisregard this was supposed to be for question 127"
      },
      {
        "index": 8,
        "text": "Anonymous: HiddenClouds 2Â years, 7Â months ago\nSelected Answer: C\nTook the exam this week this question is on there, C is the correct answer you need to remember to filter."
      },
      {
        "index": 9,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: C\nC is the correct answer,\nYou export the bill to BigQuery and filter for the Egress cost for the particular application, and send an email if the cost is over 100 dollars, to send an email you need to use cloud function to monitor and trigger based on the conditions."
      },
      {
        "index": 10,
        "text": "Anonymous: sylva91 2Â years, 10Â months ago\nSelected Answer: C\nit can only be the C because \"it's not the only app running\""
      }
    ]
  },
  {
    "id": 36,
    "source": "examtopics",
    "question": "You have designed a solution on Google Cloud that uses multiple Google Cloud products. Your company has asked you to estimate the costs of the solution. You need to provide estimates for the monthly total cost. What should you do?",
    "options": {
      "A": "For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product.",
      "B": "For each Google Cloud product in the solution, review the pricing details on the products pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product.",
      "C": "Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Cloud Console. Multiply the 1 week cost to determine the monthly costs.",
      "D": "Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Use Cloud Monitoring to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_user Highly Voted 4Â years, 4Â months ago\nYes I agree with A. Makes more sense. BobbyFlash 3Â years, 8Â months ago\nWhy not B?? Even though answer A makes sense, they are also stating to provide estimates for \"monthly total costs\". One would think that it is not only necessary to get estimates from every resource, but also consolidate them to inform the monthly total cost required. BobbyFlash 3Â years, 8Â months ago\nIgnore the comment folks. Sentence B is missing the little thing about the pricing calculator. I go with A."
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nA. For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product."
      },
      {
        "index": 3,
        "text": "Anonymous: Charlyescot Most Recent 1Â year, 5Â months ago\nvoy con A, si alguien tiene el listado completo de preguntas, me las envia a escotorin83@hotmail.com , rindo en 10 dias gracias!"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: A\nCorrect answer is A. Use GCP pricing calculator."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA seems more correct"
      },
      {
        "index": 6,
        "text": "Anonymous: Untamables 2Â years, 9Â months ago\nSelected Answer: A\nVote A\nhttps://cloud.google.com/free/docs/estimate-costs-google-cloud-platform"
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: A\nA is the correct answer, use the pricing calculator to estimate the pricing for a month and download the estimate to csv file, or you can share the URL of the pricing calculator or email the estimate to the respective people in the company."
      },
      {
        "index": 8,
        "text": "Anonymous: VietmanOfficiel 2Â years, 10Â months ago\nSelected Answer: A\n[A]\nWhen \"estimate\" you need to read \"price calculator\""
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nI agree with A"
      },
      {
        "index": 10,
        "text": "Anonymous: dishum 3Â years, 6Â months ago\nAnswer is C\nThe question says 'cost of solution of the design' means, how it is going to work in practical, means the traffic load, no of users, uploads, downloads, transcations etc etc.\nIn this case, the more nearer option is to run for a week, then calculate.\nAnswer A is not correct, becoz A is applicable when the cost of the product is to be determined. PiperMe 1Â year, 5Â months ago\nC and D are not optimal due to deployment costs. Provisioning the solution for an extended period just to estimate cost incurs actual expenses. Additionally, a week may not adequately capture fluctuations in usage patterns over a full month. SunnyDey 2Â years, 8Â months ago\nQuestion has asked to estimate the costs of the solution i.e. the initial setup cost not the running cost. Therefore, C and D discarded and B is of no use without pricing calculator. Question is testing whether you are aware of ths calculator service from Google."
      }
    ]
  },
  {
    "id": 37,
    "source": "examtopics",
    "question": "You have an application that receives SSL-encrypted TCP traffic on port 443. Clients for this application are located all over the world. You want to minimize latency for the clients. Which load balancing option should you use?",
    "options": {
      "A": "HTTPS Load Balancer",
      "B": "Network Load Balancer",
      "C": "SSL Proxy Load Balancer",
      "D": "Internal TCP/UDP Load Balancer. Add a firewall rule allowing ingress traffic from 0.0.0.0/0 on the target instances."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Bituz Highly Voted 5Â years, 5Â months ago\nSSL Proxy Load Balancing support for the following ports: 25, 43, 110, 143, 195, 443, 465, 587, 700, 993, 995, 1883, 3389, 5222, 5432, 5671, 5672, 5900, 5901, 6379, 8085, 8099, 9092, 9200, and 9300. When you use Google- managed SSL certificates with SSL Proxy Load Balancing, the frontend port for traffic must be 443 to enable the Google-managed SSL certificates to be provisioned and renewed."
      },
      {
        "index": 2,
        "text": "Anonymous: DarioFama23 Highly Voted 5Â years, 6Â months ago\nC is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: C\nThe SSL Proxy Load Balancer is designed to handle SSL-encrypted TCP traffic (such as on port 443) and provides global load balancing with Googleâ€™s worldwide edge network. It minimizes latency by terminating SSL connections at the edge closest to the client and then forwarding traffic to your backend instances. The HTTPS Load Balancer is for HTTP/HTTPS (Layer 7) traffic, while the SSL Proxy Load Balancer is for SSL/TLS over TCP (Layer 4) traffic."
      },
      {
        "index": 4,
        "text": "Anonymous: 85c887f 9Â months, 3Â weeks ago\nSelected Answer: C\nSSL Proxy Load Balancer is specifically for SSL-encrypted TCP traffic. As for port 443 it is clearly stated in question, that application \"receives SSL-encrypted TCP traffic on port 443\", not HTTP(S) traffic. So we can use 443 port for TCP traffic (tcp:443). https://cloud.google.com/load-balancing/docs/ssl/setting-up-ssl#proxy-protocol"
      },
      {
        "index": 5,
        "text": "Anonymous: yomi95 1Â year, 2Â months ago\nSelected Answer: A\nAs per the GCP document below,\n\"Proxy Network Load Balancers are intended for TCP traffic only, with or without SSL. For HTTP(S) traffic, we recommend that you use an Application Load Balancer instead.\"\nhttps://cloud.google.com/load-balancing/docs/proxy-network-load-balancer\nhence even if both A and C are capable to carter for this requirement, the Google recommendation is A. As per my understanding both A and C can divert client to closest backend service reducing the latency.\nAlso in 2024, can refer to below as well.\n\"HTTP/3 on your load balancer can improve web page load times, reduce video rebuffering, and improve throughput on higher latency connections.\"\nhttps://cloud.google.com/load-balancing/docs/https"
      },
      {
        "index": 6,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nhttps://cloud.google.com/load-balancing/docs/tcp\nNote: Although external proxy Network Load Balancers can support HTTPS traffic, you should use an external Application Load Balancer for HTTPS traffic instead. External Application Load Balancers support a number of HTTP-specific features, including routing by HTTP request path and balancing by request rate."
      },
      {
        "index": 7,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: A\nI remember compliance question was in \"Associate Cloud Engineer Certification Learning Path\". And I answered SSL Proxy Load Balancer but it was incorrect. Correct answer is A."
      },
      {
        "index": 8,
        "text": "Anonymous: cooldude26 2Â years, 2Â months ago\nSelected Answer: C\nC. SSL Proxy Load Balancer\nThe SSL Proxy Load Balancer is designed specifically for SSL-encrypted traffic and provides SSL termination, minimizing latency for clients worldwide by handling SSL connections efficiently. This load balancer is suitable for applications that receive SSL-encrypted TCP traffic on port 443, making it a good choice for the scenario."
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: C\nCorrect answer is C.\nExternal proxy load balancer supports global and regional scope. While external passthrough network load balancer supports regional scope."
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC is the right answer , read it carefully TCP traffic yayalm 1Â year, 11Â months ago\nIt's not about TCP Traffic. It's about minimizing latency, so in order to achieve that we need to use SSL termination which is a feature of the SSL Proxy LB. yayalm 1Â year, 11Â months ago\nI think i'm wrong cuz HTTPS use TCP and SSL, because HTTPS is HTTP over TLS/SSL(now) so the ans in my pov is A"
      }
    ]
  },
  {
    "id": 38,
    "source": "examtopics",
    "question": "You have an application on a general-purpose Compute Engine instance that is experiencing excessive disk read throttling on its Zonal SSD Persistent Disk. The application primarily reads large files from disk. The disk size is currently 350 GB. You want to provide the maximum amount of throughput while minimizing costs.\nWhat should you do?",
    "options": {
      "A": "Increase the size of the disk to 1 TB.",
      "B": "Increase the allocated CPU to the instance.",
      "C": "Migrate to use a Local SSD on the instance.",
      "D": "Migrate to use a Regional SSD on the instance."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: DarioFama23 Highly Voted 4Â years, 6Â months ago\nC is correct, local SSD has more IOPS RegisFTM 3Â years ago\ntrick question... locak-ssd is not persistent. increasing the size of of the disk will also increase the iops. A is correct imho. obeythefist 2Â years, 10Â months ago\nI once thought that A was the correct response because of the persistence problem, but reading the question carefully, we must choose C over A.\nThe question does not stipulate that the local files must be persistent, and this is the only reason why you would choose \"A\" over \"C\".\nAlso, the question has an important key word: Minimising costs.\n1TB of zonal persistent disk costs a huge amount more than 350GB of local disk.\nWe should choose C. pfabio 2Â years, 8Â months ago\nVery Nice, agree C is correct shanx910 2Â years, 12Â months ago\nLocal SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The data that you store on a local SSD persists only until the instance is stopped or deleted. TAvenger 3Â years, 11Â months ago\nAgree. This is also cheaper than having 350 Gb persistent SSD:\nHere are calculations (taken from GCP when creating instance)\n350 Gb SSD Persistent disk: 59.50$/month, read IOPS: 10 500 with n1-standard-1\n1000 Gb SSD Persistent disk: 170.00$/month, read IOPS: 15 000 with n1-standard-1\n375 Gb Local SSD (NVMe): 30.00$/month, read IOPS: 170 000 with n1-standard-1"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (C):\nPerformance\nStandard persistent disks are efficient and economical for handling sequential read/write operations, but they aren't optimized to handle high rates of random input/output operations per second (IOPS). If your apps require high rates of random IOPS, use SSD persistent disks. SSD persistent disks are designed for single-digit millisecond latencies. Observed latency is application specific.\nhttps://cloud.google.com/compute/docs/disks#performance hogtrough 2Â years, 11Â months ago\nA local SSD is not the same as an SSD persistent disk.\n\"Local SSDs are physically attached to the server that hosts your VM instance. Local SSDs have higher throughput and lower latency than standard persistent disks or SSD persistent disks. The data that you store on a local SSD persists only until the instance is stopped or deleted.\"\nhttps://cloud.google.com/compute/docs/disks#localssds\nThe answer is C. hogtrough 2Â years, 11Â months ago\nSorry, A not C."
      },
      {
        "index": 3,
        "text": "Anonymous: guaose Most Recent 2Â months, 2Â weeks ago\nSelected Answer: A\nZonal SSD Persistent Disks in Google Cloud scale read and write throughput linearly with disk size.\nAt 350 GB, your disk has limited throughput. By increasing the size to 1 TB, you significantly increase the read IOPS and throughput, which helps reduce disk read throttling.\nThis approach is cost-effective because:\nYou stay within the same disk type (Zonal SSD).\nYou avoid the higher cost of Local SSDs or Regional SSDs unless absolutely necessary."
      },
      {
        "index": 4,
        "text": "Anonymous: dead1407 4Â months, 2Â weeks ago\nSelected Answer: A\nOn Google Cloud, the throughput and IOPS of a Zonal SSD Persistent Disk scale with the disk size. By increasing the disk size (e.g., to 1 TB), you increase the available throughput and IOPS, which will help reduce disk read throttling. This is a cost-effective solution compared to using Local SSDs (which are more expensive and ephemeral) or Regional SSDs (which are for high availability, not performance). Increasing CPU does not affect disk throughput."
      },
      {
        "index": 5,
        "text": "Anonymous: 85c887f 9Â months, 3Â weeks ago\nSelected Answer: A\nCorrect A. A Local SSD is generally more expensive than a Zonal SSD Persistent Disk. And The ephemeral nature of Local SSDs can lead to additional costs for data replication or backups. So C is a good option for high throughput, but in terms of achieving increased throughput for the least amount of cost, A is the better option."
      },
      {
        "index": 6,
        "text": "Anonymous: cooldude26 1Â year, 2Â months ago\nSelected Answer: C\nC. Migrate to use a Local SSD on the instance.\nLocal SSDs provide high-throughput, low-latency storage that is physically attached to the instance. This can be beneficial for applications experiencing excessive disk read throttling, especially when dealing with large files. Local SSDs are ideal for temporary data that can be recomputed or regenerated if lost, and they can offer improved performance compared to Zonal SSD Persistent Disks.\n(https://cloud.google.com/compute/docs/disks/local-ssd#create_local_ssd_instance) for detailed instructions."
      },
      {
        "index": 7,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: C\nLocal SSD provides more throughput than Persistent disks and is cost effective solution. So, correct answer is C."
      },
      {
        "index": 8,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: C\nC. Migrate to use a Local SSD on the instance. Local SSDs provide higher throughput and lower latency compared to Zonal SSD Persistent Disks, and are optimized for use cases that require high-speed, temporary storage. They are physically attached to the instance, so network latencies are minimized. However, they are not intended for long-term storage and may not provide the same level of durability as persistent disks. Since the application is primarily reading large files from disk and experiencing disk read throttling, using a Local SSD should provide a significant improvement in performance while minimizing costs. Increasing the size of the Zonal SSD Persistent Disk or increasing the allocated CPU to the instance may provide some improvement, but are unlikely to fully address the disk read throttling issue. Migrating to a Regional SSD is also not likely to improve performance significantly, as the disk is still separate from the instance and network latencies can impact performance."
      },
      {
        "index": 9,
        "text": "Anonymous: researched_answer_boi 1Â year, 12Â months ago\nAccording to the page containing the tables and to the tables \"https://cloud.google.com/compute/docs/disks/performance#n1_vms\" and \"https://cloud.google.com/compute/docs/disks/performance#n2_vms\", the number of CPU cores greatly influence the available max. read THROUGHPUT (\"excessive disk read throttling on its Zonal SSD Persistent Disk\", \"The application primarily reads large files from disk.\") on general purpose VMs.\nThe question also requires minimizing the costs, however, as Local SSDs are EPHEMERAL, they are out of question for the scenario at hand.\nSo, answer \"B\" seems to be the correct one."
      },
      {
        "index": 10,
        "text": "Anonymous: AwesomeGCP 2Â years, 3Â months ago\nSelected Answer: C\nC. Migrate to use a Local SSD on the instance"
      }
    ]
  },
  {
    "id": 39,
    "source": "examtopics",
    "question": "Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnet with range 172.16.20.128/25. There are no private IP addresses available in the VPC network. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?",
    "options": {
      "A": "Modify the existing subnet range to 172.16.20.0/24.",
      "B": "Create a new Secondary IP Range in the VPC and configure the VMs to use that range.",
      "C": "Create a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network.",
      "D": "Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrection.\nCorrect Answers is (A):\ngcloud compute networks subnets expand-ip-range\nNAME\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork ccieman2016 3Â years, 4Â months ago\nI think, you can't expand ip range subnet, if there isn't space in VPC. I read this question a lot, VPC CIDR like with 172.16.20.128/25 and there's only one subnet 172.16.20.128/25 inside this VPC, so you can't expand nothing. for me, there's Letter C and D works, but letter D is necessary extra work. LETTER C is right. FeaRoX 2Â years, 11Â months ago\nThere's no information about VPC CIDR, only subnet. You can't tell that there's no space BigMac666 2Â years, 7Â months ago\nVPC's DO NOT have IP range limitations.\nYou can only object if 172.16.20.0/25 is in use in the same VPC or in a VPC that this VPC is already peered with.\n.128/25 expands to .0/24 (i.e. \"backwards\") So as long as it's free, you're good.\nIn a question like this, it's obvious that the simple answer is the right one, i.e. A - Expansion. gastonreppeto77 2Â years, 3Â months ago\n- The statement is clear with point 1A and 2A:\n1A.- \"single Virtual Private Cloud (VPC) network in a single subnet\"\n2A.- \"There are no private IP addresses available in the VPC network.\"\n- Question: How can you expand if there is a single VPC with a single subnet\nand there are no private IP addresses available in the only VPC network ???\n- Result: Yes it has limitation, this question is clear, this is and exam, not the real life, we cannot verify anything else and we have the limitation os the statement."
      },
      {
        "index": 2,
        "text": "Anonymous: francisco_guerra Highly Voted 5Â years, 5Â months ago\nI think is A, passmepls 5Â years, 5Â months ago\nthank you Priyanka109 3Â years, 3Â months ago\nNo it can't be as you can't modify ip address but can expand. There is no ip in the existing vpc so you have to create a new vpc and connect it using peering."
      },
      {
        "index": 3,
        "text": "Anonymous: alex70 Most Recent 9Â months, 3Â weeks ago\nSelected Answer: B\nB is the Best Answer:\nMinimal steps: Just add a secondary range and assign it to new VMs.\nNo downtime: Existing Dataproc cluster remains unaffected.\nSame VPC communication: No need for peering or route changes."
      },
      {
        "index": 4,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nSelected Answer: A\nThe answer is A, but the question is worded badly: \"There are no private IP addresses available in the VPC network. \"\nI think they meant to say there are no remaining private IP addresses left to assign to new virtual machine (VM) instances within the specified subnet of the VPC network.\nit is impossible to suggest that they run out of the class B private IP range because, in the Class B private IP address range, there are approximately 65,534 IP addresses."
      },
      {
        "index": 5,
        "text": "Anonymous: vansbg 1Â year, 1Â month ago\nSelected Answer: C\nI would go with C because \"There are no private IP addresses available in the VPC network.\""
      },
      {
        "index": 6,
        "text": "Anonymous: deskj 1Â year, 3Â months ago\nSelected Answer: A\nThe question asks for the \"minimum number of steps\", not what is wrong or right. A is the \"minimum number of steps\"."
      },
      {
        "index": 7,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "index": 8,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: A\nA is correct answer"
      },
      {
        "index": 9,
        "text": "Anonymous: RKS_2021 1Â year, 4Â months ago\nSelected Answer: A\nExpand the subnet"
      },
      {
        "index": 10,
        "text": "Anonymous: klayhung 1Â year, 4Â months ago\nSelected Answer: A\nThe correct answer is A because increasing the disk size is the simplest way to address the issue. Option C is overly complex and unnecessary."
      }
    ]
  },
  {
    "id": 40,
    "source": "examtopics",
    "question": "You manage an App Engine Service that aggregates and visualizes data from BigQuery. The application is deployed with the default App Engine Service account.\nThe data that needs to be visualized resides in a different project managed by another team. You do not have access to this project, but you want your application to be able to read data from the BigQuery dataset. What should you do?",
    "options": {
      "A": "Ask the other team to grant your default App Engine Service account the role of BigQuery Job User.",
      "B": "Ask the other team to grant your default App Engine Service account the role of BigQuery Data Viewer.",
      "C": "In Cloud IAM of your project, ensure that the default App Engine service account has the role of BigQuery Data Viewer.",
      "D": "In Cloud IAM of your project, grant a newly created service account from the other team the role of BigQuery Job User in your project."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Hjameel Highly Voted 5Â years, 5Â months ago\nI think B is the answer KC_go_reply 2Â years, 7Â months ago\n'I think' could you elaborate further please?"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (B):\nSorry, I copied/pasted the the wrong statement.\nThis is the proper explanation regarding to Big Query Data Viewer Role.\nThe resource that you need to get access is in the other project.\nroles/bigquery.dataViewer BigQuery Data Viewer\nWhen applied to a table or view, this role provides permissions to:\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\nWhen applied to a dataset, this role provides permissions to:\nRead the dataset's metadata and list tables in the dataset.\nRead data and metadata from the dataset's tables.\nWhen applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are necessary to allow the running of jobs."
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 3Â weeks ago\nSelected Answer: B\nWhy B is Correct\nThe application will run its query jobs in your project (Project-A), so it needs the bigquery.jobs.create permission there (which is typically included in BigQuery Job User).\nHowever, that job needs permission to read the data from the other project (Project-B). The BigQuery Data Viewer (roles/bigquery.dataViewer) role provides exactly this permission (bigquery.tables.getData) and is the role with the least privilege for this task.\nTherefore, you must ask the other team to add your service account as a principal in their project and grant it the BigQuery Data Viewer role on their dataset or project.\nWhy A is not correct:\nThe BigQuery Job User role is needed to run jobs. Since your application runs the job in its own project, it doesn't need that permission on the other team's project."
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nBigQuery Data Viewer\n(roles/bigquery.dataViewer)\nWhen applied to a table or view, this role provides permissions to:\nRead data and metadata from the table or view.\nThis role cannot be applied to individual models or routines.\nhttps://cloud.google.com/bigquery/docs/access-control"
      },
      {
        "index": 5,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nSelected Answer: A\nBig Query Data Viewer Role allows to read data only in the Cloud Console, you cannot use SQL to select or aggregate data, I assume the app will need to run jobs. How would you write such an app? Please correct me if I am wrong."
      },
      {
        "index": 6,
        "text": "Anonymous: klayhung 1Â year, 4Â months ago\nSelected Answer: B\nTo read data from a BigQuery dataset, you only need to grant the App Engine Service account the role of BigQuery Data Viewer."
      },
      {
        "index": 7,
        "text": "Anonymous: pzacariasf7 1Â year, 10Â months ago\nSelected Answer: B\nB B B B"
      },
      {
        "index": 8,
        "text": "Anonymous: abhi2704 1Â year, 10Â months ago\nSelected Answer: B\nAs the question clearly says - 'read'\n'You do not have access to this project, but you want your application to be able to READ data from the BigQuery dataset'\nAwnser is B"
      },
      {
        "index": 9,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: B\nA is too permissive. This role includes BigQuery Data Viewer permissions but also the ability to create, run, and manage BigQuery jobs. Your application only needs to read data, not manipulate it. B grants read-only access to BigQuery datasets and tables, which perfectly aligns with the requirement of the App Engine service only needing to visualize data."
      },
      {
        "index": 10,
        "text": "Anonymous: Vijay9032 1Â year, 11Â months ago\nSelected Answer: B\nTo see data, you need either BigQuery User or BigQuery Data Viewer roles\nYou CANNOT see data with BigQuery Job User roles sukouto 1Â year, 11Â months ago\nIt's true, the jobUser role does not contain permissions to get datasets or tables. I think with the phrasing we can assume that the data is aggregated within the app--not BigQuery--so the query permission is less important than reading the data.\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser\nhttps://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer"
      }
    ]
  },
  {
    "id": 41,
    "source": "examtopics",
    "question": "You need to create a copy of a custom Compute Engine virtual machine (VM) to facilitate an expected increase in application traffic due to a business acquisition.\nWhat should you do?",
    "options": {
      "A": "Create a Compute Engine snapshot of your base VM. Create your images from that snapshot.",
      "B": "Create a Compute Engine snapshot of your base VM. Create your instances from that snapshot.",
      "C": "Create a custom Compute Engine image from a snapshot. Create your images from that image.",
      "D": "Create a custom Compute Engine image from a snapshot. Create your instances from that image."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (D):\nPreparing your instance for an image\nYou can create an image from a disk even while it is attached to a running VM instance. However, your image will be more reliable if you put the instance in a state that is easier for the image to capture. Use one of the following processes to prepare your boot disk for the image:\nStop the instance so that it can shut down and stop writing any data to the persistent disk.\nIf you can't stop your instance before you create the image, minimize the amount of writes to the disk and sync your file system.\nPause apps or operating system processes that write data to that persistent disk.\nRun an app flush to disk if necessary. For example, MySQL has a FLUSH statement. Other apps might have similar processes.\nStop your apps from writing to your persistent disk.\nRun sudo sync.\nAfter you prepare the instance, create the image.\nhttps://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#prepare_instance_for_image"
      },
      {
        "index": 2,
        "text": "Anonymous: pca2b Highly Voted 4Â years, 9Â months ago\nB:\nwe just need to make 'a copy' of the VM, B works well for that.\nnot D: Had the question mentioned more copies, we would need to go the way of images...templates etc. D will work but not needed here. djgodzilla 4Â years, 7Â months ago\ncustom images are better a fit if its for a new business workload you just acquired wolfie09 3Â years, 7Â months ago\nWhat about the answer that says create your instanceS ??"
      },
      {
        "index": 3,
        "text": "Anonymous: yomi95 Most Recent 1Â year, 2Â months ago\nSelected Answer: D\nhttps://cloud.google.com/compute/docs/images/create-custom"
      },
      {
        "index": 4,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nSelected Answer: D\nThe correct algorithm is:"
      },
      {
        "index": 1,
        "text": "gcloud compute snapshots create"
      },
      {
        "index": 2,
        "text": "gcloud compute images create"
      },
      {
        "index": 3,
        "text": "gcloud compute instances create\nNow, decide which one sounds more accurate for you:\n- B.Â Create a Compute Engine snapshot of your base VM. Create your instances from that snapshot.\n- D.Â Create a custom Compute Engine image from a snapshot. Create your instances from that image.\nFor me, D sounds more accurate, even though we assume we already have a snapshot."
      },
      {
        "index": 5,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: D\nCompute Engine snapshot? What is that?\nThere are snapshots of disks, or there are disk image from disk snapshots...\nbut here we need the VM image. Not the disks..."
      },
      {
        "index": 6,
        "text": "Anonymous: Bagibo 2Â years ago\nSelected Answer: D\nIt is D"
      },
      {
        "index": 7,
        "text": "Anonymous: Kair 2Â years, 1Â month ago\nYou can create custom images from source disks, images, snapshots, or images stored in Cloud Storage and use these images to create virtual machine (VM) instances. Custom images are ideal for situations where you have created and modified a persistent boot disk or specific image to a certain state and need to save that state for creating VMs.\nYou need to create image from the snapshot, so answer is D\nhttps://cloud.google.com/compute/docs/images/create-custom#:~:text=You%20can%20create%20custom%20images,virtual%20machine%20(VM)%20instances."
      }
    ]
  },
  {
    "id": 42,
    "source": "examtopics",
    "question": "You have deployed an application on a single Compute Engine instance. The application writes logs to disk. Users start reporting errors with the application. You want to diagnose the problem. What should you do?",
    "options": {
      "A": "Navigate to Cloud Logging and view the application logs.",
      "B": "Connect to the instance's serial console and read the application logs.",
      "C": "Configure a Health Check on the instance and set a Low Healthy Threshold value.",
      "D": "Install and configure the Cloud Logging Agent and view the logs from Cloud Logging."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: hiteshrup Highly Voted 5Â years, 4Â months ago\nAnswer: D\nApp logs can't be visible to Cloud Logging until we install Cloud Logging Agent on GCE ashrafh 4Â years, 5Â months ago\nHi all\ncheck this document and decide :)\nhttps://cloud.google.com/logging/docs/agent/logging/installation hiteshrup 5Â years, 4Â months ago\nContinuation of reasoning.\nIf Problem statement is not having this statement \"The application writes logs to disk\", then we might assume that application is writing logs on Cloud Logging with google-fluentd agent API library. However, problem statement is clearly mentioned that logs are writing down on disk, we need agent installed on GCE to fetch those logs from disk to Cloud Logging. If that is not desirable, then option B is left hiteshrup 5Â years, 4Â months ago\n(Correction) Answer is A after rethinking and doing some research by focusing words \"App Engine\", which has by default enabled Request Logs which has App logs on each request and those logs are enabled for Cloud Logging ..\nhttps://cloud.google.com/appengine/docs/standard/python/logs#request_logs_vs_application_logs rezavage 5Â years, 4Â months ago\nCloud logging without agent only works for App engine as you stated . but the question is about the compute engine which has to be equipped first with Logging Agent in order to write logs into Cloud Logging. so based your assumption the correct answer is \"D\" Eshkrkrkr 5Â years, 2Â months ago\nWrong! Request Logs has the LIST of App logs and ONLY associated with that request! Read the links you provide! cuban123 5Â years, 1Â month ago\nyou must still install the agent:\nhttps://cloud.google.com/error-reporting/docs/setup/compute-engine#using_logging Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (D):\nIn its default configuration, the Logging agent streams logs from common third-party applications and system software to Logging; review the list of default logs. You can configure the agent to stream additional logs; go to Configuring the Logging agent for details on agent configuration and operation.\nIt is a best practice to run the Logging agent on all your VM instances. The agent runs under both Linux and Windows. To install the Logging agent, go to Installing the agent.\nhttps://cloud.google.com/logging/docs/agent sapguru 5Â years, 4Â months ago\nCloud logging enabled by default for compute engine csrazdan 3Â years, 6Â months ago\nDo you mean the logging agent is installed by default? It depends on the OS you decide. For example, it is installed in Ubuntu but not on RedHat or Windows. Besides installing of the agent is not enough. You have to configure and let the agent know where your application is writing the logs on the disk so that it can monitor and stream the log to cloud monitoring. D is the correct answer ESP_SAP 5Â years, 4Â months ago\nCORRECTION.\nCorrect Answer is (A):\nActivity logging is enabled by default for all Compute Engine projects.\nYou can see your project's activity logs through the Logs Viewer in the Google Cloud Console:\nIn the Cloud Console, go to the Logging page.\nGo to the Logging page\nWhen in the Logs Viewer, select and filter your resource type from the first drop-down list.\nFrom the All logs drop-down list, select compute.googleapis.com/activity_log to see Compute Engine activity logs.\nhttps://cloud.google.com/compute/docs/logging/activity-logs#viewing_logs\nBesides:\nActivity logs are provided as part of the Cloud Logging service. For more information about Logging in general, read the Cloud Logging documentation.\nhttps://cloud.google.com/compute/docs/logging/activity-logs babusartop17 4Â years, 5Â months ago\nI feel sorry for the woman in your life. DamonSalvatore 4Â years, 4Â months ago\nHaha! That was funny mexblood1 5Â years, 4Â months ago\nActivity Logs do not include 2rd party application logs. Activity logs are more related to operations and changes in the infrastructure. This question is tricky, I think it's either D or B, because if it's only an application on a single instance, you can connect to the instance and read the application logs directly and you save the cost of logging agent. mexblood1 5Â years, 4Â months ago\nMaybe I was assuming serial console is the same than system console, technically I guess they're not the same, hence I guess D will be my chosen answer. ESP_SAP 5Â years, 4Â months ago\nAdditional information about VM Image for AWS EC2:\nThe Logging agent streams logs from your VM instances and from selected third-party software packages to Cloud Logging. It is a best practice to run the Logging agent on all your VM instances.\nThe VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent, so you must complete these steps to install it on those instances. The agent runs under both Linux and Windows.\nIf your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image, so you can skip this page. magistrum 5Â years ago\nThis points to D then Load full discussion..."
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: D\nIn a private data center option B is the correct answer, but this is a cloud-based VM, so console access is not automatic, you need to enable it beforehand. once enabled you can access it as follows:\ngcloud compute connect-to-serial-port [INSTANCE_NAME]"
      },
      {
        "index": 4,
        "text": "Anonymous: C0D3LK 1Â year, 4Â months ago\nSelected Answer: D\nQuestions are tricky but let's reiterate this question. Hints are that there's an error in the application and that log are written to disk. Which means, it continues to write to the disk where the instance is functional. Therefore, correct method should be to install the agent and then analyze further on the output of the logs. So, answer is D"
      },
      {
        "index": 5,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: B\nThis question is from 2020, in that year there was an Logging agent, now called Legacy Logging agent. It is the Ops Agent of nowadays.\nWith that agent, yes, you can configurate it to send personalized logs to GCP. But I think this question says that the app is already malfunctioning, so the logs of that are in the disk. For me it is B. ccpmad 1Â year, 7Â months ago\nfor me is b, but there is a recent question 2023: 231 in examtopics.\nAnd D answer is:\n\"D. Install and configure the Ops agent and view the logs from Cloud Logging.\"\nSo to pass the exam, here select D. But for me, the question is confused. Because they want to see past logs, so install ops agent will not show past logs of application."
      },
      {
        "index": 6,
        "text": "Anonymous: moumou 1Â year, 11Â months ago\nSelected Answer: D\nB will be correct if we talk about VM issues (access to an instance's serial console to debug boot and networking issues, troubleshoot malfunctioning instances, interact with the GRand Unified Bootloader (GRUB), and perform other troubleshooting tasks.)"
      },
      {
        "index": 7,
        "text": "Anonymous: SHAAHIBHUSHANAWS 2Â years, 1Â month ago\nSelected Answer: D\nWhen and why do we need it? Serial console access is useful in the following situations:\nWhen the VM is not booting: You can use serial console access to see the boot messages and identify the problem.\nWhen the VM is hung: You can use serial console access to see what the VM is doing and try to unfreeze it.\nWhen you need to access the VMâ€™s BIOS or UEFI: You can use serial console access to access the VMâ€™s BIOS or UEFI, which can be useful for changing settings or troubleshooting problems.\nResolving issues with the VMâ€™s operating system."
      },
      {
        "index": 8,
        "text": "Anonymous: ogerber 2Â years, 1Â month ago\nIts B, App logs are not provided by default and requires to have an agent installed.\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console"
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD makes more sense , as application writes logs to disk and to diagnose it we need the cloud logging agent"
      },
      {
        "index": 10,
        "text": "Anonymous: Praxii 2Â years, 8Â months ago\nThe line \"application writes logs to disk\" is crucial. It means logs are not available in cloud logging to yet. Hence we need to install the logging agent to send the logs to Cloud Logging.\nAnswer is D ccpmad 1Â year, 7Â months ago\nyes you are wright, so you need to see de past logs, so it is B. For the future, after installation of Cloud Logging (now Ops Agent) we will be able to see them in Cloud Logging."
      }
    ]
  },
  {
    "id": 43,
    "source": "examtopics",
    "question": "An application generates daily reports in a Compute Engine virtual machine (VM). The VM is in the project corp-iot-insights. Your team operates only in the project corp-aggregate-reports and needs a copy of the daily exports in the bucket corp-aggregate-reports-storage. You want to configure access so that the daily reports from the VM are available in the bucket corp-aggregate-reports-storage and use as few steps as possible while following Google-recommended practices. What should you do?",
    "options": {
      "A": "Move both projects under the same folder.",
      "B": "Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage.",
      "C": "Create a Shared VPC network between both projects. Grant the VM Service Account the role Storage Object Creator on corp-iot-insights.",
      "D": "Make corp-aggregate-reports-storage public and create a folder with a pseudo-randomized suffix name. Share the folder with the IoT team."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (B):\nPredefined roles\nThe following table describes Identity and Access Management (IAM) roles that are associated with Cloud Storage and lists the permissions that are contained in each role. Unless otherwise noted, these roles can be applied either to entire projects or specific buckets.\nStorage Object Creator (roles/storage.objectCreator) Allows users to create objects. Does not give permission to view, delete, or overwrite objects.\nhttps://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles ESP_SAP 4Â years, 10Â months ago\nBasically, you are giving the permissions to the VM Service Account to create a copy of the daily report on the bucket that the other team has access."
      },
      {
        "index": 2,
        "text": "Anonymous: francisco_guerra Highly Voted 4Â years, 11Â months ago\ni think is B francisco_guerra 4Â years, 11Â months ago\nObject creator cant see object so i think is D lxgywil 4Â years, 2Â months ago\nVM doesn't need to see the obects - just to create them. It's B:\nThe VM is located in project \"corp-iot-insights\" - give its SA the Storage Object Creator role for bucket \"corp-aggregate-reports-storage\" that is located in project \"corp-aggregate-reports\", where your team operates."
      },
      {
        "index": 3,
        "text": "Anonymous: JoseCloudEng1994 Most Recent 1Â year ago\nSelected Answer: B\nIts a bit tricky with the projects. They only specify that the VM must be able to create objects on the bucket. No mention of the people on the aggregate-reports group being able to access it"
      },
      {
        "index": 4,
        "text": "Anonymous: SAMBIT 1Â year, 5Â months ago\nGuys ..shared VPC is the key to connect projects. Enjoy PiperMe 1Â year, 4Â months ago\nIAM provides granular control over object-level access, which is a better security practice than opening up entire network segments with Shared VPC. Granting the necessary Storage Object Creator permission directly to the source VM's service account is the most streamlined way to achieve the file transfer with the principle of least privilege. Hypothetically, if the VM also needed access to databases or other network resources in the corp-aggregate-reports project, then Shared VPC could be the appropriate solution."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct as it gives the service account required access"
      },
      {
        "index": 6,
        "text": "Anonymous: tatyavinchu 1Â year, 11Â months ago\nSelected Answer: B\nCorrect Answer is B"
      },
      {
        "index": 7,
        "text": "Anonymous: Naree 2Â years ago\nSelected Answer: B\nJust take below sentence from the question which is added just for confusion :)\n\"Your team operates only in the project corp-aggregate-reports\""
      },
      {
        "index": 8,
        "text": "Anonymous: StefiJohnson 2Â years, 10Â months ago\nCorrect Answer is (B)"
      },
      {
        "index": 9,
        "text": "Anonymous: theBestStudent 2Â years, 11Â months ago\nSelected Answer: B\nIf that is the default service Account of the Compute Instance, then we should do nothing. As the role is already included. Either way, we should do nothing as the role is already covered. Also we shouldnÂ´t modify Compute instance Service account. But again, I will assume it is not the default."
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years ago\nB is right"
      }
    ]
  },
  {
    "id": 44,
    "source": "examtopics",
    "question": "You built an application on your development laptop that uses Google Cloud services. Your application uses Application Default Credentials for authentication and works fine on your development laptop. You want to migrate this application to a Compute Engine virtual machine (VM) and set up authentication using Google- recommended practices and minimal changes. What should you do?",
    "options": {
      "A": "Assign appropriate access for Google services to the service account used by the Compute Engine VM.",
      "B": "Create a service account with appropriate access for Google services, and configure the application to use this account.",
      "C": "Store credentials for service accounts with appropriate access for Google services in a config file, and deploy this config file with your application.",
      "D": "Store credentials for your user account with appropriate access for Google services in a config file, and deploy this config file with your application."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (B):\nBest practices\nIn general, Google recommends that each instance that needs to call a Google API should run as a service account with the minimum permissions necessary for that instance to do its job. In practice, this means you should configure service accounts for your instances with the following process:\nCreate a new service account rather than using the Compute Engine default service account.\nGrant IAM roles to that service account for only the resources that it needs.\nConfigure the instance to run as that service account.\nGrant the instance the https://www.googleapis.com/auth/cloud-platform scope to allow full access to all Google Cloud APIs, so that the IAM permissions of the instance are completely determined by the IAM roles of the service account.\nAvoid granting more access than necessary and regularly check your service account permissions to make sure they are up-to-date.\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices Ridhanya 4Â years, 1Â month ago\nyou just gave justification for option A which is right ryumada 3Â years, 5Â months ago\nYou should read lxgywil comment. His comment explains how authentication works to access Google Services in your application.\na relevant link also:\nhttps://cloud.google.com/storage/docs/reference/libraries#setting_up_authentication ryumada 3Â years, 5Â months ago\nMaybe for the option A you are modifying the default service account because it's not explain which service account used by the VM, is it the default one or the new one?\nThe best practice is to Create a new service account rather than using the Compute Engine default service account.\nB still has the bigger prove here as the answer. cRobert 5Â years, 1Â month ago\nFrom your quote:\nConfigure the \"instance\" to run as that service account.\nFrom answer B:\nand configure the \"application\" to use this account.\nYou don't add service accounts to applications, ans A magistrum 5Â years ago\nwording is the clue :) TAvenger 4Â years, 11Â months ago\nIt's dirty play with words... All understand that we need custom SA, grant required permissions and attach this SA to the VM...\nWhy Google does this? lxgywil 4Â years, 8Â months ago\nWhen you use a GCP service within your app (code), you have to use its client libraries. When you instantiate a client with client libraries you can pass it a Service Account key, which will define on behalf of which SA the client will be acting. That's how you can configure your app to use a particular service account.\nE.g. https://cloud.google.com/storage/docs/reference/libraries#using_the_client_library akshaydoifode88 3Â years, 2Â months ago\nIn question it's written application uses application default credentials. So taking that as a hint. B is the answer because here we are configuring service account key into the application. Similar approach."
      },
      {
        "index": 2,
        "text": "Anonymous: filco72 Highly Voted 5Â years, 5Â months ago\nI would choose: A. Assign appropriate access for Google services to the service account used by the Compute Engine VM.\nas there is no need to create a new service account. Hjameel 5Â years, 5Â months ago\nI agree, there is no need to create a new service account xaqanik 2Â years, 10Â months ago\nby default a vm uses a default service account. if you grant permission to this service account it will apply to all VMs default service accounts in the project . in this case you need create a new service account and give it appropriate permission"
      },
      {
        "index": 3,
        "text": "Anonymous: Emisu Most Recent 10Â months, 3Â weeks ago\nSelected Answer: A\nA. Assign appropriate access for Google services to the service account used by the Compute Engine VM.\nHere's why:\nApplication Default Credentials (ADC): When running on Compute Engine, the application will use the default service account of the VM. By assigning the appropriate access to this service account, you ensure that the application can authenticate seamlessly without requiring additional configuration changes.\nLeast Privilege Principle: Assigning only the necessary permissions to the service account adheres to security best practices.\nNo Credential Management Hassle: This approach eliminates the need to manage service account keys or include sensitive credentials in configuration files.\nOption B is also a valid option, but it would require additional configuration steps to make the application explicitly use the new service account.\nOptions C and D involve storing credentials in config files, which is not recommended due to security risks."
      },
      {
        "index": 4,
        "text": "Anonymous: jopaca1216 1Â year, 2Â months ago\nSelected Answer: A\nCorrect is A.\nlook.. \"minimal changes\", so can't be the option B."
      },
      {
        "index": 5,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nCreate a new user-managed service account rather than using the Compute Engine default service account, and grant IAM roles to that service account for only the resources and operations that it needs.\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices"
      },
      {
        "index": 6,
        "text": "Anonymous: AchourOussama 1Â year, 4Â months ago\nI think the \"minimal changes\" hint here would make the first option the more suitable one since it doesn't involve creating a new service account."
      },
      {
        "index": 7,
        "text": "Anonymous: ccpmad 1Â year, 7Â months ago\nSelected Answer: A\nIs not possible to add the service accounts to the application"
      },
      {
        "index": 8,
        "text": "Anonymous: pzacariasf7 1Â year, 10Â months ago\nSelected Answer: B\nB is the answer"
      },
      {
        "index": 9,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nI'd strongly lean towards Option B (Create a service account with appropriate access for Google services and configure the application to use this account) as the most likely correct answer.\nGoogle's exams emphasize secure design principles. The principle of least privilege is a core tenet, and custom service accounts embody this. Option B aligns precisely with the best practice for production environments and demonstrates a clear understanding of IAM concepts. While Option A could be acceptable with careful permission adjustments, exams often favor the solution most demonstrably secure and aligned with recommended practice out of the box.\nI believe option A might be a trap. Default service accounts can have varying levels of access. The exam might purposely use this ambiguity to test your knowledge of security principles. Focusing on the step of creating a custom service account signals your understanding of the correct IAM workflow."
      },
      {
        "index": 10,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: B\nWhen you create a new Compute Engine VM, it is assigned a default service account, but this default service account is not unique to each VM. Instead, it's a project-wide default service account."
      }
    ]
  },
  {
    "id": 45,
    "source": "examtopics",
    "question": "You need to create a Compute Engine instance in a new project that doesn't exist yet. What should you do?",
    "options": {
      "A": "Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project.",
      "B": "Enable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project flag to specify a new project.",
      "C": "Using the Cloud SDK, create the new instance, and use the --project flag to specify the new project. Answer yes when prompted by Cloud SDK to enable the Compute Engine API.",
      "D": "Enable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (A):\nQuickstart: Creating a New Instance Using the Command Line\nBefore you begin"
      },
      {
        "index": 1,
        "text": "In the Cloud Console, on the project selector page, select or create a Cloud project."
      },
      {
        "index": 2,
        "text": "Make sure that billing is enabled for your Google Cloud project. Learn how to confirm billing is enabled for your project.\nTo use the gcloud command-line tool for this quickstart, you must first install and initialize the Cloud SDK:"
      },
      {
        "index": 1,
        "text": "Download and install the Cloud SDK using the instructions given on Installing Google Cloud SDK."
      },
      {
        "index": 2,
        "text": "Initialize the SDK using the instructions given on Initializing Cloud SDK.\nTo use gcloud in Cloud Shell for this quickstart, first activate Cloud Shell using the instructions given on Starting Cloud Shell.\nhttps://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli#before-you-begin"
      },
      {
        "index": 2,
        "text": "Anonymous: filco72 Highly Voted 4Â years, 11Â months ago\nI would choose A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project.\nas first I need to create a project. Instance creation cannot automatically create a project."
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nI always think of \"PAP\" to help me with this question: Project: A new project must exist as a container for all your resources.\nAPI: Enable the necessary API, in this case, the Compute Engine API, so you can use its services.\nProvision: Now you can actually create the resource, like your Compute Engine instance."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer, as it follow the right path , to create the compute Engine instance"
      },
      {
        "index": 5,
        "text": "Anonymous: tatyavinchu 1Â year, 11Â months ago\nCorrect Answer is A"
      },
      {
        "index": 6,
        "text": "Anonymous: dr1ka 3Â years, 6Â months ago\nSelected Answer: A\nVote A"
      }
    ]
  },
  {
    "id": 46,
    "source": "examtopics",
    "question": "Your company runs one batch process in an on-premises server that takes around 30 hours to complete. The task runs monthly, can be performed offline, and must be restarted if interrupted. You want to migrate this workload to the cloud while minimizing cost. What should you do?",
    "options": {
      "A": "Migrate the workload to a Compute Engine Preemptible VM.",
      "B": "Migrate the workload to a Google Kubernetes Engine cluster with Preemptible nodes.",
      "C": "Migrate the workload to a Compute Engine VM. Start and stop the instance as needed.",
      "D": "Create an Instance Template with Preemptible VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: juliandm Highly Voted 5Â years, 5Â months ago\ni understand preemptible as a no-go because of \"must be restarted if interrupted\" here meaning \"starting from scratch\" . So C seems right jcloud965 4Â years, 6Â months ago\nI agree, C.\nyou won't run 30 hours job on preemptible instances that can be stopped at any time and can't run more than 24 hours.\nIf the job could be splitted, then preemptible VM is an option. Vador 4Â years, 3Â months ago\nPreemptible seems fine on batch jobs for at least 24hours, not the case in here dttncl 4Â years, 3Â months ago\nI agree with C. You can't risk running a processes that take 30 hours on a preemptible VM (Compute Engine always stops preemptible instances after they run for 24 hours). They are good for \"short-lived\" batch jobs. The scenario is NOT fault tolerant as the whole process restarts if interrupted.\nhttps://cloud.google.com/compute/docs/instances/preemptible"
      },
      {
        "index": 2,
        "text": "Anonymous: stepkurniawan Highly Voted 5Â years, 4Â months ago\nPreemptible will be perfect for a batch job that takes less than 24 hours. But it's not in this case. Linus11 4Â years, 8Â months ago\nWhat if it is a managed group of Pre emptible instances like in D. If one instance stops, another instance will take over.\nI choose D. sanhoo 4Â years, 7Â months ago\nis there an option to specify Pre emptible instances while creating template? I couldn't find that. If so then D can't be true djgodzilla 4Â years, 7Â months ago\nYes under management> Availability policy > premptibility ON/OFF"
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nOption A"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nOption C is correct, bcoz the job is running for more than 30 hours"
      },
      {
        "index": 5,
        "text": "Anonymous: tatyavinchu 2Â years, 5Â months ago\nCorrect Answer is C"
      },
      {
        "index": 6,
        "text": "Anonymous: Naree 2Â years, 6Â months ago\nSelected Answer: C\nJob runs for 30 hours and must be restarted if interrupted are \"indirectly proportional\" to \"Preemptible\"\nAns: C"
      },
      {
        "index": 7,
        "text": "Anonymous: ankyt9 3Â years, 1Â month ago\nSelected Answer: C\nPreemptible VMs are cheaper, but they will not be available beyond 24hrs"
      },
      {
        "index": 8,
        "text": "Anonymous: Charumathi 3Â years, 3Â months ago\nSelected Answer: C\nC is the correct answer,\nInstall the workload in a compute engine VM, start and stop the instance as needed, because as per the question the VM runs for 30 hours, process can be performed offline and should not be interrupted, if interrupted we need to restart the batch process again. Preemptible VMs are cheaper, but they will not be available beyond 24hrs, and if the process gets interrupted the preemptible VM will restart."
      },
      {
        "index": 9,
        "text": "Anonymous: KapilDhamija 3Â years, 5Â months ago\nSelected Answer: C\nC is the correct answer"
      },
      {
        "index": 10,
        "text": "Anonymous: ryumada 3Â years, 5Â months ago\nSelected Answer: C\nThe preemptible instance in GKE is same as Compute Engine Instance. They have same behavior that will be last for 24 hours.\nAlso, see the key here \"...and must be restarted if interrupted.\". That means the job will start from the scratch again if the preemptible instance terminated. So, you will just wasted your preemptible instances because the job will never be finished.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#overview"
      }
    ]
  },
  {
    "id": 47,
    "source": "examtopics",
    "question": "You are developing a new application and are looking for a Jenkins installation to build and deploy your source code. You want to automate the installation as quickly and easily as possible. What should you do?",
    "options": {
      "A": "Deploy Jenkins through the Google Cloud Marketplace.",
      "B": "Create a new Compute Engine instance. Run the Jenkins executable.",
      "C": "Create a new Kubernetes Engine cluster. Create a deployment for the Jenkins image.",
      "D": "Create an instance template with the Jenkins executable. Create a managed instance group with this template."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: filco72 Highly Voted 4Â years, 5Â months ago\nI would choose A. Deploy Jenkins through the Google Cloud Marketplace.\nas this is a well known opportunity on the GCP Marketplace"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (A):\nInstalling Jenkins\nIn this section, you use Cloud Marketplace to provision a Jenkins instance. You customize this instance to use the agent image you created in the previous section.\nGo to the Cloud Marketplace solution for Jenkins.\nClick Launch on Compute Engine.\nChange the Machine Type field to 4 vCPUs 15 GB Memory, n1-standard-4.\nMachine type selection for Jenkins deployment.\nClick Deploy and wait for your Jenkins instance to finish being provisioned. When it is finished, you will see:\nJenkins has been deployed.\nhttps://cloud.google.com/solutions/using-jenkins-for-distributed-builds-on-compute-engine#installing_jenkins"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nAnswer is : A as it's the most quick option"
      },
      {
        "index": 4,
        "text": "Anonymous: tatyavinchu 1Â year, 5Â months ago\nCorrect Answer is C"
      },
      {
        "index": 5,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: Charumathi 2Â years, 3Â months ago\nSelected Answer: A\nA is correct answer,\nTo quickly deploy Jenkins, deploy it through google cloud marketplace."
      },
      {
        "index": 7,
        "text": "Anonymous: KapilDhamija 2Â years, 5Â months ago\nSelected Answer: A\nremember as quickly as possible, also Google encourage things to be performed in minimal steps so A is the quickest and easiest choice"
      },
      {
        "index": 8,
        "text": "Anonymous: kiwi123 2Â years, 5Â months ago\nGo for A, the easiest"
      },
      {
        "index": 9,
        "text": "Anonymous: pspandher 2Â years, 6Â months ago\nThis is Repeat Question."
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nCloud Market Place is fastest and best .. A is right"
      }
    ]
  },
  {
    "id": 48,
    "source": "examtopics",
    "question": "You have downloaded and installed the gcloud command line interface (CLI) and have authenticated with your Google Account. Most of your Compute Engine instances in your project run in the europe-west1-d zone. You want to avoid having to specify this zone with each CLI command when managing these instances.\nWhat should you do?",
    "options": {
      "A": "Set the europe-west1-d zone as the default zone using the gcloud config subcommand.",
      "B": "In the Settings page for Compute Engine under Default location, set the zone to europe×’â‚¬\"west1-d.",
      "C": "In the CLI installation directory, create a file called default.conf containing zone=europe×’â‚¬\"west1×’â‚¬\"d.",
      "D": "Create a Metadata entry on the Compute Engine page with key compute/zone and value europe×’â‚¬\"west1×’â‚¬\"d."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (A):\nChange your default zone and region in the metadata server\nNote: This only applies to the default configuration.\nYou can change the default zone and region in your metadata server by making a request to the metadata server. For example:\ngcloud compute project-info add-metadata \\\n--metadata google-compute-default-region=europe-west1,google-compute-default-zone=europe-west1-b\nThe gcloud command-line tool only picks up on new default zone and region changes after you rerun the gcloud init command. After updating your default metadata, run gcloud init to reinitialize your default configuration.\nhttps://cloud.google.com/compute/docs/gcloud-compute#change_your_default_zone_and_region_in_the_metadata_server Examan1 4Â years, 4Â months ago\nUsing gcloud config you can set the zone in your active configuration only. This setting does not apply to other gcloud configurations and does not become the default for the project.\nRef: https://cloud.google.com/sdk/gcloud/reference/config/set\nSo I believe correct answer is B as per https://cloud.google.com/compute/docs/regions-zones/changing-default-zone-region#console\nIn the Cloud Console, go to the Settings page.\nFrom the Zone drop-down menu, select a default zone. tavva_prudhvi 4Â years, 3Â months ago\nbro, it mentioned going into the console settings, not the compute engine settings!\nTo change your default region or zone:\nIn the Cloud Console, go to the Settings page.\nGo to the Settings page\nFrom the Region drop-down menu, select a default region.\nFrom the Zone drop-down menu, select a default zone. jcloud965 4Â years ago\nThis setting in the Cloud Console won't be taken into account for gcloud on your active config dttncl 3Â years, 9Â months ago\nI agree the answer is A.\ngcloud config - view and edit Cloud SDK properties\nzone\nDefault zone to use when working with zonal Compute Engine resources.\nhttps://cloud.google.com/sdk/gcloud/reference/config xtian2900 4Â years, 10Â months ago\ndoes your comment imply that the answer is D ? i'm confused mahesh0049 3Â years, 6Â months ago\nevery thing is correct in your explanation but instead of using gcloud compute command they used gcloud config. bobthebuilder55110 2Â years, 11Â months ago\nYou can use the gcloud config set command here, https://cloud.google.com/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client"
      },
      {
        "index": 2,
        "text": "Anonymous: SSPC Highly Voted 4Â years, 11Â months ago\nI would go with the answer A"
      },
      {
        "index": 3,
        "text": "Anonymous: blackBeard33 Most Recent 1Â year, 5Â months ago\nSelected Answer: A\nI would choose A\nreference : https://cloud.google.com/sdk/gcloud/reference/config/set"
      },
      {
        "index": 4,
        "text": "Anonymous: VijKall 1Â year, 8Â months ago\nSelected Answer: A\ngcloud config set compute/zone europe-west1-b"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer, just set default in cloud console in the starting by the cloud shell commands"
      },
      {
        "index": 6,
        "text": "Anonymous: tatyavinchu 1Â year, 11Â months ago\nCorrect Answer is A"
      },
      {
        "index": 7,
        "text": "Anonymous: GokulVelusaamy 2Â years, 5Â months ago\nSelected Answer: A\nWe can set the defailt zone using the below CLI command,\ngcloud config set compute/zone ZONE\nRefer : https://cloud.google.com/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client"
      },
      {
        "index": 8,
        "text": "Anonymous: Angel_99 2Â years, 11Â months ago\nSelected Answer: A\nIt is clearly mentioned it is to be done via CLI not console"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nA is right"
      },
      {
        "index": 10,
        "text": "Anonymous: Tirthankar17 3Â years, 1Â month ago\nSelected Answer: A\nA, it is clearly mentioned it is to be done via CLI not console."
      }
    ]
  },
  {
    "id": 49,
    "source": "examtopics",
    "question": "The core business of your company is to rent out construction equipment at large scale. All the equipment that is being rented out has been equipped with multiple sensors that send event information every few seconds. These signals can vary from engine status, distance traveled, fuel level, and more. Customers are billed based on the consumption monitored by these sensors. You expect high throughput `\" up to thousands of events per hour per device `\" and need to retrieve consistent data based on the time of the event. Storing and retrieving individual signals should be atomic. What should you do?",
    "options": {
      "A": "Create a file in Cloud Storage per device and append new data to that file.",
      "B": "Create a file in Cloud Filestore per device and append new data to that file.",
      "C": "Ingest the data into Datastore. Store data in an entity group based on the device.",
      "D": "Ingest the data into Cloud Bigtable. Create a row key based on the event timestamp."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: hiteshrup Highly Voted 4Â years, 10Â months ago\nAnswer: D\nKeyword need to look for\n- \"High Throughput\",\n- \"Consistent\",\n- \"Property based data insert/fetch like ngine status, distance traveled, fuel level, and more.\" which can be designed in column,\n- \"Large Scale Customer Base + Each Customer has multiple sensor which send event in seconds\" This will go for pera bytes situation,\n- Export data based on the time of the event.\n- Atomic\no BigTable will fit all requirement.\no DataStore is not fully Atomic\no CloudStorage is not a option where we can export data based on time of event. We need another solution to do that\no FireStore can be used with MobileSDK.\nSo go with Option D: Big Table"
      },
      {
        "index": 2,
        "text": "Anonymous: Hjameel Highly Voted 4Â years, 11Â months ago\nD is the best answer , Cloud Bigtable har_riy 4Â years, 5Â months ago\nSimple analogy.\nInformation every few seconds --> Time Series --> Big Table"
      },
      {
        "index": 3,
        "text": "Anonymous: SHAAHIBHUSHANAWS Most Recent 1Â year, 7Â months ago\nSelected Answer: D\nhttps://cloud.google.com/bigtable/docs/overview"
      },
      {
        "index": 4,
        "text": "Anonymous: Linhtinh603 1Â year, 7Â months ago\nAnswer D is the best for high throughput and IoT, but I concern about creating a row key based on the event timestamp, will it be leading a hotspots issue?"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD is the right answer, as its can help with Automatically"
      },
      {
        "index": 6,
        "text": "Anonymous: Tosssha 2Â years, 5Â months ago\nReadModifyWriteRow requests are atomic:\nhttps://cloud.google.com/bigtable/docs/writes"
      },
      {
        "index": 7,
        "text": "Anonymous: xaqanik 2Â years, 5Â months ago\nit say any type of information. it means there can be an image file for instance. so, Bigtable is the best fit for this scenario"
      },
      {
        "index": 8,
        "text": "Anonymous: alex000 2Â years, 6Â months ago\nSelected Answer: C\nAnswer: C\nkey work: atomic transaction\nhttps://cloud.google.com/datastore/docs/concepts/overview"
      },
      {
        "index": 9,
        "text": "Anonymous: Cornholio_LMC 2Â years, 10Â months ago\nhad this question today"
      },
      {
        "index": 10,
        "text": "Anonymous: tomis2 3Â years ago\nSelected Answer: D\nTimeseries + IoT = Bigtable"
      }
    ]
  },
  {
    "id": 50,
    "source": "examtopics",
    "question": "You are asked to set up application performance monitoring on Google Cloud projects A, B, and C as a single pane of glass. You want to monitor CPU, memory, and disk. What should you do?",
    "options": {
      "A": "Enable API and then share charts from project A, B, and C.",
      "B": "Enable API and then give the metrics.reader role to projects A, B, and C.",
      "C": "Enable API and then use default dashboards to view all projects in sequence.",
      "D": "Enable API, create a workspace under project A, and then add projects B and C."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: jlclaude Highly Voted 4Â years, 5Â months ago\nD. workspaces is made for monitoring multiple projects."
      },
      {
        "index": 2,
        "text": "Anonymous: Hjameel Highly Voted 4Â years, 5Â months ago\nD , Workspace to monitor multiple projects. Khoka 4Â years, 2Â months ago\nhttps://cloud.google.com/monitoring/workspaces"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: D\nOption D is the correct Answer, First create the Workspace under A then add it to the Project B and C"
      },
      {
        "index": 4,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: D\nD. Enable API, create a workspace under project A, and then add projects B and C.\nTo monitor multiple Google Cloud projects in a single pane of glass, you can use Google Cloud's operations suite, formerly known as Stackdriver. By enabling the Cloud Monitoring API and creating a workspace under project A, you can add projects B and C to the same workspace. This will allow you to view metrics for CPU, memory, and disk usage for all projects in the same workspace. You can also set up alerting policies to be notified of any potential issues across all projects.\nEnabling the API alone or giving metrics.reader role to the projects will not provide a single pane of glass view of all the projects. Similarly, using default dashboards will not provide a unified view of all projects in a single dashboard."
      },
      {
        "index": 5,
        "text": "Anonymous: SathishBandi 2Â years, 1Â month ago\nSelected Answer: D\nIn question, mentioned 'as a single pane of glass' and workspace are meant for Monitoring"
      },
      {
        "index": 6,
        "text": "Anonymous: Charumathi 2Â years, 3Â months ago\nSelected Answer: D\nD is the correct answer,\nKeep Project A as host project in workspace and Project B and C as Service Project, and monitor the metrics of the Project A for a centralized view."
      },
      {
        "index": 7,
        "text": "Anonymous: ale_brd_111 2Â years, 3Â months ago\nSelected Answer: D\nStackdriver workspaces are deprecated, now in the monitoring page of the Project you want, you need to select the \"Scopes\". Anyway he closest answer is D.\nScopes allow you to monitor multiple projects.\nhttps://cloud.google.com/monitoring/settings/multiple-projects"
      },
      {
        "index": 8,
        "text": "Anonymous: KapilDhamija 2Â years, 5Â months ago\nSelected Answer: D\nD should be the correct answer"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nD is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: Rutu_98 2Â years, 8Â months ago\nSelected Answer: D\nD is correct"
      }
    ]
  },
  {
    "id": 51,
    "source": "examtopics",
    "question": "You created several resources in multiple Google Cloud projects. All projects are linked to different billing accounts. To better estimate future charges, you want to have a single visual representation of all costs incurred. You want to include new cost data as soon as possible. What should you do?",
    "options": {
      "A": "Configure Billing Data Export to BigQuery and visualize the data in Data Studio.",
      "B": "Visit the Cost Table page to get a CSV export and visualize it using Data Studio.",
      "C": "Fill all resources in the Pricing Calculator to get an estimate of the monthly cost.",
      "D": "Use the Reports view in the Cloud Billing Console to view the desired cost information."
    },
    "correct": "A",
    "discussions": []
  },
  {
    "id": 52,
    "source": "examtopics",
    "question": "Your company has workloads running on Compute Engine and on-premises. The Google Cloud Virtual Private Cloud (VPC) is connected to your WAN over a\nVirtual Private Network (VPN). You need to deploy a new Compute Engine instance and ensure that no public Internet traffic can be routed to it. What should you do?",
    "options": {
      "A": "Create the instance without a public IP address.",
      "B": "Create the instance with Private Google Access enabled.",
      "C": "Create a deny-all egress firewall rule on the VPC network.",
      "D": "Create a route on the VPC to route all traffic to the instance over the VPN tunnel."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MohammedGhouse Highly Voted 4Â years, 5Â months ago\nA: answer looks right"
      },
      {
        "index": 2,
        "text": "Anonymous: lxs Highly Voted 3Â years, 2Â months ago\nThe question is about ingress traffic from Internet\nA - If the VM does not have public IP it is not routable from Internet. Correct answear\nB - it is about how to access Google Services API. It does not tell about ingress Internet traffic\nC - It is about egress traffic\nD - It could be but we do not know anything about Internet ingress traffic to on prem. What's more default route tells about egress traffic to Internet. Nothing how Internet can access Compute instance.\nCorrect answer is A."
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: A\nBy not assigning a public IP address to the Compute Engine instance, you ensure that it cannot receive traffic directly from the public internet. The instance will only be accessible via the private network (including your VPN connection), which meets the requirement of preventing public internet traffic from being routed to it."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct option, as other not limit the ingrres traffic"
      },
      {
        "index": 5,
        "text": "Anonymous: dataengineeruser34 1Â year, 11Â months ago\nA for sure"
      },
      {
        "index": 6,
        "text": "Anonymous: SK1990 2Â years ago\nSelected Answer: A\nA - for sure"
      },
      {
        "index": 7,
        "text": "Anonymous: SK1990 2Â years ago\nSelected Answer: A\nA is the best anmswer."
      },
      {
        "index": 8,
        "text": "Anonymous: Nazz1977 2Â years, 1Â month ago\nSelected Answer: A\nA for sure"
      },
      {
        "index": 9,
        "text": "Anonymous: Sam98845 2Â years, 2Â months ago\nshould be A. VMs cannot communicate over the internet without a public IP address. Private Google Access permits access to Google APIs and services in Google's production infrastructure.\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "index": 10,
        "text": "Anonymous: kailash 2Â years, 3Â months ago\nSelected Answer: A\nElimination"
      }
    ]
  },
  {
    "id": 53,
    "source": "examtopics",
    "question": "Your team maintains the infrastructure for your organization. The current infrastructure requires changes. You need to share your proposed changes with the rest of the team. You want to follow Google's recommended best practices. What should you do?",
    "options": {
      "A": "Use Deployment Manager templates to describe the proposed changes and store them in a Cloud Storage bucket.",
      "B": "Use Deployment Manager templates to describe the proposed changes and store them in Cloud Source Repositories.",
      "C": "Apply the changes in a development environment, run gcloud compute instances list, and then save the output in a shared Storage bucket.",
      "D": "Apply the changes in a development environment, run gcloud compute instances list, and then save the output in Cloud Source Repositories."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (A):\nConnecting to Cloud Storage buckets\nCloud Storage is a flexible, scalable, and durable storage option for your virtual machine instances. You can read and write files to Cloud Storage buckets from almost anywhere, so you can use buckets as common storage between your instances, App Engine, your on-premises systems, and other cloud services.\nhttps://cloud.google.com/compute/docs/disks/gcs-buckets\nWhy not (B)?\nCaution\nCloud Source Repositories are intended to store only the source code for your app and not user or personal data. Don't store any Core App Engine Customer Data (as defined in your License Agreement) in Cloud Source Repositories.\nhttps://cloud.google.com/source-repositories/docs/features stepkurniawan 5Â years, 4Â months ago\nyou store the sensitive data NOT in the instance template, that is the current best practice. But you need version control like GIT or Google's GIT (Cloud Source Repo) to backup your code somehow and able to roll back if needed. JohnnieWalker 4Â years, 6Â months ago\nB is the answer. Deployment Manager Template can be written in either Jinja or Python, this is Infrastructure as Code (IaC) we are talking about here, same as AWS Cloudformation, or Terraform. Therefore, they should be stored on a git repository such as Google Cloud Source Repositories. magistrum 5Â years ago\nLook at my post above, cloud repo is for code, not templates gcpengineer 4Â years, 5Â months ago\nB is the ans pYWORLD 4Â years, 5Â months ago\nI agree with what are you saying, but the problem that you know how the deployment manager template looks? Is jinja/yaml file that means that are source code, so better to put them inside of an repository.\nSo, for my perpective I will go with the B. ashrafh 4Â years, 5Â months ago\nmaybe below link will help\nhttps://cloud.google.com/deployment-manager/docs/configuration/templates/hosting-templates-externally\nfrom that we can take a idea on deciding cloud storage or repo :), Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: SSPC Highly Voted 5Â years, 5Â months ago\nB is correct. https://cloud.google.com/source-repositories/docs/features AmitKM 5Â years, 4Â months ago\nUsing Cloud Storage Repos, you can add comments and describe your changes to the team.Hence this might be a better option. magistrum 5Â years ago\nI don't see how you can do this when I tried creating:\nAdd code to your repository\ninfo\nYour repository is currently empty. Add some code using a selected method and then refresh your browser. Contents added to this repository can take some time to show up in search results. Learn more.\nSelect an option to push code to your repository:\nPush code from a local Git repository\nClone your repository to a local Git repository"
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: B\nGoogle-recommended best practices for infrastructure management include using infrastructure as code (such as Deployment Manager templates) and storing code in a version-controlled repository (like Cloud Source Repositories). This enables collaboration, review, and tracking of changes by the team."
      },
      {
        "index": 4,
        "text": "Anonymous: Qjb8m9h 11Â months, 1Â week ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: yomi95 1Â year, 2Â months ago\nSelected Answer: A\nEffective June 17, 2024, Cloud Source Repositories isn't available to new customers. So only option is A for latest, (this question/answers might not be valid anymore)\nhttps://cloud.google.com/source-repositories/docs"
      },
      {
        "index": 6,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: A\nEffective June 17, 2024, Cloud Source Repositories isn't available to new customers. If your organization hasn't previously used Cloud Source Repositories, you can't enable the API or use Cloud Source Repositories. New projects not connected to an organization can't enable the Cloud Source Repositories API. Organizations that have used Cloud Source Repositories prior to June 17, 2024 are not affected by this change.\nI think that the question does not exist already, or A is right answer"
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: B\nUse of Deployment Manager Templates:\nGoogle Cloud Deployment Manager is a tool that allows you to automate the creation and management of Google Cloud resources. It uses templates written in YAML, Python, or Jinja2 to describe your resources and their configurations.\nBy using Deployment Manager templates, you can provide a clear, codified, and repeatable description of the proposed changes to your infrastructure.\nVersion Control and Collaboration:\nCloud Source Repositories provide managed and scalable Git repositories hosted on Google Cloud. Storing your Deployment Manager templates in a source repository enables version control, which is a best practice in software and infrastructure development.\nThis approach facilitates collaboration among team members, allowing for review, commenting, and history tracking of changes to the templates. Cynthia2023 2Â years ago\nA. Store in Cloud Storage Bucket: While storing templates in a Cloud Storage bucket makes them accessible, it does not provide the benefits of version control and collaborative features offered by source control systems."
      },
      {
        "index": 8,
        "text": "Anonymous: sara11190 2Â years, 3Â months ago\nB is the correct answer"
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 10,
        "text": "Anonymous: pritampanda1988 2Â years, 5Â months ago\nSelected Answer: A\nOption B (storing in Cloud Source Repositories) might be suitable for storing application code, but it's not the best practice for storing infrastructure configuration templates."
      }
    ]
  },
  {
    "id": 54,
    "source": "examtopics",
    "question": "You have a Compute Engine instance hosting an application used between 9 AM and 6 PM on weekdays. You want to back up this instance daily for disaster recovery purposes. You want to keep the backups for 30 days. You want the Google-recommended solution with the least management overhead and the least number of services. What should you do?",
    "options": {
      "A": "1. Update your instances' metadata to add the following value: snapshot×’â‚¬\"schedule: 0 1 * * * 2. Update your instances' metadata to add the following value: snapshot×’â‚¬\"retention: 30",
      "B": "1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: - Schedule frequency: Daily - Start time: 1:00 AM ×’â‚¬\" 2:00 AM - Autodelete snapshots after: 30 days",
      "C": "1. Create a Cloud Function that creates a snapshot of your instance's disk. 2. Create a Cloud Function that deletes snapshots that are older than 30 days. 3. Use Cloud Scheduler to trigger both Cloud Functions daily at 1:00 AM.",
      "D": "1. Create a bash script in the instance that copies the content of the disk to Cloud Storage. 2. Create a bash script in the instance that deletes data older than 30 days in the backup Cloud Storage bucket. 3. Configure the instance's crontab to execute these scripts daily at 1:00 AM."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (B):\nCreating scheduled snapshots for persistent disk\nThis document describes how to create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. Use snapshot schedules as a best practice to back up your Compute Engine workloads. After creating a snapshot schedule, you can apply it to one or more persistent disks.\nhttps://cloud.google.com/compute/docs/disks/scheduled-snapshots"
      },
      {
        "index": 2,
        "text": "Anonymous: Ridhanya Highly Voted 3Â years, 7Â months ago\nit is b. we cannot define snapshot config in instance metadata.\nVM instance metadata is used only for:\nstartup and shutdown scripts\nhost maintanence\nguest attributes"
      },
      {
        "index": 3,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\nNow in 2024, use backup&DR for this. But in 2020 it was B"
      },
      {
        "index": 4,
        "text": "Anonymous: Jonassamr 1Â year, 2Â months ago\nSelected Answer: B\nhttps://cloud.google.com/compute/docs/instances/schedule-instance-start-stop ccpmad 1Â year, 1Â month ago\nNo, it is not about schedule start-stop, is about snapshot schedule of the disks...\nbut yes, it is B."
      },
      {
        "index": 5,
        "text": "Anonymous: idk_4 1Â year, 6Â months ago\nSelected Answer: B\nI think when we think about best practice, we should always think about being practical. The most practical method is usually the best practice with a few exceptions. In this scenario, Answers C and D require a lot of effort. Answer A seems not quite relevant. Answer B is the only correct option."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: Rajat2309sharma 2Â years, 5Â months ago\nSelected Answer: B\nB is ans"
      },
      {
        "index": 8,
        "text": "Anonymous: slcvlctetri 2Â years, 6Â months ago\nSelected Answer: B\ngot this question 2 days ago. B is right."
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nB is more appropriate"
      },
      {
        "index": 10,
        "text": "Anonymous: nhadi82 3Â years, 1Â month ago\nSelected Answer: B\nCorrect Answer B"
      }
    ]
  },
  {
    "id": 55,
    "source": "examtopics",
    "question": "Your existing application running in Google Kubernetes Engine (GKE) consists of multiple pods running on four GKE n1`\"standard`\"2 nodes. You need to deploy additional pods requiring n2`\"highmem`\"16 nodes without any downtime. What should you do?",
    "options": {
      "A": "Use gcloud container clusters upgrade. Deploy the new services.",
      "B": "Create a new Node Pool and specify machine type n2×’â‚¬\"highmem×’â‚¬\"16. Deploy the new pods.",
      "C": "Create a new cluster with n2×’â‚¬\"highmem×’â‚¬\"16 nodes. Redeploy the pods and delete the old cluster.",
      "D": "Create a new cluster with both n1×’â‚¬\"standard×’â‚¬\"2 and n2×’â‚¬\"highmem×’â‚¬\"16 nodes. Redeploy the pods and delete the old cluster."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 3Â years, 10Â months ago\nB is correct answer, read below form google docs;\nThis tutorial demonstrates how to migrate workloads running on a Google Kubernetes Engine (GKE) cluster to a new set of nodes within the same cluster without incurring downtime for your application. Such a migration can be useful if you want to migrate your workloads to nodes with a different machine type.\nBackground\nA node pool is a subset of machines that all have the same configuration, including machine type (CPU and memory) authorization scopes. Node pools represent a subset of nodes within a cluster; a container cluster can contain one or more node pools.\nWhen you need to change the machine profile of your Compute Engine cluster, you can create a new node pool and then migrate your workloads over to the new node pool.\nTo migrate your workloads without incurring downtime, you need to:\nMark the existing node pool as unschedulable.\nDrain the workloads running on the existing node pool.\nDelete the existing node pool.\nhttps://cloud.google.com/kubernetes-engine/docs/tutorials/migrating-node-pool#creating_a_node_pool_with_large_machine_type"
      },
      {
        "index": 2,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nB is the right answer , if you need the new, and if you want to old one also then its D"
      },
      {
        "index": 3,
        "text": "Anonymous: ashtonez 1Â year, 10Â months ago\nSelected Answer: B\nB is correct, creating another cluster just doesnt make any sense, node pools are intended for this situations"
      },
      {
        "index": 4,
        "text": "Anonymous: Chiunara 1Â year, 10Â months ago\nSelected Answer: B\nAnswer is obviously B (read @GCP_Student1 and @Bobbybash replies)"
      },
      {
        "index": 5,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: B\nB. Create a new Node Pool and specify machine type n2\"highmem\"16. Deploy the new pods.\nCreating a new Node Pool with the required machine type is the correct approach to deploy additional pods without any downtime. This approach allows you to scale the cluster horizontally by adding more nodes to the existing cluster. By creating a new Node Pool, you can add n2\"highmem\"16 nodes to the existing cluster, and deploy new pods on these nodes without affecting the existing services running on the n1\"standard\"2 nodes. This way, you can ensure high availability and zero downtime during the deployment. Option A (gcloud container clusters upgrade) upgrades the entire cluster, and Option C and D (creating a new cluster) involve deleting the existing cluster, which may cause downtime."
      },
      {
        "index": 6,
        "text": "Anonymous: BlueJay20 1Â year, 11Â months ago\nSelected Answer: D\nThe keyword is \"additional\". Answer B is good if you want to replace with the new VMs. In this case you want the existing ones as well as the new ones. Therefore D. swa99 1Â year, 11Â months ago\nThe keyword is \"additional\", in option D you are deleting the old cluster. SO the answer is B"
      },
      {
        "index": 7,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nB makes perfect sense."
      },
      {
        "index": 8,
        "text": "Anonymous: Tirthankar17 2Â years, 7Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 9,
        "text": "Anonymous: arsh1916 3Â years, 8Â months ago\nB is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: Jacky_YO 3Â years, 9Â months ago\nANS : B"
      }
    ]
  },
  {
    "id": 56,
    "source": "examtopics",
    "question": "You have an application that uses Cloud Spanner as a database backend to keep current state information about users. Cloud Bigtable logs all events triggered by users. You export Cloud Spanner data to Cloud Storage during daily backups. One of your analysts asks you to join data from Cloud Spanner and Cloud\nBigtable for specific users. You want to complete this ad hoc request as efficiently as possible. What should you do?",
    "options": {
      "A": "Create a dataflow job that copies data from Cloud Bigtable and Cloud Storage for specific users.",
      "B": "Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.",
      "C": "Create a Cloud Dataproc cluster that runs a Spark job to extract data from Cloud Bigtable and Cloud Storage for specific users.",
      "D": "Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AmitKM Highly Voted 5Â years, 5Â months ago\nI think it should be D. https://cloud.google.com/bigquery/external-data-sources SSPC 5Â years, 5Â months ago\nThe question says: \" Join data from Cloud Spanner and Cloud Bigtable for specific users\" You can see the Google documentation in the link https://cloud.google.com/spanner/docs/export Eshkrkrkr 5Â years, 2Â months ago\nOh my god, SSPC read you your links!\nThe process uses Dataflow and exports data to a folder in a Cloud Storage bucket. The resulting folder contains a set of Avro files and JSON manifest files. And what next? I will tell - next you read below: Compute Engine: Before running your export job, you must set up initial quotas for Recommended starting values are:\nCPUs: 200\nIn-use IP addresses: 200\nStandard persistent disk: 50 TB\nStill think its A?"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (D):\nIntroduction to external data sources\nThis page provides an overview of querying data stored outside of BigQuery.\nhttps://cloud.google.com/bigquery/external-data-sources ESP_SAP 5Â years, 5Â months ago\nBigQuery offers support for querying data directly from:\nBigtable\nCloud Storage\nGoogle Drive\nCloud SQL (beta) djgodzilla 4Â years, 7Â months ago\nbut here we're not talking about joining Cloud Storage and Cloud Bigtable external tables.\nthe join happens between a distributed relational database (Spanner) and key-value NoSQL Database (BigTable) . how's converting Spanner to cloud storage an implicit and trivial step. djgodzilla 4Â years, 7Â months ago\n\"The Cloud Spanner to Cloud Storage Text template is a batch pipeline that reads in data from a Cloud Spanner table, optionally transforms the data via a JavaScript User Defined Function (UDF) that you provide, and writes it to Cloud Storage as CSV text files.\"\nhttps://cloud.google.com/dataflow/docs/guides/templates/provided-batch#cloudspannertogcstext\n\"The Dataflow connector for Cloud Spanner lets you read data from and write data to Cloud Spanner in a Dataflow pipeline\"\nhttps://cloud.google.com/spanner/docs/dataflow-connector ryzior 3Â years, 10Â months ago\nupdate:\nBigQuery supports the following external data sources:\nBigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive"
      },
      {
        "index": 3,
        "text": "Anonymous: Ice_age Most Recent 1Â year, 1Â month ago\nInteresting how most people are choosing D, yet that answer makes no reference to Cloud Spanner. I'm going to have to go with B since it specifically mentions Cloud Spanner and Cloud Bigtable."
      },
      {
        "index": 4,
        "text": "Anonymous: kuracpalac 1Â year, 11Â months ago\nSelected Answer: B\nThe Q says that an analyst wants to analyze data about a user from 2 different sources, which Dataflow will give you, plus as Google states, it allows you more time analyzing stuff and less time fiddling with setting things up, which option D is talking about, which is wrong per the asked Q."
      },
      {
        "index": 5,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nSelected Answer: D\nD is apt and possible."
      },
      {
        "index": 6,
        "text": "Anonymous: KC_go_reply 2Â years, 7Â months ago\nSelected Answer: D\nBigQuery is powerful. If you have data in one of the popular sources like Cloud Storage or Bigtable, it is much more efficient - both for cost and computation - to create an external table on those data sources, than to copy their data around.\nBesides that, also keep in mind that table clones and snapshots are much more efficient than full table copy etc."
      },
      {
        "index": 7,
        "text": "Anonymous: Praxii 2Â years, 8Â months ago\nSelected Answer: B\nI go for option B. As in option D, the data is backed up data and not the most recent data."
      },
      {
        "index": 8,
        "text": "Anonymous: Bobbybash 2Â years, 11Â months ago\nSelected Answer: B\nB. Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users.\nTo join data from Cloud Spanner and Cloud Bigtable for specific users, creating a dataflow job that copies data from both sources is the most efficient option. This approach allows you to process the data in parallel, and you can take advantage of Cloud Dataflow's autoscaling feature to handle large volumes of data. You can use Cloud Dataflow to read data from Cloud Bigtable and Cloud Spanner, join the data based on the user fields, and write the output to a new location or send it to the analyst. Option A (copying data from Cloud Storage) does not provide data from Cloud Spanner, and option C (running a Spark job on a Dataproc cluster) involves higher overhead costs. Option D (using BigQuery external tables) is not efficient for ad hoc requests, as data is exported from Spanner to Cloud Storage during backups, so there may be a delay in data availability."
      },
      {
        "index": 9,
        "text": "Anonymous: anolive 3Â years, 2Â months ago\nSelected Answer: D\nI thinks is D, but not 100% sure, because D does not have any infomation about the specific user like others options."
      },
      {
        "index": 10,
        "text": "Anonymous: Charumathi 3Â years, 3Â months ago\nSelected Answer: D\nD is the correct answer,\nAn external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage.\nBigQuery supports the following external data sources:\nAmazon S3\nAzure Storage\nCloud Bigtable\nCloud Spanner\nCloud SQL\nCloud Storage\nDrive"
      }
    ]
  },
  {
    "id": 57,
    "source": "examtopics",
    "question": "You are hosting an application from Compute Engine virtual machines (VMs) in us`\"central1`\"a. You want to adjust your design to support the failure of a single\nCompute Engine zone, eliminate downtime, and minimize cost. What should you do?",
    "options": {
      "A": "×’â‚¬\" Create Compute Engine resources in us×’â‚¬\"central1×’â‚¬\"b. ×’â‚¬\" Balance the load across both us×’â‚¬\"central1×’â‚¬\"a and us×’â‚¬\"central1×’â‚¬\"b.",
      "B": "×’â‚¬\" Create a Managed Instance Group and specify us×’â‚¬\"central1×’â‚¬\"a as the zone. ×’â‚¬\" Configure the Health Check with a short Health Interval.",
      "C": "×’â‚¬\" Create an HTTP(S) Load Balancer. ×’â‚¬\" Create one or more global forwarding rules to direct traffic to your VMs.",
      "D": "×’â‚¬\" Perform regular backups of your application. ×’â‚¬\" Create a Cloud Monitoring Alert and be notified if your application becomes unavailable. ×’â‚¬\" Restore from backups when notified."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nA. Create Compute Engine resources in us \"central1 \"b. \" Balance the load across both us \"central1\"a and us \"central1\"b."
      },
      {
        "index": 2,
        "text": "Anonymous: obeythefist Highly Voted 3Â years, 4Â months ago\nThis seems straightforward. \"A\" is the only answer that involves putting instances in more than one zone!\nA. Yes, creating instances in another zone and balancing the loads will fix this problem\nB. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nC. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures.\nD. Wrong. This keeps all the instances in one zone, but the question says we want to protect against zone failures."
      },
      {
        "index": 3,
        "text": "Anonymous: JoseCloudEng1994 Most Recent 1Â year ago\nSelected Answer: A\nThe ideal solution would be to have a 'regional' instance group with a Load Balancer"
      },
      {
        "index": 4,
        "text": "Anonymous: accd3fd 1Â year, 3Â months ago\nthe Answer is B. the 2 Main ask are 1. Single Zone and Minimizes Cost\noption B is a cost-effective solution that can provide high availability within a single zone. By creating a Managed Instance Group in us-central1-a and configuring a Health Check with a short Health Interval, you can ensure that if one instance becomes unavailable, the Managed Instance Group will automatically create a new instance to replace it. This can help minimize downtime and ensure that your application remains available within the us-central1-a zone. ccpmad 1Â year, 1Â month ago\nNo, it is not B."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA seems more right as it help with the Zone failure, all other create the same in same zone"
      },
      {
        "index": 6,
        "text": "Anonymous: DrLegendgun 2Â years, 3Â months ago\nSelected Answer: A\nThe Answer is B"
      },
      {
        "index": 7,
        "text": "Anonymous: Angel_99 2Â years, 11Â months ago\nSelected Answer: A\nA is best option"
      },
      {
        "index": 8,
        "text": "Anonymous: abirroy 2Â years, 11Â months ago\nSelected Answer: A\nA is the best option"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nA is fine."
      },
      {
        "index": 10,
        "text": "Anonymous: Ridhanya 3Â years, 7Â months ago\nA is correct because we have to eliminate single zone failure problem"
      }
    ]
  },
  {
    "id": 58,
    "source": "examtopics",
    "question": "A colleague handed over a Google Cloud Platform project for you to maintain. As part of a security checkup, you want to review who has been granted the Project\nOwner role. What should you do?",
    "options": {
      "A": "In the console, validate which SSH keys have been stored as project-wide keys.",
      "B": "Navigate to Identity-Aware Proxy and check the permissions for these resources.",
      "C": "Enable Audit Logs on the IAM & admin page for all resources, and validate the results.",
      "D": "Use the command gcloud projects get×’â‚¬\"iam×’â‚¬\"policy to view the current role assignments."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (D):\nA simple approach would be to use the command flags available when listing all the IAM policy for a given project. For instance, the following command:\n`gcloud projects get-iam-policy $PROJECT_ID --flatten=\"bindings[].members\" --format=\"table(bindings.members)\" --filter=\"bindings.role:roles/owner\"`\noutputs all the users and service accounts associated with the role â€˜roles/ownerâ€™ in the project in question.\nhttps://groups.google.com/g/google-cloud-dev/c/Z6sZs7TvygQ?pli=1"
      },
      {
        "index": 2,
        "text": "Anonymous: MohammedGhouse Highly Voted 4Â years, 11Â months ago\nD: is the answer SSPC 4Â years, 11Â months ago\nD is the correct. yurstev 4Â years, 7Â months ago\nD IS THE ANSWER"
      },
      {
        "index": 3,
        "text": "Anonymous: blackBeard33 Most Recent 1Â year, 5Â months ago\nSelected Answer: D\nThe Answer is D. Per documentation: https://cloud.google.com/sdk/gcloud/reference/projects/get-iam-policy.\nAlso, just tried in my own account and it brought a list if all users and their roles."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD seems, more correct"
      },
      {
        "index": 5,
        "text": "Anonymous: tomis2 3Â years ago\nSelected Answer: D\ngcloud iam get-iam-policy"
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 3Â years ago\nD is right"
      },
      {
        "index": 7,
        "text": "Anonymous: Rutu_98 3Â years, 1Â month ago\nSelected Answer: D\nAnswer is D"
      },
      {
        "index": 8,
        "text": "Anonymous: somenick 3Â years, 3Â months ago\nSelected Answer: D\ngcloud projects get-iam-policy $PROJECT_ID"
      },
      {
        "index": 9,
        "text": "Anonymous: obeythefist 3Â years, 4Â months ago\nI chose D by a process of elimination. Here's my take:\nA. There's more than one way to access an instance than just the SSH keys, and SSH keys have nothing to do with Project Owner role.\nB. Barking up the wrong tree here, Identity-Aware Proxy is more for remotely accessing resources, rather than Project Owner IAM roles.\nC. This will only work if everyone who is a Project Owner accesses the system so you can see them in the logs. What if a Project Owner doesn't access the Project for a while? How long will you wait? Nope.\nD. By elimination, this is the best result. BigQuery 3Â years, 4Â months ago\nNICE EXPLANATION; WAY TO G0 D"
      },
      {
        "index": 10,
        "text": "Anonymous: HansKloss611 3Â years, 5Â months ago\nSelected Answer: D\nD is correct"
      }
    ]
  },
  {
    "id": 59,
    "source": "examtopics",
    "question": "You are running multiple VPC-native Google Kubernetes Engine clusters in the same subnet. The IPs available for the nodes are exhausted, and you want to ensure that the clusters can grow in nodes when needed. What should you do?",
    "options": {
      "A": "Create a new subnet in the same region as the subnet being used.",
      "B": "Add an alias IP range to the subnet used by the GKE clusters.",
      "C": "Create a new VPC, and set up VPC peering with the existing VPC.",
      "D": "Expand the CIDR range of the relevant subnet for the cluster."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (D):\ngcloud compute networks subnets expand-ip-range\nNAME\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range magistrum 4Â years ago\nOk D it is, here's the GKE specific documentation\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips\nEvery subnet must have a primary IP address range. You can expand the primary IP address range at any time, even when Google Cloud resources use the subnet; however, you cannot shrink or change a subnet's primary IP address scheme after the subnet has been created. The first two and last two IP addresses of a primary IP address range are reserved by Google Cloud."
      },
      {
        "index": 2,
        "text": "Anonymous: MohammedGhouse Highly Voted 4Â years, 5Â months ago\nD: is the answer SSPC 4Â years, 5Â months ago\nI agree with you. https://cloud.google.com/vpc/docs/configure-alias-ip-ranges#gcloud_1"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: D\nD is the correct Answer, as you just expand the range"
      },
      {
        "index": 4,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: D\nD. Expand the CIDR range of the relevant subnet for the cluster.\nExpanding the CIDR range of the relevant subnet for the cluster would increase the number of available IP addresses and allow the clusters to grow when needed. This can be done by modifying the existing subnet's IP address range in the VPC network settings. Adding a new subnet or VPC peering would not directly address the issue of running out of available IP addresses in the current subnet. Adding an alias IP range to the subnet could provide additional IP addresses, but may not be sufficient for long-term growth."
      },
      {
        "index": 5,
        "text": "Anonymous: AwesomeGCP 2Â years, 3Â months ago\nSelected Answer: D\nD. Expand the CIDR range of the relevant subnet for the cluster."
      },
      {
        "index": 6,
        "text": "Anonymous: learn_GCP 2Â years, 3Â months ago\nSelected Answer: D\nD. Expanding CIDR range is enough."
      },
      {
        "index": 7,
        "text": "Anonymous: sonuricky 2Â years, 5Â months ago\nC is the right answer ryumada 2Â years, 5Â months ago\nPlease provide the reason why you choose C as the right answer. ESP_SAP explains clearly about the reason why he choose D as the right answer even he add Google Documentation link too to prove his answer."
      },
      {
        "index": 8,
        "text": "Anonymous: Bumbah 2Â years, 6Â months ago\nSelected Answer: D\nCorrect answer is D:\nhttps://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\nJust expand your subnet."
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nD is right"
      },
      {
        "index": 10,
        "text": "Anonymous: GCP_Student1 3Â years, 10Â months ago\nThis might help\nNode limiting ranges\nThe maximum number of Pods and Services for a given GKE cluster is limited by the size of the cluster's secondary ranges. The maximum number of nodes in the cluster is limited by the size of the cluster's subnet's primary IP address range and the cluster's Pod address range.\nThe Cloud Console shows error messages like the following to indicate that either the subnet's primary IP address range or the cluster's Pod IP address range (the subnet's secondary IP address range for Pods) has been exhausted:\nInstance [node name] creation failed: IP space of [cluster subnet] is\nexhausted\nNote: Secondary subnets are not visible in Cloud Console. If you can't find the [cluster subnet] reported by the above error message it means that the error is caused by IP exhaustion in a secondary subnet. In this case check the secondary ranges of the primary subnet.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#node_limiters GCP_Student1 3Â years, 10Â months ago\nBy the way the answer is;\nD. Expand the CIDR range of the relevant subnet for the cluster."
      }
    ]
  },
  {
    "id": 60,
    "source": "examtopics",
    "question": "You have a batch workload that runs every night and uses a large number of virtual machines (VMs). It is fault-tolerant and can tolerate some of the VMs being terminated. The current cost of VMs is too high. What should you do?",
    "options": {
      "A": "Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs.",
      "B": "Run a test using simulated maintenance events. If the test is successful, use N1 Standard VMs when running future jobs.",
      "C": "Run a test using a managed instance group. If the test is successful, use N1 Standard VMs in the managed instance group when running future jobs.",
      "D": "Run a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 3Â years, 11Â months ago\nCorrect Answer is (A):\nCreating and starting a preemptible VM instance\nThis page explains how to create and use a preemptible virtual machine (VM) instance. A preemptible instance is an instance you can create and run at a much lower price than normal instances. However, Compute Engine might terminate (preempt) these instances if it requires access to those resources for other tasks. Preemptible instances will always terminate after 24 hours. To learn more about preemptible instances, read the preemptible instances documentation.\nPreemptible instances are recommended only for fault-tolerant applications that can withstand instance preemptions. Make sure your application can handle preemptions before you decide to create a preemptible instance. To understand the risks and value of preemptible instances, read the preemptible instances documentation.\nhttps://cloud.google.com/compute/docs/instances/create-start-preemptible-instance"
      },
      {
        "index": 2,
        "text": "Anonymous: MohammedGhouse Highly Voted 3Â years, 11Â months ago\nA: is the answer SSPC 3Â years, 11Â months ago\n\"A\" is correct juliandm 3Â years, 11Â months ago\nWhat about a mixture of preemptible N1 and normal N1 instances? i can't believe just having preemptible is a good practice Ale1973 3Â years, 10Â months ago\nGood point, in real-world your solution, is the best. For this scenario, the answer is A."
      },
      {
        "index": 3,
        "text": "Anonymous: akhun Most Recent 1Â year, 5Â months ago\nSelected Answer: A\nIt is specific on Batch workload , runs in less than 24 hrs, is fault tolerant. The best candidate for this is job is a preemptible VM"
      },
      {
        "index": 4,
        "text": "Anonymous: Pr44 1Â year, 8Â months ago\nSelected Answer: A\nPreemptible to save cost and even it is fault tolerant"
      },
      {
        "index": 5,
        "text": "Anonymous: Charumathi 1Â year, 9Â months ago\nSelected Answer: A\nA is correct, preemptible VMs reduce cost, and this is recommended to run batch jobs which run less than 24 hours"
      },
      {
        "index": 6,
        "text": "Anonymous: gcpreviewer 1Â year, 9Â months ago\nSelected Answer: A\nI Vote A as it is clearly correct. Whenever something runs in under 24 hours and is fault tolerant we should be looking at preemptible VMs to save costs."
      },
      {
        "index": 7,
        "text": "Anonymous: ccieman2016 1Â year, 10Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: gcpj 2Â years ago\nSelected Answer: A\nAnswer should be A: preemptible VM instances. Because the workload is fault-tolerant and can tolerate some of the VMs being terminated."
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 2Â years ago\nA is right .."
      },
      {
        "index": 10,
        "text": "Anonymous: NBR1 2Â years, 1Â month ago\nSelected Answer: A\nI believe it is A"
      }
    ]
  },
  {
    "id": 61,
    "source": "examtopics",
    "question": "You are working with a user to set up an application in a new VPC behind a firewall. The user is concerned about data egress. You want to configure the fewest open egress ports. What should you do?",
    "options": {
      "A": "Set up a low-priority (65534) rule that blocks all egress and a high-priority rule (1000) that allows only the appropriate ports.",
      "B": "Set up a high-priority (1000) rule that pairs both ingress and egress ports.",
      "C": "Set up a high-priority (1000) rule that blocks all egress and a low-priority (65534) rule that allows only the appropriate ports.",
      "D": "Set up a high-priority (1000) rule to allow the appropriate ports."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (A):\nImplied rules\nEvery VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:\nImplied allow egress rule. An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by Google Cloud. A higher priority firewall rule may restrict outbound access. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a Cloud NAT instance. For more information, see Internet access requirements.\nImplied deny ingress rule. An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming connections to them. A higher priority rule might allow incoming access. The default network includes some additional rules that override this one, allowing certain types of incoming connections.\nhttps://cloud.google.com/vpc/docs/firewalls#default_firewall_rules patashish 3Â years ago\nThe correct answer is C ryumada 2Â years, 11Â months ago\nYou should visit the documentation link he attached. He's copy those statements from the Google Docs. Roro_Brother 3Â years ago\nListen that guy because he is right"
      },
      {
        "index": 2,
        "text": "Anonymous: bobthebuilder55110 Highly Voted 2Â years, 11Â months ago\nSelected Answer: A\nAnswer is (A) :\nFirst I was going with C but then I read the question again, let's try to understand both options here, the goal is to deny egress and only allow some ports for some functions to perform. If we go with C, lower the number higher the priority (1000) so the rule with this priority 1000 will overwrite (65534), so If we allow only appropriate ports it will be overwritten with the high-priority (1000) rule and all the egress traffic will be blocked.\nRemember the goal here is to block egress but not all of it since we still want to configure the fewest open ports and this is statefull meaning for open ports traffic will be both ways.\nA fits this condition where it is saying we block all traffic but the required ports are kept open with higher priority which will only allow the required traffic to leave the network."
      },
      {
        "index": 3,
        "text": "Anonymous: Cynthia2023 Most Recent 1Â year, 6Â months ago\nSelected Answer: A\nDefault Egress Behavior: In Google Cloud VPCs, the default behavior is to allow all egress traffic. To restrict egress traffic effectively, you need to explicitly set up firewall rules.\nBlocking All Egress Traffic: The low-priority rule (priority 65534, near the lowest priority) should be configured to block all egress traffic. This creates a baseline rule that denies all egress traffic by default.\nAllowing Specific Ports: The high-priority rule (priority 1000, indicating a higher priority) should be set to allow egress traffic only on the specific ports that are required for the application. Since firewall rules are evaluated in order of priority, this rule will override the default block for these specific ports."
      },
      {
        "index": 4,
        "text": "Anonymous: jimmydice 1Â year, 8Â months ago\nCorrect answer is C: By implementing a high-priority rule to block all egress traffic (since it has a lower number than lower-priority rules), and a low-priority rule to selectively allow specific necessary egress ports (with a higher number), you minimize open egress ports to only the required ones while restricting the rest."
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: A\nThe rule is evaluated on higher priority to lower priority and depends first come first serve basis.\nhttps://cloud.google.com/firewall/docs/firewall-policies-overview#rule-evaluation"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: fragment137 2Â years, 7Â months ago\nSelected Answer: A\nCorrect answer is A.\nAnswer will not be D, because Egress traffic is Allowed by default. You will have to explicitly set the rule blocking outbound traffic."
      },
      {
        "index": 8,
        "text": "Anonymous: ryumada 2Â years, 11Â months ago\nSelected Answer: A\nRead ESP_SAP comment for the explanation. He explains it clearly."
      },
      {
        "index": 9,
        "text": "Anonymous: sonuricky 2Â years, 11Â months ago\nC is the correct answer"
      },
      {
        "index": 10,
        "text": "Anonymous: gscharly 2Â years, 11Â months ago\nSelected Answer: A\nA: is the answer"
      }
    ]
  },
  {
    "id": 62,
    "source": "examtopics",
    "question": "Your company runs its Linux workloads on Compute Engine instances. Your company will be working with a new operations partner that does not use Google\nAccounts. You need to grant access to the instances to your operations partner so they can maintain the installed tooling. What should you do?",
    "options": {
      "A": "Enable Cloud IAP for the Compute Engine instances, and add the operations partner as a Cloud IAP Tunnel User.",
      "B": "Tag all the instances with the same network tag. Create a firewall rule in the VPC to grant TCP access on port 22 for traffic from the operations partner to instances with the network tag.",
      "C": "Set up Cloud VPN between your Google Cloud VPC and the internal network of the operations partner.",
      "D": "Ask the operations partner to generate SSH key pairs, and add the public keys to the VM instances."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kulikBro Highly Voted 4Â years, 9Â months ago\nA - https://cloud.google.com/iap/docs/external-identities SajadAhm 3Â weeks, 1Â day ago\nThe feature in your link is for Web Apps; for SSH access without a Google account, Option D is the intended answer."
      },
      {
        "index": 2,
        "text": "Anonymous: Bhagirathi Highly Voted 5Â years, 1Â month ago\nfull of confusions for any reader....\nYou guys all say A, B, C & D but which one is correct ? yc25744 4Â years, 6Â months ago\nnothing"
      },
      {
        "index": 3,
        "text": "Anonymous: guaose Most Recent 2Â months, 1Â week ago\nSelected Answer: D\nEl socio no usa cuentas de Google, por lo que no puedes asignar roles IAM ni usar Cloud IAP (requiere identidad Google).\nLa forma recomendada para acceso SSH en Compute Engine sin cuentas de Google es usar claves SSH:\nEl socio genera un par de claves (privada y pÃºblica).\nTÃº agregas la clave pÃºblica en la metadata de la VM o en el proyecto."
      },
      {
        "index": 4,
        "text": "Anonymous: dead1407 4Â months, 2Â weeks ago\nSelected Answer: D\nSince the operations partner does not use Google Accounts, the recommended way to grant them access is by using SSH key pairs. Have them generate SSH keys and add their public keys to the VM instances, allowing secure access without requiring Google Accounts."
      },
      {
        "index": 5,
        "text": "Anonymous: f12345112 10Â months, 1Â week ago\nSelected Answer: D\nA is not correct as it requires Google account which operations partner does not use.\nD is correct."
      },
      {
        "index": 6,
        "text": "Anonymous: peddyua 12Â months ago\nSelected Answer: D\nvoting for D"
      },
      {
        "index": 7,
        "text": "Anonymous: ritvikk49 1Â year ago\nSelected Answer: D\nit is D"
      },
      {
        "index": 8,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nSelected Answer: D\nA - wrong Identity-Aware Proxy (IAP) is not designed for external users who do not have Google Accounts.\nB- possible but It does open up port 22, which could pose security risks if not properly managed.\nC- This is a complex solution and ongoing management compared to option D.\nD- This method allows external users to access VMs without needing Google Accounts, using standard SSH key authentication(simple method)"
      },
      {
        "index": 9,
        "text": "Anonymous: imazy 1Â year, 2Â months ago\nSelected Answer: A\nPeople voting for D assuming the Google account is the only mandatory requirement to configure IAP is not true , we can use microsoft, facebook and custom email as well"
      },
      {
        "index": 10,
        "text": "Anonymous: sivakarthick16 1Â year, 3Â months ago\nSelected Answer: D\nA is incorrect because the operations partner does not have a Google account. Activating and enabling Cloud IAP (Identity-Aware Proxy) for the Compute Engine instances would only allow access to users with Google Accounts. In this case, the third-party service provider does not use Google Accounts, so this option would not enable their access."
      }
    ]
  },
  {
    "id": 63,
    "source": "examtopics",
    "question": "You have created a code snippet that should be triggered whenever a new file is uploaded to a Cloud Storage bucket. You want to deploy this code snippet. What should you do?",
    "options": {
      "A": "Use App Engine and configure Cloud Scheduler to trigger the application using Pub/Sub.",
      "B": "Use Cloud Functions and configure the bucket as a trigger resource.",
      "C": "Use Google Kubernetes Engine and configure a CronJob to trigger the application using Pub/Sub.",
      "D": "Use Dataflow as a batch job, and configure the bucket as a data source."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (B):\nGoogle Cloud Storage Triggers\nCloud Functions can respond to change notifications emerging from Google Cloud Storage. These notifications can be configured to trigger in response to various events inside a bucketâ€”object creation, deletion, archiving and metadata updates.\nNote: Cloud Functions can only be triggered by Cloud Storage buckets in the same Google Cloud Platform project.\nEvent types\nCloud Storage events used by Cloud Functions are based on Cloud Pub/Sub Notifications for Google Cloud Storage and can be configured in a similar way.\nSupported trigger type values are:\ngoogle.storage.object.finalize\ngoogle.storage.object.delete\ngoogle.storage.object.archive\ngoogle.storage.object.metadataUpdate\nObject Finalize\nTrigger type value: google.storage.object.finalize\nThis event is sent when a new object is created (or an existing object is overwritten, and a new generation of that object is created) in the bucket.\nhttps://cloud.google.com/functions/docs/calling/storage#event_types"
      },
      {
        "index": 2,
        "text": "Anonymous: francisco_guerra Highly Voted 4Â years, 11Â months ago\nThe answer is B SSPC 4Â years, 11Â months ago\nSure B? Please you could share the link with the Google documentation Ale1973 4Â years, 10Â months ago\nhttps://cloud.google.com/functions/docs/calling/storage"
      },
      {
        "index": 3,
        "text": "Anonymous: Cynthia2023 Most Recent 1Â year, 6Â months ago\nSelected Answer: B\nGoogle Cloud Functions supports several types of triggers, allowing you to run your functions in response to various events in the Google Cloud environment or via HTTP requests."
      },
      {
        "index": 1,
        "text": "HTTP Triggers:\nâ€¢ HTTP triggers allow your Cloud Function to be invoked via standard HTTP requests. These are useful for building APIs, webhooks, and other services that are accessible over the internet or within your internal network."
      },
      {
        "index": 2,
        "text": "Cloud Pub/Sub Triggers:\nâ€¢ Cloud Functions can be triggered by messages published to Cloud Pub/Sub topics. This is useful for asynchronous event-driven architectures and integrating with systems that publish events to Pub/Sub. Cynthia2023 1Â year, 6Â months ago"
      },
      {
        "index": 3,
        "text": "Cloud Storage Triggers:\nâ€¢ Functions can respond to changes in Google Cloud Storage, such as creating, deleting, or updating objects. This is helpful for processing uploaded files, data backups, and more."
      },
      {
        "index": 4,
        "text": "Firestore Triggers:\nâ€¢ These triggers allow functions to execute in response to changes in Google Cloud Firestore data, including document creation, updates, and deletions. They are useful for syncing Firestore data with other data stores, or for handling real-time data updates."
      },
      {
        "index": 5,
        "text": "Firebase Realtime Database Triggers:\nâ€¢ Cloud Functions can be triggered by changes in Firebase Realtime Database. This is similar to Firestore triggers but specific to Firebase's Realtime Database service. Cynthia2023 1Â year, 6Â months ago"
      },
      {
        "index": 6,
        "text": "Firebase Authentication Triggers:\nâ€¢ Functions can react to Firebase Authentication events, such as user creation, deletion, or attribute updates. These triggers are useful for custom user management workflows and integration with external systems."
      },
      {
        "index": 7,
        "text": "Google Analytics for Firebase Triggers:\nâ€¢ These triggers enable functions to respond to Analytics events collected by Firebase, useful for custom event processing and integrations."
      }
    ]
  },
  {
    "id": 64,
    "source": "examtopics",
    "question": "You have been asked to set up Object Lifecycle Management for objects stored in storage buckets. The objects are written once and accessed frequently for 30 days. After 30 days, the objects are not read again unless there is a special need. The objects should be kept for three years, and you need to minimize cost.\nWhat should you do?",
    "options": {
      "A": "Set up a policy that uses Nearline storage for 30 days and then moves to Archive storage for three years.",
      "B": "Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years.",
      "C": "Set up a policy that uses Nearline storage for 30 days, then moves the Coldline for one year, and then moves to Archive storage for two years.",
      "D": "Set up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (B):\nThe key to understand the requirement is : \"The objects are written once and accessed frequently for 30 days\"\nStandard Storage\nStandard Storage is best for data that is frequently accessed (\"hot\" data) and/or stored for only brief periods of time.\nArchive Storage\nArchive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the \"coldest\" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days. Archive Storage is the best choice for data that you plan to access less than once a year.\nhttps://cloud.google.com/storage/docs/storage-classes#standard naveedpk00 4Â years, 10Â months ago\nWhat if we chose option D to minimize the cost as asked in the question? What do you think? gcper 4Â years, 9Â months ago\nIt doesn't minimize the costs. Check the costs of coldline vs archival"
      },
      {
        "index": 2,
        "text": "Anonymous: SSPC Highly Voted 4Â years, 11Â months ago\nI think the correct one is B. Because Nearline has a 30-day minimum storage duration.\nhttps://cloud.google.com/storage/docs/storage-classes pepepy 4Â years, 11Â months ago\nThe object should be kept for three years, and you need to minimize cost, after 30 days it will be moved to archive, ans A pepepy 4Â years, 11Â months ago\nSorry you are right accessed frequently for 30 days, its B"
      },
      {
        "index": 3,
        "text": "Anonymous: Tanidanindo Most Recent 1Â year, 6Â months ago\nSelected Answer: B\nThe objects are written once and accessed frequently for 30 days. Then rarely accessed."
      },
      {
        "index": 4,
        "text": "Anonymous: Jin1206t 1Â year, 8Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nKey terms frequently accessed for 30 days -> Standard storage class.\nNot accessed unless special need for 3 years -> Archive storage class."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct answer, as the data for first 30 days in accessed frequently so for it we can use the standard , and after it to minimize the cost we can use the archive storage for 3 years"
      },
      {
        "index": 7,
        "text": "Anonymous: ashtonez 2Â years, 4Â months ago\nAnswer is B, we cannot select A because data is accedesed frequently and nearline only allows access once per month (you can access more incurring in aditional cost but being not a cost optimized selection)"
      },
      {
        "index": 8,
        "text": "Anonymous: thaliath 2Â years, 6Â months ago\nAnswer is A: there is a retrieval fee for data access from nearline. Please check https://cloud.google.com/storage/docs/storage-classes. So Standard storage is the cheaper option"
      },
      {
        "index": 9,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: B\nB is the correct Answer,\nFrequently accessed data 'Hot Data' should be stored in Standard Storage for 30 days,\nThen this can be moved to Archive after 30 days for period of three years which is accessed only when a special need arises, to reduce cost."
      },
      {
        "index": 10,
        "text": "Anonymous: taiyi078 3Â years ago\nhttps://cloud.google.com/storage/docs/storage-classes#nearline\nNearline storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice.\nNearline storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline storage or Archive storage are more cost-effective, as they offer lower storage costs. taiyi078 3Â years ago\nNearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline storage is a better choice than Standard storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs."
      }
    ]
  },
  {
    "id": 65,
    "source": "examtopics",
    "question": "You are storing sensitive information in a Cloud Storage bucket. For legal reasons, you need to be able to record all requests that read any of the stored data. You want to make sure you comply with these requirements. What should you do?",
    "options": {
      "A": "Enable the Identity Aware Proxy API on the project.",
      "B": "Scan the bucket using the Data Loss Prevention API.",
      "C": "Allow only a single Service Account access to read the data.",
      "D": "Enable Data Access audit logs for the Cloud Storage API."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (D):\nLogged information\nWithin Cloud Audit Logs, there are two types of logs:\nAdmin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object.\nData Access logs: Entries for operations that modify objects or read a project, bucket, or object. There are several sub-types of data access logs:\nADMIN_READ: Entries for operations that read the configuration or metadata of a project, bucket, or object.\nDATA_READ: Entries for operations that read an object.\nDATA_WRITE: Entries for operations that create or modify an object.\nhttps://cloud.google.com/storage/docs/audit-logs#types"
      },
      {
        "index": 2,
        "text": "Anonymous: francisco_guerra Highly Voted 4Â years, 11Â months ago\nD is the correct one SSPC 4Â years, 11Â months ago\nYes D is the correct"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nD is the best answer:\n- Data Access audit logs are specifically designed to track Google Cloud API operations related to data, including reads from Cloud Storage buckets.\n- These logs include details about the user or service account making the request, the time, and the specific data resource accessed.\n- Having this audit trail is essential for demonstrating adherence to regulations around sensitive data handling.\nWhy Others Aren't as Ideal:\nA: Identity-Aware Proxy (IAP): IAP focuses on controlling access to web apps behind firewalls but doesn't inherently log all data read operations.\nB: Data Loss Prevention (DLP): DLP is excellent for identifying sensitive data within your bucket but doesn't provide a continuous audit log of every access.\nC: Restricting Access: While limiting access is a security best practice, it doesn't address the legal requirement to log every read operation."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: D\nEnable Data access audit logs for Cloud storage bucket\nhttps://cloud.google.com/storage/docs/audit-logging"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: calm_fox 2Â years, 7Â months ago\nSelected Answer: D\nOnly logical option"
      },
      {
        "index": 7,
        "text": "Anonymous: AzureDP900 3Â years ago\nD is right for this use case"
      },
      {
        "index": 8,
        "text": "Anonymous: Akash7 3Â years, 2Â months ago\nD is correct as Data Access logs pertaining to Cloud Storage operations are not recorded by default. You have to enable them ...\nhttps://cloud.google.com/storage/docs/audit-logging"
      },
      {
        "index": 9,
        "text": "Anonymous: wael_tn 3Â years, 2Â months ago\nSelected Answer: D\nI think it's D"
      },
      {
        "index": 10,
        "text": "Anonymous: Surat 3Â years, 6Â months ago\nI also vote for D"
      }
    ]
  },
  {
    "id": 66,
    "source": "examtopics",
    "question": "You are the team lead of a group of 10 developers. You provided each developer with an individual Google Cloud Project that they can use as their personal sandbox to experiment with different Google Cloud solutions. You want to be notified if any of the developers are spending above $500 per month on their sandbox environment. What should you do?",
    "options": {
      "A": "Create a single budget for all projects and configure budget alerts on this budget.",
      "B": "Create a separate billing account per sandbox project and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per billing account.",
      "C": "Create a budget per project and configure budget alerts on all of these budgets.",
      "D": "Create a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (C):\nSet budgets and budget alerts\nOverview\nAvoid surprises on your bill by creating Cloud Billing budgets to monitor all of your Google Cloud charges in one place. A budget enables you to track your actual Google Cloud spend against your planned spend. After you've set a budget amount, you set budget alert threshold rules that are used to trigger email notifications. Budget alert emails help you stay informed about how your spend is tracking against your budget."
      },
      {
        "index": 2,
        "text": "Set budget scope\nSet the budget Scope and then click Next.\nIn the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all.\nhttps://cloud.google.com/billing/docs/how-to/budgets#budget-scop dang1986 3Â years, 11Â months ago\nYou're the only answer I take seriously \"Thumbs up\" bobthebuilder55110 3Â years, 5Â months ago\nwait a minute, why not A ?\nAs you said that\n\" In the Projects field, select one or more projects that you want to apply the budget alert to. To apply the budget alert to all the projects in the Cloud Billing account, choose Select all. \"\nAs per this I should be able to create single budget for all the projects and should be able to set alert on that, why create separate budget for all 10 projects ? Priyanka109 3Â years, 3Â months ago\nIt will be a combined budget that's why it's C"
      },
      {
        "index": 2,
        "text": "Anonymous: Hjameel Highly Voted 5Â years, 5Â months ago\nI think C is the best answer."
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nC"
      },
      {
        "index": 4,
        "text": "Anonymous: yehia2221 1Â year, 5Â months ago\nAgree, anwser C, as there is no a common billing account mentioned in the question, we need to create a budget by project."
      },
      {
        "index": 5,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: C\nThe scope of a budget in GCP can be defined at different levels:"
      },
      {
        "index": 1,
        "text": "Project-Level Budget:"
      },
      {
        "index": 2,
        "text": "Billing Account-Level Budget:"
      },
      {
        "index": 3,
        "text": "Specific Services or Labels:\nâ€¢ GCP allows you to create budgets for specific services (like Compute Engine, Cloud Storage, etc.) or resources labeled with specific labels within a project or billing account. This level of granularity is useful for tracking costs associated with particular services or resource categories."
      },
      {
        "index": 4,
        "text": "Credits and Other Filters:\nâ€¢ When setting up a budget, you can include or exclude certain types of costs, such as credits, discounts, or taxes, depending on your monitoring needs."
      }
    ]
  },
  {
    "id": 67,
    "source": "examtopics",
    "question": "You are deploying a production application on Compute Engine. You want to prevent anyone from accidentally destroying the instance by clicking the wrong button. What should you do?",
    "options": {
      "A": "Disable the flag ×’â‚¬Delete boot disk when instance is deleted.×’â‚¬",
      "B": "Enable delete protection on the instance.",
      "C": "Disable Automatic restart on the instance.",
      "D": "Enable Preemptibility on the instance."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (B):\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation.\nAs part of your workload, there might be certain VM instances that are critical to running your application or services, such as an instance running a SQL server, a server used as a license manager, and so on. These VM instances might need to stay running indefinitely so you need a way to protect these VMs from being deleted.\nBy setting the deletionProtection flag, a VM instance can be protected from accidental deletion. If a user attempts to delete a VM instance for which you have set the deletionProtection flag, the request fails. Only a user that has been granted a role with compute.instances.create permission can reset the flag to allow the resource to be deleted.\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion Naree 1Â year, 6Â months ago\nMr.ESP_SAP, your answers are on the spot and I look forward to your notes on all the questions first.. Appreciate your effort and support for this cloud community.. :)"
      },
      {
        "index": 2,
        "text": "Anonymous: MohammedGhouse Highly Voted 4Â years, 5Â months ago\n\"B\" is the answer"
      },
      {
        "index": 3,
        "text": "Anonymous: mufuuuu Most Recent 1Â year, 1Â month ago\nSelected Answer: B\nB. Enable delete protection on the instance. mufuuuu 1Â year, 1Â month ago\nEnabling delete protection helps safeguard your instances from accidental deletion. This means that even if someone attempts to delete the instance through the console or API, they will receive an error, preventing accidental deletion. It acts as an additional layer of protection to avoid critical mistakes."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: B\nhttps://cloud.google.com/compute/docs/instances/preventing-accidental-vm-deletion"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB is the correct answer , as it helps to prevent critical instance to get deleted"
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nThis is straight forward question, enable delete protection. B is right"
      },
      {
        "index": 7,
        "text": "Anonymous: Himadhar1997 2Â years, 7Â months ago\nSelected Answer: B\nPreventing Accidental VM Deletion\nThis document describes how to protect specific VM instances from deletion by setting the deletionProtection property on an Instance resource. To learn more about VM instances, read the Instances documentation."
      },
      {
        "index": 8,
        "text": "Anonymous: Surat 3Â years ago\nB seems right option"
      },
      {
        "index": 9,
        "text": "Anonymous: kped21 3Â years ago\nB - on VM Enable delete protection"
      },
      {
        "index": 10,
        "text": "Anonymous: jaffarali 3Â years, 1Â month ago\nSelected Answer: B\nAnswer is B. there is an Option in VM instance while creating"
      }
    ]
  },
  {
    "id": 68,
    "source": "examtopics",
    "question": "Your company uses a large number of Google Cloud services centralized in a single project. All teams have specific projects for testing and development. The\nDevOps team needs access to all of the production services in order to perform their job. You want to prevent Google Cloud product changes from broadening their permissions in the future. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Grant all members of the DevOps team the role of Project Editor on the organization level.",
      "B": "Grant all members of the DevOps team the role of Project Editor on the production project.",
      "C": "Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the production project.",
      "D": "Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the organization level."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 11Â months ago\nCorrect Answer is (C):\nUnderstanding IAM custom roles\nKey Point: Custom roles enable you to enforce the principle of least privilege, ensuring that the user and service accounts in your organization have only the permissions essential to performing their intended functions.\nBasic concepts\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. Custom roles are not maintained by Google; when new permissions, features, or services are added to Google Cloud, your custom roles will not be updated automatically.\nWhen you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\nhttps://cloud.google.com/iam/docs/understanding-custom-roles#basic_concepts"
      },
      {
        "index": 2,
        "text": "Anonymous: SSPC Highly Voted 4Â years, 11Â months ago\n\"You want to prevent Google Cloud product changes from broadening their permissions in the future.\" then CUSTOM ROLE Rothmansua 3Â years, 9Â months ago\nGreat hint, thanks!"
      },
      {
        "index": 3,
        "text": "Anonymous: ram2022 Most Recent 1Â year, 7Â months ago\nSelected Answer: B\nThe answer would be B as it will help the DevOps team to work on any resources for others future production project. kuracpalac 1Â year, 4Â months ago\nBut if Google change their roles, they can broaden the rights to those engineers, so that would be a wrong answer IMO. C looks like the correct one from the list."
      },
      {
        "index": 4,
        "text": "Anonymous: rahulrauki 1Â year, 9Â months ago\nSelected Answer: C\nThe giveaway is \"prevent google cloud product changes from broadening their permissions\". Which means that we need to create a custom role. Also they mentioned all production services and not production projects so C"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: C\nCustom roles help you enforce the principle of least privilege, because they help to ensure that the principals in your organization have only the permissions that they need.\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. When you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\nNote: You cannot define custom roles at the folder level. If you need to use a custom role within a folder, define the custom role at the organization level.\nhttps://cloud.google.com/iam/docs/roles-overview#custom"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: C\nCustom roles help you enforce the principle of least privilege, because they help to ensure that the principals in your organization have only the permissions that they need.\nCustom roles are user-defined, and allow you to bundle one or more supported permissions to meet your specific needs. When you create a custom role, you must choose an organization or project to create it in. You can then grant the custom role on the organization or project, as well as any resources within that organization or project.\nNote: You cannot define custom roles at the folder level. If you need to use a custom role within a folder, define the custom role at the organization level.\nhttps://cloud.google.com/iam/docs/roles-overview#custom"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC is the correct answer"
      },
      {
        "index": 8,
        "text": "Anonymous: sabrinakloud 2Â years, 3Â months ago\nSelected Answer: C\nC is correct"
      },
      {
        "index": 9,
        "text": "Anonymous: slcvlctetri 2Â years, 6Â months ago\nSelected Answer: C\nHad this question 2 days ago. C is correct."
      },
      {
        "index": 10,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: C\nC is the correct answer, give the devops team the least privileged role, only the required permissions to access the production services, as the question states 'to prevent product changes' for which editor role is not recommended either at Project or organizational level, organizational level access gives broad scope to all the projects in the organization, this role cannot be given to the devops team.\nA. Editor has privilege to change the products, and the scope is broad\nB. Editor has privilege to change the products\nC. Recommended, as this will give only required permission at project level to devops team.\nD. They require only project level access. This gives access to all project in the organization."
      }
    ]
  },
  {
    "id": 69,
    "source": "examtopics",
    "question": "You are building an application that processes data files uploaded from thousands of suppliers. Your primary goals for the application are data security and the expiration of aged data. You need to design the application to:\n* Restrict access so that suppliers can access only their own data.\n* Give suppliers write access to data only for 30 minutes.\n* Delete data that is over 45 days old.\nYou have a very short development cycle, and you need to make sure that the application requires minimal maintenance. Which two strategies should you use?\n(Choose two.)",
    "options": {
      "A": "Build a lifecycle policy to delete Cloud Storage objects after 45 days.",
      "B": "Use signed URLs to allow suppliers limited time access to store their objects.",
      "C": "Set up an SFTP server for your application, and create a separate user for each supplier.",
      "D": "Build a Cloud function that triggers a timer of 45 days to delete objects that have expired."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 4Â months ago\nCorrect Answers are: (AB):\n(A) Object Lifecycle Management\nDelete\nThe Delete action deletes an object when the object meets all conditions specified in the lifecycle rule.\nException: In buckets with Object Versioning enabled, deleting the live version of an object causes it to become a noncurrent version, while deleting a noncurrent version deletes that version permanently.\nhttps://cloud.google.com/storage/docs/lifecycle#delete\n(B) Signed URLs\nThis page provides an overview of signed URLs, which you use to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "index": 2,
        "text": "Anonymous: francisco_guerra Highly Voted 4Â years, 5Â months ago\nAB is the answer"
      },
      {
        "index": 3,
        "text": "Anonymous: scanner2 Most Recent 1Â year, 4Â months ago\nSelected Answer: AB\nCreate object lifecycle policy to automatically delete the objects after 45 days. Create signed URLs to temporarily provide the access for specified amount of time."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: AB\nAB is the correct answer, as A helps to make a lifecycle policy to delete the data after 45 days and b helps the customer to acces their data as per the question requiremnt"
      },
      {
        "index": 5,
        "text": "Anonymous: jrisl1991 1Â year, 11Â months ago\nSelected Answer: AB\nIt's a bit obvious. Cloud functions wouldn't really work well with this and would probably require lots of maintenance just as any other option. AB are the correct ones."
      },
      {
        "index": 6,
        "text": "Anonymous: diasporabro 2Â years, 2Â months ago\nSelected Answer: AB\nAB achieves this objective"
      },
      {
        "index": 7,
        "text": "Anonymous: olme59 2Â years, 4Â months ago\nSelected Answer: AB\nits clearly AB, life cycle and provider private url"
      },
      {
        "index": 8,
        "text": "Anonymous: Angel_99 2Â years, 5Â months ago\nSelected Answer: AB\nCorrect Answer Combo: (AB)"
      },
      {
        "index": 9,
        "text": "Anonymous: abirroy 2Â years, 5Â months ago\nSelected Answer: AB\nCorrect Answers are: (AB)"
      },
      {
        "index": 10,
        "text": "Anonymous: patashish 2Â years, 6Â months ago\nCorrect Answers are: A and B"
      }
    ]
  },
  {
    "id": 70,
    "source": "examtopics",
    "question": "Your company wants to standardize the creation and management of multiple Google Cloud resources using Infrastructure as Code. You want to minimize the amount of repetitive code needed to manage the environment. What should you do?",
    "options": {
      "A": "Develop templates for the environment using Cloud Deployment Manager.",
      "B": "Use curl in a terminal to send a REST request to the relevant Google API for each individual resource.",
      "C": "Use the Cloud Console interface to provision and manage all related resources.",
      "D": "Create a bash script that contains all requirement steps as gcloud commands."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: jmgf Highly Voted 4Â years, 4Â months ago\nA\nYou can use Google Cloud Deployment Manager to create a set of Google Cloud resources and manage them as a unit, called a deployment. For example, if your team's development environment needs two virtual machines (VMs) and a BigQuery database, you can define these resources in a configuration file, and use Deployment Manager to create, change, or delete these resources. You can make the configuration file part of your team's code repository, so that anyone can create the same environment with consistent results.\nhttps://cloud.google.com/deployment-manager/docs/quickstart"
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nA. Develop templates for the environment using Cloud Deployment Manager."
      },
      {
        "index": 3,
        "text": "Anonymous: omunoz Most Recent 1Â year, 2Â months ago\nA.\nInfrastructure as Code = Cloud Deployment Manager."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: A\nCloud Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Google Cloud services, such as Cloud Storage, Compute Engine, and Cloud SQL, configured to work together.\nYou can use Google Cloud Deployment Manager to create a set of Google Cloud resources and manage them as a unit, called a deployment.\nhttps://cloud.google.com/deployment-manager/docs"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA seems more correct, u can use the deployement manager to create your instances with the same configurstion file"
      },
      {
        "index": 6,
        "text": "Anonymous: N_A 2Â years, 2Â months ago\nSelected Answer: A\nA. Develop templates for the environment using Cloud Deployment Manager.\nAlthough the preferred IaC tool is Terraform. There no mention of Deployment Manager anymore in the Google on-demand courses but there is an entire course on Terraform."
      },
      {
        "index": 7,
        "text": "Anonymous: PPP_D 2Â years, 3Â months ago\nI'm going with A"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3Â years ago\nA is right"
      },
      {
        "index": 9,
        "text": "Anonymous: POOJA3808 3Â years, 4Â months ago\nSelected Answer: A\nDevelop templates for the environment using Cloud Deployment Manager."
      },
      {
        "index": 10,
        "text": "Anonymous: look1 3Â years, 7Â months ago\nSelected Answer: A\nTemplates only"
      }
    ]
  },
  {
    "id": 71,
    "source": "examtopics",
    "question": "You are performing a monthly security check of your Google Cloud environment and want to know who has access to view data stored in your Google Cloud\nProject. What should you do?",
    "options": {
      "A": "Enable Audit Logs for all APIs that are related to data storage.",
      "B": "Review the IAM permissions for any role that allows for data access.",
      "C": "Review the Identity-Aware Proxy settings for each resource.",
      "D": "Create a Data Loss Prevention job."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JelloMan Highly Voted 3Â years, 1Â month ago\nSelected Answer: B\nOnly use audit logs to look at history (PAST)\nIf you need current, up-to-date, info regarding permissions always go to IAM"
      },
      {
        "index": 2,
        "text": "Anonymous: Alejondri Highly Voted 3Â years, 2Â months ago\nSelected Answer: B\nB is the one:\nA. Enable Audit Logs for all APIs that are related to data storage. --> That is not the correct answer, if someone with permissions has not accessed or does not access, it will not be listed.\nB. Review the IAM permissions for any role that allows for data access. --> That's correct\nC. Review the Identity-Aware Proxy settings for each resource. --> Nothing relevant, Proxy? Is configured? The question don't ask or tell something about if it is configured.\nD. Create a Data Loss Prevention job. --> Data Loss Prevention nothing to see here."
      },
      {
        "index": 3,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\nSelected Answer: B\n\"who has access\" > We neet to know present, so check IAM > it is B\nIf question says, check who has accessed, yes is past, audit logs"
      },
      {
        "index": 4,
        "text": "Anonymous: Jonassamr 1Â year, 2Â months ago\nSelected Answer: B\nacces => IAM\nHistory => Logs"
      },
      {
        "index": 5,
        "text": "Anonymous: snkhatri 2Â years, 10Â months ago\nB \"WHO HAS ACCESS\" Naree 2Â years ago\nYes, that's the catch.. The question here is \"Who has access?\" and not \"Who has accessed?\"\nAnswer is \"B\"."
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 3Â years ago\nB is correct"
      },
      {
        "index": 7,
        "text": "Anonymous: akshaychavan7 3Â years, 1Â month ago\nSelected Answer: B\nWithout any doubt, it's B."
      },
      {
        "index": 8,
        "text": "Anonymous: Terzlightyear 3Â years, 2Â months ago\nSelected Answer: B\nB is the one sdflkds 3Â years, 2Â months ago\nB. 'Audit logs help you answer \"who did what, where, and when?\"'(from https://cloud.google.com/logging/docs/audit). So, not who has access, but rather who accessed."
      },
      {
        "index": 9,
        "text": "Anonymous: Maltb 3Â years, 2Â months ago\nSelected Answer: A\nLa rÃ©ponse A."
      }
    ]
  },
  {
    "id": 72,
    "source": "examtopics",
    "question": "Your company has embraced a hybrid cloud strategy where some of the applications are deployed on Google Cloud. A Virtual Private Network (VPN) tunnel connects your Virtual Private Cloud (VPC) in Google Cloud with your company's on-premises network. Multiple applications in Google Cloud need to connect to an on-premises database server, and you want to avoid having to change the IP configuration in all of your applications when the IP of the database changes.\nWhat should you do?",
    "options": {
      "A": "Configure Cloud NAT for all subnets of your VPC to be used when egressing from the VM instances.",
      "B": "Create a private zone on Cloud DNS, and configure the applications with the DNS name.",
      "C": "Configure the IP of the database as custom metadata for each instance, and query the metadata server.",
      "D": "Query the Compute Engine internal DNS from the applications to retrieve the IP of the database."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kopper2019 Highly Voted 3Â years, 9Â months ago\nB,\nForwarding zones\nCloud DNS forwarding zones let you configure target name servers for specific private zones. Using a forwarding zone is one way to implement outbound DNS forwarding from your VPC network.\nA Cloud DNS forwarding zone is a special type of Cloud DNS private zone. Instead of creating records within the zone, you specify a set of forwarding targets. Each forwarding target is an IP address of a DNS server, located in your VPC network, or in an on-premises network connected to your VPC network by Cloud VPN or Cloud Interconnect.\nA does not apply, that is to provide internet access to resources\nC, does not apply\nD, I don't get it\nso B MacFreak 2Â years, 4Â months ago\n\"A does not apply, that is to provide internet access to resources\" - do you really think NAT is only being used between public and private? Well...it's not! :) meh009 3Â years, 9Â months ago\nAgreed, It's B although I chose A intitally. After some careful consideration and understanding how Cloud NAT works, I'm sticking with B\nhttps://cloud.google.com/nat/docs/overview meh009 3Â years, 9Â months ago\nFurther clarification:\n''On-premises clients can resolve records in private zones, forwarding zones, and peering zones for which the VPC network has been authorized. On-premises clients use Cloud VPN or Cloud Interconnect to connect to the VPC network.'' djgodzilla 3Â years, 7Â months ago\nthis is talking about On-premises client resolving nodes outside their network . the question is about how would the application tier within the VPC would resolve the database server . you're confusing the resolution direction my friend djgodzilla 3Â years, 7Â months ago\nIt is still B , but it's rather outbound forward that's needed here :\nDNS outbound Forwarding :\n- Set up outbound forwarding private zones to query on-premises servers (On-prem Authoritative Zone: corp.example.com)\n- In Cloud Router , add a custom route advertisement for GCP DNS proxies range 35.199.192.0/19 to the on-premises environment.\n- Make sure inbound DNS traffic from 35.199.192.0/19 is allowed on on-prem firewall\n- Cloud Router should be learning on-prem network route from On-prem Router\nhttps://youtu.be/OH_Jw8NhEGU?t=1283\nhttps://cloud.google.com/dns/docs/best-practices#use_forwarding_zones_to_query_on-premises_servers"
      },
      {
        "index": 2,
        "text": "Anonymous: pondai Highly Voted 3Â years, 9Â months ago\nhttps://cloud.google.com/dns/docs/best-practices#best_practices_for_dns_forwarding_zones_and_server_policies\nCloud DNS offers DNS forwarding zones and DNS server policies to allow lookups of DNS names between your on-premises and Google Cloud environment. You have multiple options for configuring DNS forwarding. The following section lists best practices for hybrid DNS setup. These best practices are illustrated in the Reference architectures for hybrid DNS.\nSo I think B is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: thewalker Most Recent 1Â year, 1Â month ago\nSelected Answer: B\nB\nhttps://cloud.google.com/dns/docs/overview"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: B\nDNS is a hierarchical distributed database that lets you store IP addresses and other data and look them up by name. Cloud DNS lets you publish your zones and records in DNS without the burden of managing your own DNS servers and software.\nCloud DNS offers both public zones and private managed DNS zones. A public zone is visible to the public internet, while a private zone is visible only from one or more Virtual Private Cloud (VPC) networks that you specify.\nhttps://cloud.google.com/dns/docs/overview"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB is the correct Answer"
      },
      {
        "index": 6,
        "text": "Anonymous: jrisl1991 1Â year, 11Â months ago\nSelected Answer: B\nBased on this - https://cloud.google.com/dns/docs/overview#dns-forwarding-methods B must be the best option. I don't think there's a \"typo\" (or completely wrongly worded answer) in option D (there's comments saying that instead of Compute Engine it should be on-premise). I believe option D is wrong on purpose to create a confusion."
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 2Â years, 3Â months ago\nSelected Answer: B\nB is correct answer,\nConfigure Private Google Access for on-premises hosts,\nDNS configuration\nYour on-premises network must have DNS zones and records configured so that Google domain names resolve to the set of IP addresses for either private.googleapis.com or restricted.googleapis.com. You can create Cloud DNS managed private zones and use a Cloud DNS inbound server policy, or you can configure on-premises name servers. For example, you can use BIND or Microsoft Active Directory DNS.\nhttps://cloud.google.com/vpc/docs/configure-private-google-access-hybrid#config-domain"
      },
      {
        "index": 8,
        "text": "Anonymous: aforolt 2Â years, 3Â months ago\nAns is D, looks like there is typo"
      },
      {
        "index": 9,
        "text": "Anonymous: habros 2Â years, 5Â months ago\nB. DNS works best with dynamic IPs."
      },
      {
        "index": 10,
        "text": "Anonymous: patashish 2Â years, 6Â months ago\nCorrect Ans is B\nRef - https://cloud.google.com/dns/docs/best-practices#best_practices_for_private_zones"
      }
    ]
  },
  {
    "id": 73,
    "source": "examtopics",
    "question": "You have developed a containerized web application that will serve internal colleagues during business hours. You want to ensure that no costs are incurred outside of the hours the application is used. You have just created a new Google Cloud project and want to deploy the application. What should you do?",
    "options": {
      "A": "Deploy the container on Cloud Run for Anthos, and set the minimum number of instances to zero.",
      "B": "Deploy the container on Cloud Run (fully managed), and set the minimum number of instances to zero.",
      "C": "Deploy the container on App Engine flexible environment with autoscaling, and set the value min_instances to zero in the app.yaml.",
      "D": "Deploy the container on App Engine flexible environment with manual scaling, and set the value instances to zero in the app.yaml."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: crysk Highly Voted 4Â years, 10Â months ago\nI think that is B the correct answer, because Cloud Run can scale to 0:\nhttps://cloud.google.com/run/docs/about-instance-autoscaling\nAnd App Engine Flexible can't scale to 0, the minimum instance number is 1:\nhttps://cloud.google.com/appengine/docs/the-appengine-environments#comparing_high-level_features ryumada 3Â years, 5Â months ago\nNo for the App Engine Flexible Environment, but App Engine Standard can also scale to zero."
      },
      {
        "index": 2,
        "text": "Anonymous: pca2b Highly Voted 4Â years, 9Â months ago\nB:\nnot A because Anthos is an add-on to GKE clusters, 'new project' means we dont have a GKE cluster to work with\nhttps://cloud.google.com/kuberun/docs/architecture-overview#components_in_the_default_installation"
      },
      {
        "index": 3,
        "text": "Anonymous: JoseCloudEng1994 Most Recent 1Â year ago\nSelected Answer: B\nTrick question.\n- With AppEngine you pay all the time for the resources that you are using\n- With Cloud Run you pay for startup/shutdown + ONLY the time the app is servicing requests. Since they clarify 'during business' hours the answer is B, definitively tricky though"
      },
      {
        "index": 4,
        "text": "Anonymous: 09bd94b 1Â year, 1Â month ago\nSelected Answer: B\nCloud Run is the only option provided able to scale to 0"
      },
      {
        "index": 5,
        "text": "Anonymous: ccpmad 1Â year, 8Â months ago\nSelected Answer: B\nContainer = cloud run (not App Engine)\nOn AE, app runs as a node process, like booting it up with npm start locally. AE is a traditional hosting platform: it runs continuously and serves requests as they come in. At the end of the month, you pay for the amount of time it was running, which is typically â€œthe entire monthâ€.\nCloud Run runs containers, so for each release you have to build a container and push it to GCP. Unlike App Engine, Cloud Run only runs when requests come in, so you donâ€™t pay for time spent idling."
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: B\nCloud Run is a managed compute platform that lets you run containers directly on top of Google's scalable infrastructure.\nCloud Run adds and removes instances automatically to handle all incoming requests. If there are no incoming requests to your service, even the last remaining instance will be removed. This behavior is commonly referred to as scale to zero.\nhttps://cloud.google.com/run/docs/overview/what-is-cloud-run"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB is the correct answer, in c, d as App engine flexible environment can't scale to Zero and A in this GKE cluster is used but we have just created a project so it will add extra cost"
      },
      {
        "index": 8,
        "text": "Anonymous: N_A 2Â years, 8Â months ago\nSelected Answer: B\nC. and D. are wrong answers as only the App Engine standard environment scales down to zero.\nAnswer A. will incur extra cost as Cloud Run for Anthos runs on Kubernetes, so need to have a k8s cluster available.\nB. Is correct, as \"Cloud Run automatically scales up or down from zero to N depending on traffic, leveraging container image streaming for a fast startup time.\" from https://cloud.google.com/run"
      },
      {
        "index": 9,
        "text": "Anonymous: Charumathi 3Â years, 3Â months ago\nSelected Answer: B\nB is the correct answer,\nCloud Functions can scale to zero, whereas App Engine will not be able to scale to zero, it should have at least one instance.\nAdd-on Info,\nApp-Engine Standard can scale to zero, whereas App-Engine Flexible couldn't scale down to zero."
      },
      {
        "index": 10,
        "text": "Anonymous: sylva91 3Â years, 4Â months ago\nSelected Answer: B\nB is the answer since we can scale to 0 and the other key word is \"containerized\""
      }
    ]
  },
  {
    "id": 74,
    "source": "examtopics",
    "question": "You have experimented with Google Cloud using your own credit card and expensed the costs to your company. Your company wants to streamline the billing process and charge the costs of your projects to their monthly invoice. What should you do?",
    "options": {
      "A": "Grant the financial team the IAM role of ×’â‚¬Billing Account User×’â‚¬ on the billing account linked to your credit card.",
      "B": "Set up BigQuery billing export and grant your financial department IAM access to query the data.",
      "C": "Create a ticket with Google Billing Support to ask them to send the invoice to your company.",
      "D": "Change the billing account of your projects to the billing account of your company."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: j_mrn Highly Voted 3Â years, 10Â months ago\n1000% Ans D"
      },
      {
        "index": 2,
        "text": "Anonymous: rsuresh27 Highly Voted 2Â years, 8Â months ago\nPlease do not overthink the question. The question does not mention anything about finance teams, so A cannot be correct. D is the only one that makes sense out of the remaining options."
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: D\nThe question is NOT asking to view it by granting IAM roles. What you want is the cost to go to your company. Option D will assign it to your company billing."
      },
      {
        "index": 4,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: D\nD is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: N_A 1Â year, 8Â months ago\nWith A. the financial team can only link the billing account linked to the credit card.\nB. C. are wrong (no comment).\nD. is the only correct answer even though it doesn't give the exact permission to grant in oder to do this. I guess the financial team already has the Billing Project Manager role at a folder or organization level which would allow them to make the change."
      },
      {
        "index": 6,
        "text": "Anonymous: iamlearning2 1Â year, 11Â months ago\nSelected Answer: D\nD is the answer"
      },
      {
        "index": 7,
        "text": "Anonymous: anjanc 2Â years ago\nSelected Answer: D\nd seems the answer"
      },
      {
        "index": 8,
        "text": "Anonymous: Cornholio_LMC 2Â years, 3Â months ago\nhad this one today"
      },
      {
        "index": 9,
        "text": "Anonymous: deadlydeb 2Â years, 6Â months ago\nSelected Answer: D\nD D D D D"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 2Â years, 6Â months ago\nD is perfect"
      }
    ]
  },
  {
    "id": 75,
    "source": "examtopics",
    "question": "You are running a data warehouse on BigQuery. A partner company is offering a recommendation engine based on the data in your data warehouse. The partner company is also running their application on Google Cloud. They manage the resources in their own project, but they need access to the BigQuery dataset in your project. You want to provide the partner company with access to the dataset. What should you do?",
    "options": {
      "A": "Create a Service Account in your own project, and grant this Service Account access to BigQuery in your project.",
      "B": "Create a Service Account in your own project, and ask the partner to grant this Service Account access to BigQuery in their project.",
      "C": "Ask the partner to create a Service Account in their project, and have them give the Service Account access to BigQuery in their project.",
      "D": "Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nD. Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project."
      },
      {
        "index": 2,
        "text": "Anonymous: pondai Highly Voted 4Â years, 3Â months ago\nBigQuery is in our project,so we need to create a service account and grant it access BigQuery role.That can make partner company to use this account to use it to access our project's BigQuery.So I vote A. akshaychavan7 3Â years, 1Â month ago\nYour understanding is bit wrong here, my friend! tavva_prudhvi 4Â years, 3Â months ago\nSee, the ones who want our access needs to create a service account(in our case it's the partner company), then we give access to the service account with the user permissions. Clearly, D says the same thing!"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: D\nD is the best answer:\n- By having the partner create a Service Account and you granting access to it, you maintain ownership and control over your BigQuery data.\n- You can specifically grant the partner's Service Account the necessary BigQuery permissions (e.g., \"BigQuery Data Viewer\"), avoiding overly broad access.\n- Each company manages Service Accounts within their own projects, maintaining a separation of concerns.\nWhy Others Aren't as Ideal:\nA & B: Creating a Service Account in your project and sharing it with the partner (or vice versa) introduces potential management complexities and blurs the lines of responsibility for that Service Account.\nC: Giving the partner company full control to grant their own service accounts access to your dataset could open up broader access than intended."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: D\nCross project access. Application in Project A want to access a service in project B."
      },
      {
        "index": 1,
        "text": "Create a service account in project A."
      },
      {
        "index": 2,
        "text": "Give the required permission to access the services in project B."
      },
      {
        "index": 5,
        "text": "Anonymous: N_A 2Â years, 2Â months ago\nSelected Answer: D\nA. Useless if the private key of the Service Account is not shared with the partner (this would not be a good practice in terms of security)\nB. Not possible.\nC. Useless as the won't have access to the data in our data warehouse on BigQuery.\nD. Is the correct answer and follow best practices."
      },
      {
        "index": 6,
        "text": "Anonymous: hiromi 2Â years, 8Â months ago\nSelected Answer: D\nShould be D"
      },
      {
        "index": 7,
        "text": "Anonymous: Aninina 2Â years, 8Â months ago\nSelected Answer: D\n\"Service accounts are both identities and resources. Because service accounts are identities, you can let a service account access resources in your project by granting it a role, just like you would for any other principal.\""
      },
      {
        "index": 8,
        "text": "Anonymous: Cornholio_LMC 2Â years, 10Â months ago\nhad this one today"
      }
    ]
  },
  {
    "id": 76,
    "source": "examtopics",
    "question": "Your web application has been running successfully on Cloud Run for Anthos. You want to evaluate an updated version of the application with a specific percentage of your production users (canary deployment). What should you do?",
    "options": {
      "A": "Create a new service with the new version of the application. Split traffic between this version and the version that is currently running.",
      "B": "Create a new revision with the new version of the application. Split traffic between this version and the version that is currently running.",
      "C": "Create a new service with the new version of the application. Add an HTTP Load Balancer in front of both services.",
      "D": "Create a new revision with the new version of the application. Add an HTTP Load Balancer in front of both revisions."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: crysk Highly Voted 4Â years, 10Â months ago\nIn my opinion correct answer is B:\nhttps://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration?utm_campaign=CDR_ahm_aap-severless_cloud-run-faq_&utm_source=external&utm_medium=web\nCloud Run can split traffic between revisions TAvenger 4Â years, 10Â months ago\nThe google doc link is incorrect. You need to specify CloudRun for Anthos\nhttps://cloud.google.com/kuberun/docs/rollouts-rollbacks-traffic-migration\nAnyway principles for CloudRun and CloundRun for Anthos are the same. Traffic can be split between multiple revisions.\nThe answer is \"B\""
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 10Â months ago\nB. Create a new revision with the new version of the application. Split traffic between this version and the version that is currently running."
      },
      {
        "index": 3,
        "text": "Anonymous: scanner2 Most Recent 2Â years, 4Â months ago\nSelected Answer: B\nhttps://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "index": 4,
        "text": "Anonymous: kumar262639 2Â years, 8Â months ago\nwhich answer is correct. the one \"Correct Answer\" or the Community vote distribution winner ? idontlikeme_3342 2Â years, 4Â months ago\nI recommend to you to go with the answer that has the most number of Upvotes iooj 1Â year, 4Â months ago\nI recommend that you analyze all options and make an informed decision, rather than just following the upvotes."
      },
      {
        "index": 5,
        "text": "Anonymous: Emmanski08 3Â years ago\nKeyword - \"Updated Version\"\nB. Create a new \"revision\""
      },
      {
        "index": 6,
        "text": "Anonymous: AzFarid 3Â years ago\nB is ok"
      },
      {
        "index": 7,
        "text": "Anonymous: Untamables 3Â years, 2Â months ago\nSelected Answer: B\nThe latest Document\nhttps://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration"
      },
      {
        "index": 8,
        "text": "Anonymous: nosense 3Â years, 2Â months ago\nSelected Answer: B\nB. Create a new revision"
      },
      {
        "index": 9,
        "text": "Anonymous: sylva91 3Â years, 4Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 10,
        "text": "Anonymous: thimai 3Â years, 4Â months ago\nSelected Answer: B\ni think B"
      }
    ]
  },
  {
    "id": 77,
    "source": "examtopics",
    "question": "Your company developed a mobile game that is deployed on Google Cloud. Gamers are connecting to the game with their personal phones over the Internet. The game sends UDP packets to update the servers about the gamers' actions while they are playing in multiplayer mode. Your game backend can scale over multiple virtual machines (VMs), and you want to expose the VMs over a single IP address. What should you do?",
    "options": {
      "A": "Configure an SSL Proxy load balancer in front of the application servers.",
      "B": "Configure an Internal UDP load balancer in front of the application servers.",
      "C": "Configure an External HTTP(s) load balancer in front of the application servers.",
      "D": "Configure an External Network load balancer in front of the application servers."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kopper2019 Highly Voted 4Â years, 3Â months ago\nAnswer is D, cell phones are sending UDP packets and the only that can receive that type of traffic is a External Network TCP/UDP\nhttps://cloud.google.com/load-balancing/docs/network ashrafh 3Â years, 11Â months ago\nGoogle Cloud HTTP(S) Load Balancing is a global, proxy-based Layer 7 load balancer that enables you to run and scale your services worldwide behind a single external IP address. External HTTP(S) Load Balancing distributes HTTP and HTTPS traffic to backends hosted on Compute Engine and Google Kubernetes Engine (GKE).\nhttps://cloud.google.com/load-balancing/docs/https patashish 3Â years ago\nwhat you are trying to say ? What is your answer ? A B C D ? ryumada 2Â years, 11Â months ago\nAll the load balancer products in GCP give you a single IP address for the backend servers you registered to it.\nAlso, External HTTP(s) load balancer only support the port that used by HTTP which is the port 80 and HTTPS which is the port 443.\nAnd Google Cloud external TCP/UDP Network Load Balancing is referred to as \"Network Load Balancing\" which supports UDP packets.\n- https://cloud.google.com/load-balancing/docs/load-balancing-overview#about\n- https://cloud.google.com/load-balancing/docs/network\n- https://cloud.google.com/load-balancing/docs/https"
      },
      {
        "index": 2,
        "text": "Anonymous: JH86 Highly Voted 4Â years, 1Â month ago\nAnswer is D. there are so many confusion here, from B,C or D. For myself im eliminating all options except B,D due to the traffic type. which leaves me with B or D. Then next the traffic source either external or internal which in this case is an external traffic from the internet, therefore my final answer is D.\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer BobbyFlash 3Â years, 8Â months ago\nFollowing the diagram, there's no doubt about D. We have external clients connecting to our gaming service on google cloud that works using UDP traffic that results in using External Network Load Balancing. I feel that it's simple as it is. I also go with D."
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: D\nAn External Network Load Balancer supports UDP traffic and allows you to expose multiple backend VMs over a single public IP address, which is ideal for your multiplayer game scenario. The other load balancer types do not support UDP or are intended for internal traffic only."
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: D"
      },
      {
        "index": 1,
        "text": "UDP Traffic Support:\nâ€¢ An external Network Load Balancer in Google Cloud supports both TCP and UDP traffic. Since your game uses UDP packets for multiplayer interactions, the Network Load Balancer is appropriate for handling this type of traffic."
      },
      {
        "index": 2,
        "text": "Single IP for Multiple VMs:\nâ€¢ Network Load Balancers allow you to use a single, anycast IP address that can distribute incoming traffic across multiple VMs in your backend. This aligns with your requirement to expose the backend servers through a single IP address."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nfor udp external load balancer, D is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: CVGCP 2Â years, 1Â month ago\nBy elimination\nA: SSL proxy LB is for TCP traffic not for UDP, eliminated\nB: External LB is required, Eliminated\nC: Http LB works at layer 7, here protocol is UDP, eliminated\nD: Correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: kumar262639 2Â years, 2Â months ago\n\"Correct Answer\" says A and community vote says D(100%)\nwhich one is correct?"
      },
      {
        "index": 8,
        "text": "Anonymous: PPP_D 2Â years, 3Â months ago\nGoing with D"
      }
    ]
  },
  {
    "id": 78,
    "source": "examtopics",
    "question": "You are working for a hospital that stores its medical images in an on-premises data room. The hospital wants to use Cloud Storage for archival storage of these images. The hospital wants an automated process to upload any new medical images to Cloud Storage. You need to design and implement a solution. What should you do?",
    "options": {
      "A": "Create a Pub/Sub topic, and enable a Cloud Storage trigger for the Pub/Sub topic. Create an application that sends all medical images to the Pub/Sub topic.",
      "B": "Deploy a Dataflow job from the batch template, ×’â‚¬Datastore to Cloud Storage.×’â‚¬ Schedule the batch job on the desired interval.",
      "C": "Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.",
      "D": "In the Cloud Console, go to Cloud Storage. Upload the relevant images to the appropriate bucket."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: TAvenger Highly Voted 4Â years, 4Â months ago\nFrom the question the key point is \"upload ANY NEW medical images to Cloud Storage\". So we are not interested in old images. That's why we need some trigger that will upload images. I think option \"A\" with PubSub is the best dunhill 4Â years, 4Â months ago\nI am not sure but the question also mentions that \"wants to use Cloud Storage for archival storage of these images\". It can create an application that sends all medical images to storage and no need via PubSub? pca2b 4Â years, 3Â months ago\nPub/Sub will be good for all future files in in-prem data-storage.\nwe want to sync all + new, so a local on-prem server running a cron job (not GCE CronJob) to run gsutil to transfer files to Cloud Storage would work.\nI vote for C yvinisiupacuando 4Â years, 2Â months ago\nSorry you are wrong, the question clearly indicates \"The hospital wants an automated process to upload ANY NEW medical images to Cloud Storage.\" It does not mention the need to upload the original stock of images, only the new ones. Then I think the right answer must be A, as you said \"Pub/sub will be good for all future files in prem data-storage\" which is exactly what the questions is pointing to. gcpengineer 3Â years, 11Â months ago\nans is C Priyanka109 2Â years, 9Â months ago\nIn option C we are using a cron job, not dragging and dropping the images."
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nC. Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job. cserra 2Â years, 11Â months ago\nWhere does it say that the on-premises images are already digitized? and even if they are, where does it say that we also keep the old images?\nI think the correct answer is \"A\" theBestStudent 2Â years, 10Â months ago\nTell yo yourself how the images would end up in the pubsub first of all. Also usually the process is in the other way around for pubsub notifications: Once an object lands in GCS the pubsub is notified of it.\nOption A makes totally nonsense. Check the flow again.\nFrom the options the only one that \"makes more sense\" is Option C"
      },
      {
        "index": 3,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: C\nThe best option is:\n\"Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.\"\nThis solution is straightforward and efficient for automating the upload of new medical images. The gsutil tool is designed for interacting with Cloud Storage and can synchronize files effectively. Scheduling the script as a cron job ensures that any new images are automatically uploaded at regular intervals without manual intervention. Using Pub/Sub is more complex and better suited for event-driven architectures rather than bulk data transfer from on-premises storage."
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: C\nA. Pub/Sub Topic with Cloud Storage Trigger: While Cloud Pub/Sub is great for event-driven architectures, it's not directly applicable for file synchronization scenarios. It would also require substantial modification to the existing infrastructure to send images to Pub/Sub.\nB. Dataflow Job: Dataflow is a powerful service for stream and batch data processing, but using it solely for file synchronization is overkill. It also requires more setup and maintenance compared to a simple gsutil script.\nD. Manual Upload in Cloud Console: This is not feasible for an automated process, as it requires manual intervention and isnâ€™t practical for a large number of files."
      },
      {
        "index": 5,
        "text": "Anonymous: jkim1708 1Â year, 10Â months ago\nSelected Answer: A\nI am also for A. Any new data should be send to Cloud Storage. Yes you need to create an application. To send data to pubsub. But for possible migration to cloud you can use the existing setup"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC is the right answer as they want the automated process to upload any new medical image"
      },
      {
        "index": 7,
        "text": "Anonymous: respawn 1Â year, 10Â months ago\nSelected Answer: C\nC is correct.\nA will not work because pub/sub is meant for service to service communication only:\nhttps://cloud.google.com/pubsub/docs/overview#compare_service-to-service_and_service-to-client_communication\nYes C option will sync any new images from onprem to cloud."
      },
      {
        "index": 8,
        "text": "Anonymous: shreykul 1Â year, 12Â months ago\nSelected Answer: A\nNew images can use Pub/Sub"
      },
      {
        "index": 9,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: C\nC is the correct answer.\nKeyword, they require cloud storage for archival and the want to automate the process to upload new medical image to cloud storage, hence we go for gsutil to copy on-prem images to cloud storage and automate the process via cron job. whereas Pub/Sub listens to the changes in the Cloud Storage bucket and triggers the pub/sub topic, which is not required. Naree 2Â years ago\nI agree. The requirement is for both history images and future images. So I go with Option \"C\"."
      },
      {
        "index": 10,
        "text": "Anonymous: zolthar_z 2Â years, 11Â months ago\nSelected Answer: C\nThe Hospital wants Cloud storage for archival of old images and also sync the new images, for this logic the answer is C"
      }
    ]
  },
  {
    "id": 79,
    "source": "examtopics",
    "question": "Your auditor wants to view your organization's use of data in Google Cloud. The auditor is most interested in auditing who accessed data in Cloud Storage buckets. You need to help the auditor access the data they need. What should you do?",
    "options": {
      "A": "Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.",
      "B": "Assign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs.",
      "C": "Assign the appropriate permissions, and then use Cloud Monitoring to review metrics.",
      "D": "Use the export logs API to provide the Admin Activity Audit Logs in the format they want."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: iri_gcp Highly Voted 4Â years, 10Â months ago\nIt should be A.\nData access log are not enabled by default due to the fact that it incurs costs.\nSo you need to enable it first.\nAnd then you can filter it in the log viewer"
      },
      {
        "index": 2,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 10Â months ago\nA. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage."
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nOnce again wrong answer. It should be option A"
      },
      {
        "index": 4,
        "text": "Anonymous: pzacariasf7 1Â year, 10Â months ago\nSelected Answer: A\nA. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage."
      },
      {
        "index": 5,
        "text": "Anonymous: NoCrapEva 2Â years, 4Â months ago\nIF Data Access Logs had ALREADY been enabled, then option B would be a good answer\nReason - (1) best practice for cloud auditing - enable Admin Activity audit logs, then set IAM permissions\n(ref: https://cloud.google.com/logging/docs/audit/best-practices)\nand (2) Create a Data Studio (now renamed to Looker) report on Admin Activity Audit Logs\n(ref: https://cloud.google.com/looker/docs/looker-core-audit-logging)\nBut you cannot assume from the question that Data Access Logs are enabled (NB: they are NOT by default)"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the right answer as first we need to turn on the data access logs"
      },
      {
        "index": 7,
        "text": "Anonymous: anolive 3Â years, 2Â months ago\nI have doubts about the answer A, the auditor wants to see the audit logs, and in this answer it is not explicit if he will be allowed to see it."
      },
      {
        "index": 8,
        "text": "Anonymous: Charumathi 3Â years, 3Â months ago\nSelected Answer: A\nA is the correct answer,\nSince the auditor wants to know who accessed the cloud storage data, we need data acces logs for cloud storage.\nTypes of audit logs\nCloud Audit Logs provides the following audit logs for each Cloud project, folder, and organization:\nAdmin Activity audit logs\nData Access audit logs\nSystem Event audit logs\nPolicy Denied audit logs\n***Data Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data.\nhttps://cloud.google.com/logging/docs/audit#types"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years, 6Â months ago\nA is right"
      },
      {
        "index": 10,
        "text": "Anonymous: Jman007 3Â years, 7Â months ago\nSelected Answer: A\nquestion says auditor is most interested in who accessED data in Cloud Storage. im not sure how auditoring is done for those who answered A but this means they want the logs for past users who accessed the data from a sepecified time. Turning on the feature now is kind of too late. poorly written question and answers. No point in an auditor coming in and giving the company all the exact questions they are going to ask and come back and ask them in a few months time. A seems like the better choices though"
      }
    ]
  },
  {
    "id": 80,
    "source": "examtopics",
    "question": "You received a JSON file that contained a private key of a Service Account in order to get access to several resources in a Google Cloud project. You downloaded and installed the Cloud SDK and want to use this private key for authentication and authorization when performing gcloud commands. What should you do?",
    "options": {
      "A": "Use the command gcloud auth login and point it to the private key.",
      "B": "Use the command gcloud auth activate-service-account and point it to the private key.",
      "C": "Place the private key file in the installation directory of the Cloud SDK and rename it to ×’â‚¬credentials.json×’â‚¬.",
      "D": "Place the private key file in your home directory and rename it to ×’â‚¬GOOGLE_APPLICATION_CREDENTIALS×’â‚¬."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 10Â months ago\nB. Use the command gcloud auth activate-service-account and point it to the private key.\nAuthorizing with a service account\ngcloud auth activate-service-account authorizes access using a service account. As with gcloud init and gcloud auth login, this command saves the service account credentials to the local system on successful completion and sets the specified account as the active account in your Cloud SDK configuration.\nhttps://cloud.google.com/sdk/docs/authorizing#authorizing_with_a_service_account"
      },
      {
        "index": 2,
        "text": "Anonymous: TAvenger Highly Voted 4Â years, 10Â months ago\nB.\ngcloud auth activate-service-account --help\nNAME)\ngcloud auth activate-service-account - authorize access to Google Cloud\nPlatform with a service account\nSYNOPSIS\ngcloud auth activate-service-account [ACCOUNT] --key-file=KEY_FILE\n[--password-file=PASSWORD_FILE | --prompt-for-password]\n[GCLOUD_WIDE_FLAG ...]\nDESCRIPTION\nTo allow gcloud (and other tools in Cloud SDK) to use service account\ncredentials to make requests, use this command to import these credentials\nfrom a file that contains a private authorization key, and activate them\nfor use in gcloud. gcloud auth activate-service-account serves the same\nfunction as gcloud auth login but uses a service account rather than Google\nuser credentials. eBooKz 2Â years, 11Â months ago\nSee below information suggesting that service account can be used to authorize with the command \"gcloud auth login\". Not sure if this is a recent update:\n\"The gcloud auth login command authorizes access by using workload identity federation, which provides access to external workloads, or by using a service account key.\"\n\"To activate your service account, run gcloud auth login with the --cred-file flag:\ngcloud auth login --cred-file=CONFIGURATION_OR_KEY_FILE\nReplace CONFIGURATION_OR_KEY_FILE with the path to one of the following:\nA credential configuration file for workload identity federation\nA service account key file\"\nhttps://cloud.google.com/sdk/docs/authorizing#authorize_with_a_service_account itsimranmalik 2Â years, 4Â months ago\nAs per google - gcloud auth activate-service-account serves the same function as gcloud auth login but uses a service account rather than Google user credentials.\nRef: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "index": 3,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: B\nTo authenticate with a service account using a private key file, you should run:\ngcloud auth activate-service-account --key-file=PATH_TO_KEY_FILE.json\nThis configures the Cloud SDK to use the service account for authentication and authorization."
      },
      {
        "index": 4,
        "text": "Anonymous: yomi95 1Â year, 2Â months ago\nSelected Answer: A\nA works\nhttps://cloud.google.com/sdk/docs/authorizing#auth-login\nCheck this sub menu: \"Authorize a service account using a service account key\" yomi95 1Â year, 2Â months ago\nCorrection, B also works,\nBut for the question at hand, B seems to be more relatable. Question seems to be confusing."
      },
      {
        "index": 5,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nAuthorize with a service account\nThe gcloud auth login command can authorize access with a service account by using a credential file stored on your local file system. This credential can be a user credential with permission to impersonate the service account, a credential configuration file for workload identity federation, or a service account key.\nhttps://cloud.google.com/sdk/docs/authorizing#auth-login denno22 1Â year, 3Â months ago\nNow, I see that while A works, B is a better answer."
      },
      {
        "index": 6,
        "text": "Anonymous: blackBeard33 1Â year, 11Â months ago\nSelected Answer: B\nThe Answer is A. The command to use service account for authentication is precisely gcloud auth activate-service-account where you can point out to the key file using the flag --key-file.\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: B\nâ€¢ The command syntax is gcloud auth activate-service-account --key-file=PATH_TO_KEY_FILE, where PATH_TO_KEY_FILE is the path to the JSON file containing the service account's private key."
      },
      {
        "index": 8,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: B\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "index": 9,
        "text": "Anonymous: N_A 2Â years, 8Â months ago\nD. This method is for application default credentials. See: https://cloud.google.com/docs/authentication/application-default-credentials\nA. This method is to obtain credentials for a user account.\nC. This does nothing. Useless.\nB. Is the correct answer. See: https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"
      },
      {
        "index": 10,
        "text": "Anonymous: Abhi00754 2Â years, 9Â months ago\nSelected Answer: B\nhttps://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account\nB"
      }
    ]
  },
  {
    "id": 81,
    "source": "examtopics",
    "question": "You are working with a Cloud SQL MySQL database at your company. You need to retain a month-end copy of the database for three years for audit purposes.\nWhat should you do?",
    "options": {
      "A": "Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.",
      "B": "Save the automatic first-of-the-month backup for three years. Store the backup file in an Archive class Cloud Storage bucket.",
      "C": "Set up an on-demand backup for the first of the month. Write the backup to an Archive class Cloud Storage bucket.",
      "D": "Convert the automatic first-of-the-month backup to an export file. Write the export file to a Coldline class Cloud Storage bucket."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: TAvenger Highly Voted 4Â years, 4Â months ago\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nnot B: Automatic backups are made EVERY SINGLE DAY. You can set only the number of backups up to 365. Also you cannot choose your Archival storage as destination\nnot C: You cannot setup \"on-demand\" backup. User would have to make backups manually every month. Also you cannot choose your Archival storage as destination\nnot D: You cannot conver backup to export file. Also Coldline class is less cost-effective than Archival class.\nThe only option left is \"A\"\nYou can set up your job with any date/time schedule. You can export file to any storage with any storage class. djgodzilla 4Â years, 1Â month ago\nfrom the same link :\nCan I export a backup?\nNo, you can't export a backup. You can only export instance data. See Exporting data from Cloud SQL to a dump in Cloud storage."
      },
      {
        "index": 2,
        "text": "Anonymous: JieHeng Highly Voted 4Â years ago\nFirst need to understand backup vs export, two different concepts. - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups\nA â€“ yes, you can export data from Cloud SQL to Cloud Storage- https://cloud.google.com/sql/docs/mysql/import-export/exporting#cloud-sql\nNot B, C, D â€“ be it automatic or on-demand backup, according to the doc â€œNo, you can't export a backup. You can only export instance data.â€"
      },
      {
        "index": 3,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: A\nYes, the solution works and aligns with best practices:\n\"Set up an export job for the first of the month. Write the export file to an Archive class Cloud Storage bucket.\"\nThe Archive class is cost-effective for long-term storage, and exporting the database ensures a consistent snapshot for audit purposes. Ensure the export job is automated and retains the required files for three years by configuring appropriate lifecycle policies in Cloud Storage."
      },
      {
        "index": 4,
        "text": "Anonymous: omunoz 1Â year, 2Â months ago\nShould be A:\nBackups are managed by Cloud SQL according to retention policies, and are stored separately from the Cloud SQL instance. Cloud SQL backups differ from an export uploaded to Cloud Storage, where you manage the lifecycle. Backups encompass the entire database. Exports can select specific contents.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#:~:text=for%20more%20information.-,Backups%20versus%20exports,-Backups%20are%20managed"
      },
      {
        "index": 5,
        "text": "Anonymous: santhush 1Â year, 10Â months ago\nhttps://www.exam-answer.com/retain-month-end-copy-cloud-sql-mysql-database-three-years B is the correct answer.. I am not sure why people are posting wrong answers here. Abbru00 1Â year, 8Â months ago\ncause you're the one wrong:\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer, as in a you can export as per your requirement and then moved it to archive class , but in b,c,d you can't do that"
      },
      {
        "index": 7,
        "text": "Anonymous: akkinepallyn 2Â years, 2Â months ago\nOption B is the best choice because it allows you to leverage the automatic first-of-the-month backup feature that is provided by Cloud SQL. Cloud SQL provides automated backups that can be configured to run at specific times, including the first of the month. By retaining the first-of-the-month backup for three years, you can be sure that you have a complete copy of the database for that month."
      },
      {
        "index": 8,
        "text": "Anonymous: Vismaya 2Â years, 4Â months ago\nAnswer A"
      },
      {
        "index": 9,
        "text": "Anonymous: researched_answer_boi 2Â years, 6Â months ago\nAnswer A is the correct one according to \"https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler\" and \"https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#backups_versus_exports\"."
      },
      {
        "index": 10,
        "text": "Anonymous: Kopy 2Â years, 8Â months ago\nSelected Answer: A\nSo Answer is A.\nYou can't export back up. Very clear.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/backups#can_i_export_a_backup"
      }
    ]
  },
  {
    "id": 82,
    "source": "examtopics",
    "question": "You are monitoring an application and receive user feedback that a specific error is spiking. You notice that the error is caused by a Service Account having insufficient permissions. You are able to solve the problem but want to be notified if the problem recurs. What should you do?",
    "options": {
      "A": "In the Log Viewer, filter the logs on severity 'Error' and the name of the Service Account.",
      "B": "Create a sink to BigQuery to export all the logs. Create a Data Studio dashboard on the exported logs.",
      "C": "Create a custom log-based metric for the specific error to be used in an Alerting Policy.",
      "D": "Grant Project Owner access to the Service Account."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GCP_Student1 Highly Voted 4Â years, 4Â months ago\nC. Create a custom log-based metrics for the specific error to be used in an Alerting Policy."
      },
      {
        "index": 2,
        "text": "Anonymous: greatsam321 Highly Voted 4Â years, 4Â months ago\nC seems to be the right answer."
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months ago\nSelected Answer: C\nThere are two types of Alerting Policy: https://cloud.google.com/monitoring/alerts\n- Metrics-based Alerting Policy [1] ==> Cloud Monitoring\n- Logs-based Alerting Policy [2] ==> Cloud Logging\n[1] https://cloud.google.com/monitoring/alerts/using-alerting-ui\n[2] https://cloud.google.com/logging/docs/alerting/log-based-alerts"
      },
      {
        "index": 4,
        "text": "Anonymous: pumajd 1Â year, 4Â months ago\nSelected Answer: C\nhttps://cloud.google.com/logging/docs/logs-based-metrics"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nUser wants to check the if problem recurs, that can be only possible by Alert, C is the correct option"
      },
      {
        "index": 6,
        "text": "Anonymous: _F4LLEN_ 2Â years, 3Â months ago\nC. The keyword here is \"want to be notified\" that means an alert."
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nSelected Answer: C\nC is the correct answer,\nSince the problem is resolved, We need to monitor if the error recurs, hence we create a custom log based metrics to monitor only the particular service account."
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 2Â years, 10Â months ago\nC as Keyword \"want to be notified if the problem recurs\""
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 3Â years ago\nC right"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years, 1Â month ago\nC is correct."
      }
    ]
  },
  {
    "id": 83,
    "source": "examtopics",
    "question": "You are developing a financial trading application that will be used globally. Data is stored and queried using a relational structure, and clients from all over the world should get the exact identical state of the data. The application will be deployed in multiple regions to provide the lowest latency to end users. You need to select a storage option for the application data while minimizing latency. What should you do?",
    "options": {
      "A": "Use Cloud Bigtable for data storage.",
      "B": "Use Cloud SQL for data storage.",
      "C": "Use Cloud Spanner for data storage.",
      "D": "Use Firestore for data storage."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JieHeng Highly Voted 3Â years, 6Â months ago\nC, Cloud Spanner, keywords are globally, relational structure and lastly \"clients from all over the world should get the exact identical state of the data\" which implies strong consistency is needed."
      },
      {
        "index": 2,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: C\nThe best option is:\nC. Use Cloud Spanner for data storage.\nReason:\nCloud Spanner is the only Google Cloud database that provides:\nGlobal consistency: Ensures all users worldwide see the exact same state of the data.\nRelational structure: Fully supports SQL queries and relational database schema.\nLow latency: Replicates data across multiple regions to minimize read/write latency for global users.\nScalability: Designed to handle high-scale applications like financial trading.\nOther options don't meet the requirements:\nA. Cloud Bigtable: No relational structure; optimized for wide-column use cases, not relational data.\nB. Cloud SQL: Limited to regional deployments, not suitable for globally distributed applications.\nD. Firestore: Designed for document-based structures, not ideal for relational database use."
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: C\nquestion demands, exact state of data and minimum latency to users , for this cloud spanner is the only option"
      },
      {
        "index": 4,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: C\nfinancial trading application\nrelational structure\nmultiple regions"
      },
      {
        "index": 5,
        "text": "Anonymous: ashtonez 1Â year, 10Â months ago\nSelected Answer: C\nC, always you need to select BBDD check for data analysis bigquery, something very big or fast bigtable, something with HA cloud sql, and something globally available cloud spanner, the key here is globaly available"
      },
      {
        "index": 6,
        "text": "Anonymous: Charumathi 2Â years, 3Â months ago\nSelected Answer: C\nC is the correct answer,\nKeywords, Financial data (large data) used globally, data stored and queried using relational structure (SQL), clients should get exact identical copies(Strong Consistency), Multiple region, low latency to end user, select storage option to minimize latency. Charumathi 2Â years, 3Â months ago\nSpanner powers business-critical applications in retail, financial services, gaming, media and entertainment, technology, healthcare and more.\nUse cases for Cloud Spanner\nhttps://www.youtube.com/watch?v=1b4flZwAQfM&t=1s"
      },
      {
        "index": 7,
        "text": "Anonymous: ale_brd_111 2Â years, 3Â months ago\nSelected Answer: C\nit's C 100%\nGuys come on, it's a pretty straight forward scenario.\nif you have the keywords \"relational DB\" and the word \"Globally\" in a sentence always go for Cloud Spanner."
      },
      {
        "index": 8,
        "text": "Anonymous: learn_GCP 2Â years, 3Â months ago\nSelected Answer: C\nC. is the answer"
      },
      {
        "index": 9,
        "text": "Anonymous: sri333 2Â years, 3Â months ago\nWhy not A. Big table as per keywords relational, global and low latency tonyg_2023 2Â years ago\nBigTable is not a relational database. Everything else is true for it but it a noSQL non Relational Database."
      },
      {
        "index": 10,
        "text": "Anonymous: zellck 2Â years, 4Â months ago\nSelected Answer: C\nC is the answer.\nCloud Spanner is a global relational database."
      }
    ]
  },
  {
    "id": 84,
    "source": "examtopics",
    "question": "You are about to deploy a new Enterprise Resource Planning (ERP) system on Google Cloud. The application holds the full database in-memory for fast data access, and you need to configure the most appropriate resources on Google Cloud for this application. What should you do?",
    "options": {
      "A": "Provision preemptible Compute Engine instances.",
      "B": "Provision Compute Engine instances with GPUs attached.",
      "C": "Provision Compute Engine instances with local SSDs attached.",
      "D": "Provision Compute Engine instances with M1 machine type."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Rightsaidfred Highly Voted 4Â years, 8Â months ago\nYes D, M1 Machine types for ERP i.e. SAP-HANA:\nhttps://cloud.google.com/compute/docs/machine-types"
      },
      {
        "index": 2,
        "text": "Anonymous: epuser4791 Most Recent 2Â months ago\nSelected Answer: C\nI would focus on fast disk SSD. The M1 type is optimized for memory, but still small for ERP. Can someone confirm."
      },
      {
        "index": 3,
        "text": "Anonymous: kamee15 1Â year ago\nSelected Answer: D\nThe best option is:\nD. Provision Compute Engine instances with M1 machine type.\nReason:\nThe M1 machine type is designed for applications that require large amounts of memory, making it ideal for in-memory databases like an ERP system. It provides high memory-to-CPU ratios, ensuring the database can be fully loaded into memory for fast data access.\nOther options don't align with the requirements:\nA. Preemptible Compute Engine instances: Not suitable for critical applications like ERP systems as they can be terminated at any time.\nB. Compute Engine instances with GPUs attached: GPUs are optimized for parallel processing tasks like machine learning, not for memory-intensive in-memory databases.\nC. Compute Engine instances with local SSDs attached: While local SSDs provide fast storage, the application's requirement is for in-memory data, not fast storage."
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: D\nD"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD is the right answer as M1 one are best for the databases i.e SAP hana"
      },
      {
        "index": 6,
        "text": "Anonymous: Bobbybash 2Â years, 11Â months ago\nC. Provision Compute Engine instances with local SSDs attached.\nThe best option for an ERP system that holds the full database in-memory for fast data access is to provision Compute Engine instances with local SSDs attached. Local SSDs offer high input/output operations per second (IOPS) and low latency, which can significantly improve the performance of in-memory databases. Preemptible Compute Engine instances are designed for short-lived and fault-tolerant workloads and are not recommended for a critical system like an ERP. GPUs are typically used for specialized compute-intensive workloads like machine learning and deep learning. M1 machine type is a general-purpose machine type and may not provide enough performance for an in-memory database."
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 3Â years, 3Â months ago\nSelected Answer: D\nD is the correct answer,\nM1 machine series\nMedium in-memory databases such as SAP HANA\nTasks that require intensive use of memory with higher memory-to-vCPU ratios than the general-purpose high-memory machine types.\nIn-memory databases and in-memory analytics, business warehousing (BW) workloads, genomics analysis, SQL analysis services.\nMicrosoft SQL Server and similar databases."
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 3Â years, 4Â months ago\nD, keyword \"Full database in-memory \""
      },
      {
        "index": 9,
        "text": "Anonymous: ryumada 3Â years, 5Â months ago\nSelected Answer: D\nVote for D as the right answer. M1 machine type is the one of two Memory-Optimized machine types in GCP.\nhttps://cloud.google.com/compute/docs/machine-types ryumada 3Â years, 5Â months ago\nRead this also to see the difference of the two.\nhttps://cloud.google.com/compute/docs/memory-optimized-machines"
      },
      {
        "index": 10,
        "text": "Anonymous: abirroy 3Â years, 5Â months ago\nSelected Answer: D\nD: M1 Machine types for ERP i.e. SAP-HANA\nMedium-large in-memory databases such as SAP HANA\nIn-memory databases and in-memory analytics\nMicrosoft SQL Server and similar databases"
      }
    ]
  },
  {
    "id": 85,
    "source": "examtopics",
    "question": "You have developed an application that consists of multiple microservices, with each microservice packaged in its own Docker container image. You want to deploy the entire application on Google Kubernetes Engine so that each microservice can be scaled individually. What should you do?",
    "options": {
      "A": "Create and deploy a Custom Resource Definition per microservice.",
      "B": "Create and deploy a Docker Compose File.",
      "C": "Create and deploy a Job per microservice.",
      "D": "Create and deploy a Deployment per microservice."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: obeythefist Highly Voted 2Â years, 10Â months ago\nI was a little unsure about this question, here's how I understand why D is the best answer\nA. Custom Resource Definition... we have docker containers already, which is an established kind of resource for Kubernetes. We don't need to create a whole new type of resource, so this is wrong.\nB. Docker Compose is a wholly different tool from Kubernetes.\nC. A Kubernetes job describes a specific \"task\" which involves a bunch of pods and things. It makes no sense to have one job per microservice, a \"Job\" would be a bunch of different microservices executing together.\nD. is the leftover, correct answer. You can add scaling to each Deployment, an important aspect of the question. akshaychavan7 2Â years, 8Â months ago\nThanks for your insights! Makes sense."
      },
      {
        "index": 2,
        "text": "Anonymous: Kollipara Highly Voted 3Â years, 8Â months ago\nD is the correct answer"
      },
      {
        "index": 3,
        "text": "Anonymous: taylz876 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nTo deploy a microservices-based application on Google Kubernetes Engine (GKE), it's common to create and deploy a Deployment per microservice.\nD. Create and deploy a Deployment per microservice.\nHere's why:\nDeployment: In Kubernetes, a Deployment is a resource that allows you to define, create, and manage the desired number of replicas of your application. Each microservice can be independently managed and scaled using its own Deployment.\nThis approach provides the flexibility to scale individual microservices as needed and manage their lifecycle effectively. Each microservice will have its own set of pods that can be scaled up or down independently, making it suitable for a microservices architecture."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: D\nD is the corrrect answer"
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: D\nD is the correct answer."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: D\nD seems more correct , as in A , we already have the own docker container image , so no need to create ,\nb is completely diffenret tool,\nc is also of no use"
      },
      {
        "index": 7,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nD, keyword \"each microservice can be scaled individually\"!"
      },
      {
        "index": 8,
        "text": "Anonymous: abirroy 2Â years, 5Â months ago\nSelected Answer: D\nD is the correct answer"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nD is the best answer among other choices."
      },
      {
        "index": 10,
        "text": "Anonymous: Raz0r 2Â years, 11Â months ago\nD is right!\nIt's one of Googles main ideas to distribute a complex system into microservices. They do it as well and encourage customers to do the same."
      }
    ]
  },
  {
    "id": 86,
    "source": "examtopics",
    "question": "You will have several applications running on different Compute Engine instances in the same project. You want to specify at a more granular level the service account each instance uses when calling Google Cloud APIs. What should you do?",
    "options": {
      "A": "When creating the instances, specify a Service Account for each instance.",
      "B": "When creating the instances, assign the name of each Service Account as instance metadata.",
      "C": "After starting the instances, use gcloud compute instances update to specify a Service Account for each instance.",
      "D": "After starting the instances, use gcloud compute instances update to assign the name of the relevant Service Account as instance metadata."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GoCloud Highly Voted 3Â years, 8Â months ago\nA ."
      },
      {
        "index": 2,
        "text": "Anonymous: JieHeng Highly Voted 3Â years, 6Â months ago\nA, when you create an instance using the gcloud command-line tool or the Google Cloud Console, you can specify which service account the instance uses when calling Google Cloud APIs - https://cloud.google.com/compute/docs/access/service-accounts#associating_a_service_account_to_an_instance"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nOption A is correct , when you create the instance , that time itself you can specify the service account of each instance"
      },
      {
        "index": 4,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: A\nA. When creating the instances, specify a Service Account for each instance.\nTo specify a more granular level of service account for each Compute Engine instance, you should specify a Service Account for each instance when you create it. This can be done through the Compute Engine API or the Cloud Console. By doing so, the specified Service Account will be used when calling Google Cloud APIs from that instance.\nOption B, assigning the name of each Service Account as instance metadata, is not the best solution as metadata can be accessed by anyone with access to the instance, which could potentially lead to security issues.\nOptions C and D, using gcloud compute instances update to specify a Service Account or assign the name of a Service Account as instance metadata after starting the instances, can also be done, but it is a less efficient approach as it requires additional steps and can lead to human error if not properly documented. VarunGo 1Â year, 10Â months ago\nused chatgpt"
      },
      {
        "index": 5,
        "text": "Anonymous: ryumada 2Â years, 5Â months ago\nSelected Answer: A\nVote for A, because there is no instance running yet. \"You will have several applications running...\""
      },
      {
        "index": 6,
        "text": "Anonymous: Roro_Brother 2Â years, 6Â months ago\nSelected Answer: A\nA, there is no instance running yet"
      },
      {
        "index": 7,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nA is good option for given scenario."
      },
      {
        "index": 8,
        "text": "Anonymous: somenick 2Â years, 9Â months ago\nSelected Answer: A\nYou can set/update the service account only when the instance is not running"
      },
      {
        "index": 9,
        "text": "Anonymous: Majkl93 2Â years, 11Â months ago\nSelected Answer: A\nA - the instances are not running yet"
      },
      {
        "index": 10,
        "text": "Anonymous: Raz0r 2Â years, 11Â months ago\nA: you can define which GCP service account is associated with a Compute Engine instance when creating one. It is still possible to change the service account later.\nLink to the GCP docs: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#using"
      }
    ]
  },
  {
    "id": 87,
    "source": "examtopics",
    "question": "You are creating an application that will run on Google Kubernetes Engine. You have identified MongoDB as the most suitable database system for your application and want to deploy a managed MongoDB environment that provides a support SLA. What should you do?",
    "options": {
      "A": "Create a Cloud Bigtable cluster, and use the HBase API.",
      "B": "Deploy MongoDB Atlas from the Google Cloud Marketplace.",
      "C": "Download a MongoDB installation package, and run it on Compute Engine instances.",
      "D": "Download a MongoDB installation package, and run it on a Managed Instance Group."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: arsh1916 Highly Voted 3Â years, 8Â months ago\nSimple it's B"
      },
      {
        "index": 2,
        "text": "Anonymous: lxgywil Highly Voted 3Â years, 8Â months ago\nMongoDB Atlas is actually managed and supported by third-party service providers.\nhttps://console.cloud.google.com/marketplace/details/gc-launcher-for-mongodb-atlas/mongodb-atlas lxgywil 3Â years, 8Â months ago\nI think that's it. The answer is B"
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: B\nI see why some of you are confused by the word \"Managed and SLA\". MongoDB is owned and run by a company called MongoDB Inc :-)\nWhen you deploy a service like MongoDB Atlas from the Google Cloud Marketplace, MongoDB, Inc. is responsible for the management of that service, including uptime guarantees and support. They provide an SLA that outlines their commitments regarding availability, performance, and support response times."
      },
      {
        "index": 4,
        "text": "Anonymous: rahulrauki 1Â year, 3Â months ago\nSelected Answer: B\nThe keyword is managed MongoDB environment, both C and D are managed by users, A is irrelevant, So B"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB seems more correct , just deploy it from the Market place."
      },
      {
        "index": 6,
        "text": "Anonymous: WendyLC 1Â year, 7Â months ago\nSelected Answer: B\nAnswer is B"
      },
      {
        "index": 7,
        "text": "Anonymous: PKookNN 2Â years, 2Â months ago\nSelected Answer: B\nthe best answer is B"
      },
      {
        "index": 8,
        "text": "Anonymous: nosense 2Â years, 2Â months ago\nSelected Answer: B\nb. fast and simple"
      },
      {
        "index": 9,
        "text": "Anonymous: 11kc03 2Â years, 2Â months ago\nSelected Answer: C\nAnswer is B"
      },
      {
        "index": 10,
        "text": "Anonymous: learn_GCP 2Â years, 3Â months ago\nSelected Answer: B\nB. is the answer"
      }
    ]
  },
  {
    "id": 88,
    "source": "examtopics",
    "question": "You are managing a project for the Business Intelligence (BI) department in your company. A data pipeline ingests data into BigQuery via streaming. You want the users in the BI department to be able to run the custom SQL queries against the latest data in BigQuery. What should you do?",
    "options": {
      "A": "Create a Data Studio dashboard that uses the related BigQuery tables as a source and give the BI team view access to the Data Studio dashboard.",
      "B": "Create a Service Account for the BI team and distribute a new private key to each member of the BI team.",
      "C": "Use Cloud Scheduler to schedule a batch Dataflow job to copy the data from BigQuery to the BI team's internal data warehouse.",
      "D": "Assign the IAM role of BigQuery User to a Google Group that contains the members of the BI team."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ApaMokus Highly Voted 3Â years, 8Â months ago\nD is correct\nroles/bigquery.user\nWhen applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.\nWhen applied to a project, this role also provides the ability to run jobs, including queries, within the project. A member with this role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on these new datasets."
      },
      {
        "index": 2,
        "text": "Anonymous: blan_ak Highly Voted 3Â years, 5Â months ago\nWhy on the earth would the answer be C? It has no relevance to the question. The answer is D, hands down"
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: D\nBad question.\nOption D is the best answer (of all the options), but it doesn't give the Custom role to run SQL queries against their data in BigQuery. If you are in doubt, check what the BigQuery User role allows."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: D\nD is the right answer, just assign them the role by IAM and they will be able to use BQ"
      },
      {
        "index": 5,
        "text": "Anonymous: ankyt9 2Â years, 1Â month ago\nSelected Answer: D\nD is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: anolive 2Â years, 2Â months ago\nSelected Answer: D\nmakes mor sense"
      },
      {
        "index": 7,
        "text": "Anonymous: sylva91 2Â years, 4Â months ago\nSelected Answer: D\nD is correct because google recommendations are always to privilege groups to individual accounts and this is what can make the users query the database unlike the Data Studio"
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: D\nD is right"
      },
      {
        "index": 9,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nD is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: patashish 2Â years, 6Â months ago\nD is the answer\nHint - to **run the custom SQL queries*** against the latest data in BigQuery"
      }
    ]
  },
  {
    "id": 89,
    "source": "examtopics",
    "question": "Your company is moving its entire workload to Compute Engine. Some servers should be accessible through the Internet, and other servers should only be accessible over the internal network. All servers need to be able to talk to each other over specific ports and protocols. The current on-premises network relies on a demilitarized zone (DMZ) for the public servers and a Local Area Network (LAN) for the private servers. You need to design the networking infrastructure on\nGoogle Cloud to match these requirements. What should you do?",
    "options": {
      "A": "1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ.",
      "B": "1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ.",
      "C": "1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ.",
      "D": "1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: perdigiorno Highly Voted 3Â years, 6Â months ago\nPassed the test today. About 80% of the questions are here. sumanthrao1 3Â years, 3Â months ago\nyou got same questions from this examtopics associatecloudexamuser 3Â years, 6Â months ago\nCongratulations!"
      },
      {
        "index": 2,
        "text": "Anonymous: yvinisiupacuando Highly Voted 3Â years, 8Â months ago\nA is the Right answer. You can discard B and C because they lack the need of creating Network Peering to communicate the DMZ VPC with the LAN VPC (LAN VPC is not exposed to public so they need to communicate via private addresses which cannot be achieved with 2 VPCs without Network Peering). Plus, you can discard B, as you don't need to enable the egress traffic, you always need to enable the ingress traffic as this is never enabled by default. Alela 3Â years, 8Â months ago\nA is wrong. You don't need to set up firewall rules between subnets of the same VPC. C is the answer gcpengineer 3Â years, 5Â months ago\nYou need fw rules demnok_lannik 2Â years, 11Â months ago\nof course you do Ashii 3Â years, 7Â months ago\nC is Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ. Without peering 2 VPC's how this this be done ? BenKenGo6 2Â years, 4Â months ago\nand where do you have the VPC peering to communicate both VPCs?"
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: A\nOption C is NOT valid as it overlooks the requirement for VPC peering or another connection method to enable communication between two separate VPCs."
      },
      {
        "index": 1,
        "text": "There is no default connection between different VPC"
      },
      {
        "index": 2,
        "text": "By default all incoming(ingress) traffic is denied. So, a firewall rule is needed even in the same VPC."
      },
      {
        "index": 4,
        "text": "Anonymous: taylz876 1Â year, 3Â months ago\nSelected Answer: A\nThe answer is A:\nHere's the explanation:\n-->Single VPC: Creating a single Virtual Private Cloud (VPC) is a common practice to manage your resources in Google Cloud.\nSubnet for DMZ and LAN: Creating separate subnets within the same VPC for the DMZ (public-facing) and LAN (private) resources is a recommended approach to segregate your resources.\n-->Firewall Rules: Setting up firewall rules allows you to control traffic between the DMZ and LAN subnets and enables you to define specific access policies. You also need to allow public traffic (ingress) into the DMZ to make the public-facing resources accessible from the internet."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct answer, as it meet the question requirment"
      },
      {
        "index": 6,
        "text": "Anonymous: diasporabro 2Â years, 3Â months ago\nSelected Answer: A\nA is the right choice"
      },
      {
        "index": 7,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: A\nA seems right"
      },
      {
        "index": 8,
        "text": "Anonymous: an0nym0us1 2Â years, 4Â months ago\nhi All what is the ans"
      }
    ]
  },
  {
    "id": 90,
    "source": "examtopics",
    "question": "You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?",
    "options": {
      "A": "Enable the Cloud Spanner API.",
      "B": "Configure your Cloud Spanner instance to be multi-regional.",
      "C": "Create a new VPC network with subnetworks in all desired regions.",
      "D": "Grant yourself the IAM role of Cloud Spanner Admin."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AzureDP900 Highly Voted 3Â years, 7Â months ago\nA is right\nhttps://cloud.google.com/spanner/docs/getting-started/set-up"
      },
      {
        "index": 2,
        "text": "Anonymous: pfabio Highly Voted 3Â years, 7Â months ago\nSelected Answer: A\nIf you click on Create instance, the message is show in bottom: Cloud Spanner API for your project has been enabled."
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nA<------- is the correct first step. why? Because 99% of cloud deployment is done programmatically using IaC, such as Terraform or Google's own IaC, and for that reason alone \"Enable the Cloud Spanner API\" is a must!!"
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nhttps://cloud.google.com/spanner/docs/getting-started/set-up"
      },
      {
        "index": 5,
        "text": "Anonymous: Akinzoa 2Â years ago\nB looks more like it.\nWhen creating a Cloud Spanner instance, you configure the instance details first before enabling the Cloud Spanner API, if its not enabled by default i.e. if this is the first time you are using Cloud Spanner in your project."
      },
      {
        "index": 6,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: A\nIts definitely A. Link: https://cloud.google.com/spanner/docs/quickstart-console"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nAnswer A is correct before you do anything first you needto enable the API of that particulr service"
      },
      {
        "index": 8,
        "text": "Anonymous: Capability 3Â years ago\nSelected Answer: A\nhttps://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance"
      },
      {
        "index": 9,
        "text": "Anonymous: Capability 3Â years ago\nA is right\nhttps://cloud.google.com/spanner/docs/quickstart-console?_ga=2.68426577.-1975890344.1661276010&_gac=1.161955406.1673625078.Cj0KCQiAn4SeBhCwARIsANeF9DJxfolckwcRZqaOS7Rem2pzXWGmlBaLlxK4hHSe3YZ4DtE5oHHKVMQaArPUEALw_wcB#:~:text=see%20Pricing.-,Before%20you%20begin,Enable%20the%20Cloud%20Spanner%20API,-Create%20an%20instance"
      },
      {
        "index": 10,
        "text": "Anonymous: raaad 3Â years, 1Â month ago\nSelected Answer: A\nTry the scenario yourself. Its A"
      }
    ]
  },
  {
    "id": 91,
    "source": "examtopics",
    "question": "You have created a new project in Google Cloud through the gcloud command line interface (CLI) and linked a billing account. You need to create a new Compute\nEngine instance using the CLI. You need to perform the prerequisite steps. What should you do?",
    "options": {
      "A": "Create a Cloud Monitoring Workspace.",
      "B": "Create a VPC network in the project.",
      "C": "Enable the compute googleapis.com API.",
      "D": "Grant yourself the IAM role of Computer Admin."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sylva91 Highly Voted 3Â years, 4Â months ago\nSelected Answer: C\nnothing can be done before activating the API"
      },
      {
        "index": 2,
        "text": "Anonymous: yehia2221 Most Recent 1Â year, 5Â months ago\nagree with C, there are few API enabled by default and the Compute engine API is not a part of them. do not be confused, personally, I thought this API should be part of the basic APIs enabled but it it not."
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: C\nThe compute.googleapis.com API must be enabled in your project before you can utilize Compute Engine features and issue commands to create instances."
      },
      {
        "index": 4,
        "text": "Anonymous: michalmrozik 2Â years, 12Â months ago\nWhy not B? Can you create Compute Engine instance without assigning it it VPC? lummy 2Â years, 9Â months ago\nI believe you can make use of the default vpc Mike_SG 2Â years, 8Â months ago\nWhen you create a new project on the GCP, a default VPC network is automatically created for you. Kyle1776 2Â years, 7Â months ago\nYeah, but who uses the default VPC and CIDR ranges? Technically you could, but its not best practice and RARELY would fit in with a companies existing infrastructure."
      },
      {
        "index": 5,
        "text": "Anonymous: roaming_panda 3Â years, 1Â month ago\napi > iam role .i vote for C !!"
      },
      {
        "index": 6,
        "text": "Anonymous: zellck 3Â years, 4Â months ago\nSelected Answer: C\nC is the obvious answer."
      },
      {
        "index": 7,
        "text": "Anonymous: lll_bbb 3Â years, 4Â months ago\nSelected Answer: C\napi first"
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 3Â years, 4Â months ago\nSelected Answer: C\nC the compute googleapis.com API"
      },
      {
        "index": 9,
        "text": "Anonymous: Nishanth222 3Â years, 4Â months ago\nMust be C"
      }
    ]
  },
  {
    "id": 92,
    "source": "examtopics",
    "question": "Your company has developed a new application that consists of multiple microservices. You want to deploy the application to Google Kubernetes Engine (GKE), and you want to ensure that the cluster can scale as more applications are deployed in the future. You want to avoid manual intervention when each new application is deployed. What should you do?",
    "options": {
      "A": "Deploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment.",
      "B": "Deploy the application on GKE, and add a VerticalPodAutoscaler to the deployment.",
      "C": "Create a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.",
      "D": "Create a separate node pool for each application, and deploy each application to its dedicated node pool."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: efar_cloud Highly Voted 2Â years, 7Â months ago\nAnswer is C\nThe key point is \"ensure that the CLUSTER can scale\"\nA- HorizontalPodAutoscaler - ensures to scale the number of pods\nwhile\nC- Create a GKE cluster with autoscaling enabled on the node pool. Set a minimum and maximum for the size of the node pool.\nensures to scale the number of nodes in the cluster.\nSo the answer is C."
      },
      {
        "index": 2,
        "text": "Anonymous: WendyLC Highly Voted 2Â years, 6Â months ago\nSelected Answer: C\nC is the right choice... See this for reference https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#fine-tune_gke_autoscaling\nA- HorizontalPodAutoscaler - it is best suited for stateless workers that can spin up quickly to react to usage spikes, and shut down gracefully to avoid workload instability."
      },
      {
        "index": 3,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: C\nMaybe ChatGPT never had the opportunity to get the Google Cloud $300 free tier ;-)\nHorizontalPodAutoscaler and VerticalPodAutoscaler are good for existing pods.\nCan you create a new node with HorizontalPodAutoscaler or VerticalPodAutoscaler?"
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: C\nc"
      },
      {
        "index": 5,
        "text": "Anonymous: RKS_2021 1Â year, 3Â months ago\nSelected Answer: A\nA is right ans"
      },
      {
        "index": 6,
        "text": "Anonymous: yehia2221 1Â year, 5Â months ago\nAnswer is C:\nthe HPA is used in for scaling a deployment (an application), but here, the question is asking to scale the cluster when new applications are being added which have different and independent deployments, we have scaling at cluster level, then at deployment level either horizontally or vertically."
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: A\nIn the context of deploying a new application and ensuring future scalability with minimal manual intervention, focusing on pod scalability is indeed fundamental. This is accurately addressed by option A (Deploy the application on GKE, and add a HorizontalPodAutoscaler to the deployment).\nHowever, it's also important to have node autoscaling enabled (as mentioned in option C) to ensure that the cluster can accommodate the scaling pods. Both pod and node scaling are important for a fully scalable solution, but the immediate focus when deploying a new application is typically on pod configuration and scaling. PiperMe 1Â year, 10Â months ago\nThis is incorrect. Horizontal Pod Autoscalers scale based on pod-level metrics such as CPU. While useful, HPAs don't directly address the need to add more nodes if the underlying infrastructure is at capacity. The answer is C which provides the most effective and streamlined way to achieve automatic cluster-level scaling in a GKE environment hosting multiple microservices."
      },
      {
        "index": 8,
        "text": "Anonymous: MrJkr 2Â years, 7Â months ago\nSelected Answer: A\nIts A,\nWhen you first deploy your workload to a Kubernetes cluster, you may not be sure about its resource requirements and how those requirements might change depending on usage patterns, external dependencies, or other factors. Horizontal Pod autoscaling helps to ensure that your workload functions consistently in different situations, and allows you to control costs by only paying for extra capacity when you need it."
      },
      {
        "index": 9,
        "text": "Anonymous: sabrinakloud 2Â years, 9Â months ago\nSelected Answer: A\ni think it is A \"you want to ensure that the cluster can scale as more applications are deployed in the future.\""
      },
      {
        "index": 10,
        "text": "Anonymous: sabrinakloud 2Â years, 9Â months ago\nSelected Answer: C\noption C sabrinakloud 2Â years, 9Â months ago\noption A*"
      }
    ]
  },
  {
    "id": 93,
    "source": "examtopics",
    "question": "You need to manage a third-party application that will run on a Compute Engine instance. Other Compute Engine instances are already running with default configuration. Application installation files are hosted on Cloud Storage. You need to access these files from the new instance without allowing other virtual machines (VMs) to access these files. What should you do?",
    "options": {
      "A": "Create the instance with the default Compute Engine service account. Grant the service account permissions on Cloud Storage.",
      "B": "Create the instance with the default Compute Engine service account. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance.",
      "C": "Create a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage.",
      "D": "Create a new service account and assign this service account to the new instance. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: VietmanOfficiel Highly Voted 2Â years, 4Â months ago\nSelected Answer: C\n\"without allowing other instances\" , the other instances are created with default compute engine service account. So you must create a new independant service account"
      },
      {
        "index": 2,
        "text": "Anonymous: scanner2 Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nC is correct."
      },
      {
        "index": 3,
        "text": "Anonymous: gcpreviewer 2Â years, 3Â months ago\nSelected Answer: C\nC is the clear choice. Want to create a new service account instead of using the default and grant it permissions in cloud storage. Straightforward C."
      },
      {
        "index": 4,
        "text": "Anonymous: manjtrade2 2Â years, 4Â months ago\nSelected Answer: C\nC is right"
      },
      {
        "index": 5,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: C\nC seems right to me"
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nC\nhttps://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts\nIf an application uses third-party or custom identities and needs to access a resource, such as a BigQuery dataset or a Cloud Storage bucket, it must perform a transition between principals. Because Google Cloud APIs don't recognize third-party or custom identities, the application can't propagate the end-user's identity to BigQuery or Cloud Storage. Instead, the application has to perform the access by using a different Google identity."
      },
      {
        "index": 7,
        "text": "Anonymous: KRIV_1 2Â years, 8Â months ago\nAlthough C is the correct answer notice that, as Google recommend, you first need to grant the service account the required permission before attach it to a resource."
      },
      {
        "index": 8,
        "text": "Anonymous: JelloMan 2Â years, 8Â months ago\nSelected Answer: C\nC all the way. Restricts access to other VMs since they wonâ€™t have the new service account you have associated with your new VM"
      },
      {
        "index": 9,
        "text": "Anonymous: amindbesideitself 2Â years, 8Â months ago\nSelected Answer: C\nC, other VMs will run as default service account."
      },
      {
        "index": 10,
        "text": "Anonymous: Akash7 2Â years, 8Â months ago\nC is correct as the other vms have default service accounts."
      }
    ]
  },
  {
    "id": 94,
    "source": "examtopics",
    "question": "You need to configure optimal data storage for files stored in Cloud Storage for minimal cost. The files are used in a mission-critical analytics pipeline that is used continually. The users are in Boston, MA (United States). What should you do?",
    "options": {
      "A": "Configure regional storage for the region closest to the users. Configure a Nearline storage class.",
      "B": "Configure regional storage for the region closest to the users. Configure a Standard storage class.",
      "C": "Configure dual-regional storage for the dual region closest to the users. Configure a Nearline storage class.",
      "D": "Configure dual-regional storage for the dual region closest to the users. Configure a Standard storage class."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: akshaychavan7 Highly Voted 3Â years, 8Â months ago\nSelected Answer: D\nMission critical is the keyword here which specifies that we need to have a multi-regional backup of the data to survive any regional failures.\nSo option D is the correct choice here. mav3r1ck 3Â years, 5Â months ago\nKeywords: minimal cost and mission-critical\nLooks like people are just looking to be on the cost side. You need to meet both.\nIn this case, it needs to be \"dual-region\". This is much cheaper than storage in \"multi-region\" which is obviously not in the choices. Aninina 3Â years, 2Â months ago\nDual region is expensive than multi-region. (Also mentioned in the documentation: https://cloud.google.com/storage/docs/locations)\nWhen we set objects to be multi-regional, we get to decide/shuffle the data around at will to meet our storage needs. When you take that control away from us, it reduces the flexibility of our systems, making it more expensive to operate. ryumada 3Â years, 5Â months ago\nAt the first point in this documentation says that dual-regional storage is used for business continuity and disaster recovery. Disaster can affect to a regional architecture. I think it's make sense to use dual-regional storage for this case. Also, dual-regional storage is cheaper than multi-regional.\nhttps://cloud.google.com/storage/docs/dual-regions#use-dual-region-storage"
      },
      {
        "index": 2,
        "text": "Anonymous: JelloMan Highly Voted 3Â years, 8Â months ago\nSelected Answer: B\nContinuous access to data means Standard since all of the other options are for infrequently accessed storage (Nearline, Coldline, Archive). Since no other regions are mentioned, single region is best in this case KRIV_1 3Â years, 8Â months ago\nAnd beacuse single region is \"costly-effective\"."
      },
      {
        "index": 3,
        "text": "Anonymous: peterwheat Most Recent 7Â months, 3Â weeks ago\nSelected Answer: B\nAccording to the conditions the tie breaker is the region distance from the users, that is also important if we are talking about \"optimal\" solution.\n- SLA: regional 99.9%, dual-region 99.99% -> D\n- cost: regional cheaper -> B\n- close to users: regional us-east4 (closest), dual us-central1 and us-east1 -> B"
      },
      {
        "index": 4,
        "text": "Anonymous: Hatem9 11Â months, 2Â weeks ago\nSelected Answer: B\ni go with B keywords:\nStandard storage covers performance\nRegional covers cost and location to users"
      },
      {
        "index": 5,
        "text": "Anonymous: kamee15 1Â year ago\nSelected Answer: B\nFurther explanation:\nWhy not the other options?\nâ€¢ A. Regional + Nearline storage class: Nearline storage is designed for infrequently accessed data and would incur retrieval costs and higher latency, which is not suitable for continually used pipelines.\nâ€¢ C. Dual-regional + Nearline storage class: Dual-regional storage is unnecessary if the use case does not require geo-redundancy, and Nearline storage is unsuitable for frequent access.\nâ€¢ D. Dual-regional + Standard storage class: While the Standard storage class is appropriate, dual-regional storage adds unnecessary cost for geo-redundancy, which is not required for a region-specific use case.\nConclusion:\nOption B ensures low latency, high performance, and cost-efficiency by storing frequently accessed data in a regional bucket close to the users."
      },
      {
        "index": 6,
        "text": "Anonymous: kamee15 1Â year ago\nSelected Answer: B\nB. Configure regional storage for the region closest to the users. Configure a Standard storage class.\nReason:\nâ€¢ The Standard storage class is optimized for frequently accessed data, making it ideal for mission-critical analytics pipelines that are used continually.\nâ€¢ Regional storage in a region close to the users (e.g., in or near Boston, MA) minimizes latency, providing faster access to the data."
      },
      {
        "index": 7,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nSelected Answer: B\nThis is another badly worded question, but I think, I will vote for B. Here is why:\nThe users are in Boston, MA (United States) = a single region\nMission critical = Cloud storage (famous for reliability )\nAt minimal cost that is used continually = standard storage class"
      },
      {
        "index": 8,
        "text": "Anonymous: Ciupaz 1Â year, 2Â months ago\nSelected Answer: D\nHere's why D is the optimal choice:\nDual-regional storage:\nProvides high availability across two regions\nIdeal for mission-critical workloads\nMinimizes latency for Boston users by selecting nearby regions\nProvides geographic redundancy\nStandard storage class:\nOptimal for frequently accessed data (\"used continually\")\nNo additional latency for access\nNo additional retrieval costs\nIdeal for continuously running analytics pipelines"
      },
      {
        "index": 9,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: D\nMission-critcal"
      },
      {
        "index": 10,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: D\nI will go for D"
      }
    ]
  },
  {
    "id": 95,
    "source": "examtopics",
    "question": "You are developing a new web application that will be deployed on Google Cloud Platform. As part of your release cycle, you want to test updates to your application on a small portion of real user traffic. The majority of the users should still be directed towards a stable version of your application. What should you do?",
    "options": {
      "A": "Deploy the application on App Engine. For each update, create a new version of the same service. Configure traffic splitting to send a small percentage of traffic to the new version.",
      "B": "Deploy the application on App Engine. For each update, create a new service. Configure traffic splitting to send a small percentage of traffic to the new service.",
      "C": "Deploy the application on Kubernetes Engine. For a new release, update the deployment to use the new version.",
      "D": "Deploy the application on Kubernetes Engine. For a new release, create a new deployment for the new version. Update the service to use the new deployment."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Charumathi Highly Voted 3Â years, 3Â months ago\nSelected Answer: A\nA is correct answer,\nKeyword, Version, traffic splitting, App Engine supports traffic splitting for versions before releasing."
      },
      {
        "index": 2,
        "text": "Anonymous: AzureDP900 Highly Voted 3Â years, 7Â months ago\nIt is no brainer questions, It is A."
      },
      {
        "index": 3,
        "text": "Anonymous: Ciupaz Most Recent 1Â year, 2Â months ago\nSelected Answer: A\nHere's why A is the best choice:\nVersion management:\nApp Engine natively manages multiple versions of the same application\nAllows you to keep different versions active at the same time\nEasy rollback in case of problems\nTraffic splitting:\nNative App Engine functionality\nGrained control of traffic percentage\nEasy to configure and modify\nZero downtime:\nDoes not interrupt service during testing\nFluid transitions between versions\nMaintains the stable version"
      },
      {
        "index": 4,
        "text": "Anonymous: don_v 2Â years, 1Â month ago\nA is correct.\nStill, D seems also a correct approach. One can create a canary deployment with GKE and just update a service version. kuracpalac 1Â year, 11Â months ago\nBut it's more expensive and Google wants you to think cheap as possible in general."
      },
      {
        "index": 5,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: A\nAnswer is A."
      },
      {
        "index": 6,
        "text": "Anonymous: gary_gary 2Â years, 8Â months ago\nSimilar questions seem to appear multiple times."
      },
      {
        "index": 7,
        "text": "Anonymous: urcloudpartner 3Â years ago\nsome of these questions, the default by examtopics is completely different why so, why cannot they fix it once a real answer is known."
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 3Â years, 4Â months ago\nSelected Answer: A\nA obvious choice"
      },
      {
        "index": 9,
        "text": "Anonymous: KapilDhamija 3Â years, 5Â months ago\nSelected Answer: A\nVote goes to A"
      },
      {
        "index": 10,
        "text": "Anonymous: Tirthankar17 3Â years, 7Â months ago\nA obviously. No need to create a new service."
      }
    ]
  },
  {
    "id": 96,
    "source": "examtopics",
    "question": "You need to add a group of new users to Cloud Identity. Some of the users already have existing Google accounts. You want to follow one of Google's recommended practices and avoid conflicting accounts. What should you do?",
    "options": {
      "A": "Invite the user to transfer their existing account.",
      "B": "Invite the user to use an email alias to resolve the conflict.",
      "C": "Tell the user that they must delete their existing account.",
      "D": "Tell the user to remove all personal email from the existing account."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ggupton1 Highly Voted 3Â years, 8Â months ago\nSelected Answer: A\nhttps://cloud.google.com/architecture/identity/assessing-existing-user-accounts\nIf you want to maintain the access rights and some of the data associated with the Gmail account, you can ask the owner to remove Gmail from the user account so that you can then migrate them to Cloud Identity or Google Workspace."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nIf you want to maintain the access rights and some of the data associated with the Gmail account, you can ask the owner to remove Gmail from the user account so that you can then migrate them to Cloud Identity or Google Workspace. denno22 1Â year, 3Â months ago\nNow, I think A is a better answer."
      },
      {
        "index": 3,
        "text": "Anonymous: Namik 1Â year, 5Â months ago\nSelected Answer: B\nExplanation:\nEmail alias: This approach allows users to maintain their existing Google account while using a different email address for their work-related activities.\nNo account transfer: Avoids the complexities and potential issues associated with transferring accounts.\nClear separation: Maintains a clear distinction between personal and work-related activities.\nWhy not other options:\nA. Account transfer: This is generally not recommended as it can lead to data loss or complications.\nC. and D. Deleting or modifying existing accounts: These options are not practical or desirable as they disrupt the user's existing workflow.\nBy suggesting an email alias, you provide a user-friendly and secure solution to the account conflict."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: A\nAnswer is A."
      },
      {
        "index": 5,
        "text": "Anonymous: snkhatri 3Â years, 4Â months ago\nSelected Answer: A\nA obvious choice"
      },
      {
        "index": 6,
        "text": "Anonymous: bobthebuilder55110 3Â years, 5Â months ago\nSelected Answer: A\nCorrect Answer: A bobthebuilder55110 3Â years, 5Â months ago\nHere is why ?\nQuestion states \"Some of the users already have existing Google accounts.\" Meaning they have personal account or any google account and what Option B is saying is to use aliases, as per google documentation this is only helpful when we want someone to receive emails in one inbox with 2 email names, meaning x@google.com and y@google.com goes to the same inbox BUT what you can't do is to have personal@gogle.com and company@google.com since the company wouldn't add you to their domain as that is not google recommended practice.\nhttps://support.google.com/a/answer/33327?hl=en#when_to_use"
      },
      {
        "index": 7,
        "text": "Anonymous: ryumada 3Â years, 5Â months ago\nSelected Answer: A\nVote for A as the right answer. The docs in this link:\nhttps://cloud.google.com/architecture/identity/migrating-consumer-accounts\nas provided by PAUGURU in his comment explains clearly about resolving account conflict. In the doc says nothing about to change email alias to resolve the conflict. So, following the documentation in that link means you are following the Googles Recommended Practices."
      },
      {
        "index": 8,
        "text": "Anonymous: zolthar_z 3Â years, 5Â months ago\nSelected Answer: A\nA is the answer, for security reasons google best practices recommend transfer the account"
      },
      {
        "index": 9,
        "text": "Anonymous: sai_learner 3Â years, 6Â months ago\nSelected Answer: B\nAnswer is B\nhttps://support.google.com/cloudidentity/answer/7062710 ryumada 3Â years, 5Â months ago\nI am not sure if the link you provide explains the reason of the reason of your choosen answer. As in the documentation stated the email alias after what happen if you rename the email. Also, the documentation doesn't explain about account conflict.\nBetter with the docs link provided by PAUGURU: https://cloud.google.com/architecture/identity/migrating-consumer-accounts\nIt's explains clearly about the conflicting email and best practices too. ryumada 3Â years, 5Â months ago\n*explains the reason of your choosen answer.\nsorry, messed up sentence bobthebuilder55110 3Â years, 5Â months ago\nIt should be A, I was confused with this as well but B is not relevant in this use case. Look at my Above comments."
      },
      {
        "index": 10,
        "text": "Anonymous: ramss 3Â years, 6Â months ago\nAs per my understanding, B is the correct answer."
      }
    ]
  },
  {
    "id": 97,
    "source": "examtopics",
    "question": "You need to manage a Cloud Spanner instance for best query performance. Your instance in production runs in a single Google Cloud region. You need to improve performance in the shortest amount of time. You want to follow Google best practices for service configuration. What should you do?",
    "options": {
      "A": "Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. If you exceed this threshold, add nodes to your instance.",
      "B": "Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.",
      "C": "Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. If you exceed this threshold, add nodes to your instance.",
      "D": "Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: PAUGURU Highly Voted 3Â years, 8Â months ago\nSelected Answer: C\nC looks correct, increase instances on single region if CPU above 65%\nhttps://cloud.google.com/spanner/docs/cpu-utilization#recommended-max"
      },
      {
        "index": 2,
        "text": "Anonymous: 85c887f Most Recent 9Â months, 3Â weeks ago\nSelected Answer: D\nFor \"best query performance\" looks like D will work the best for a long-terms."
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: C\nC"
      },
      {
        "index": 4,
        "text": "Anonymous: don_v 2Â years, 1Â month ago\nI believe it's D.\n\" Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.\"\nIs that ever possible to add any node to Cloud Spanner? Come on. don_v 2Â years, 1Â month ago\nTaking it back. The answer is C.\n\"Compute capacity defines amount of server and storage resources that are available to the databases in an instance. When you create an instance, you specify its compute capacity as a number of processing units or as a number of nodes, with 1000 processing units being equal to 1 node.\"\nhttps://cloud.google.com/spanner/docs/instances"
      },
      {
        "index": 5,
        "text": "Anonymous: Ahmed_Y 2Â years, 5Â months ago\nSelected Answer: C\nI was keep thinking of A until I get to the link that thanks for @rsuresh27 provided bellow. the 45% is for the multi region."
      },
      {
        "index": 6,
        "text": "Anonymous: sabrinakloud 2Â years, 9Â months ago\nSelected Answer: C\nMetric Maximum for single-region instances Maximum per region for multi-region instances\nHigh priority total 65% 45%\n24-hour smoothed aggregate 90% 90%"
      },
      {
        "index": 7,
        "text": "Anonymous: dobberzoon 2Â years, 9Â months ago\nSelected Answer: C\nC makes sense."
      },
      {
        "index": 8,
        "text": "Anonymous: Aninina 3Â years, 2Â months ago\nSelected Answer: C\nhttps://cloud.google.com/spanner/docs/cpu-utilization"
      },
      {
        "index": 9,
        "text": "Anonymous: snkhatri 3Â years, 4Â months ago\nSelected Answer: C\nC looks correct"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years, 7Â months ago\nshortest timeframe is key here , I am going with C as my answer."
      }
    ]
  },
  {
    "id": 98,
    "source": "examtopics",
    "question": "Your company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?",
    "options": {
      "A": "BigQuery",
      "B": "Cloud SQL",
      "C": "Cloud Spanner",
      "D": "Cloud Datastore"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: peugeotdude Highly Voted 3Â years, 8Â months ago\nRead the question :\nThe application is used exclusively by employees in a single physical location. theBestStudent 3Â years, 3Â months ago\nCorrect. That is the key thing. I have no idea why some people ended up thinking cloud spanner is better. Definitely is alternative A. theBestStudent 3Â years, 3Â months ago\nSorry I meant B, I had a typo."
      },
      {
        "index": 2,
        "text": "Anonymous: PAUGURU Highly Voted 3Â years, 8Â months ago\nSelected Answer: B\nB -> minimal code changes cheeseburger12388 3Â years, 8Â months ago\nCloud SQL for PostgreSQL is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform.\nhttps://cloud.google.com/sql/docs/postgres"
      },
      {
        "index": 3,
        "text": "Anonymous: 4eaa323 Most Recent 12Â months ago\nSelected Answer: B\nClue is PostgresSQL"
      },
      {
        "index": 4,
        "text": "Anonymous: kamee15 1Â year ago\nSelected Answer: B\nB. Cloud SQL\nReason:\nâ€¢ Cloud SQL supports PostgreSQL, allowing you to deploy your application with minimal code changes since the first version is already implemented in PostgreSQL.\nâ€¢ It provides ACID guarantees, strong consistency, and fast queries, which are crucial for transactional workloads like order management.\nâ€¢ Cloud SQL is a fully managed relational database service, making it ideal for use cases with structured data and multi-table transactional updates."
      },
      {
        "index": 5,
        "text": "Anonymous: JoseCloudEng1994 1Â year ago\nSelected Answer: B\nIts a matter of cost efficiency. While Spanner is also an option it is an absolute waste for this situation. You would be a prodigal to use it just for this.\nThe company I work for uses it only for the most important things and it costs them 1/4 million a month.\nSo, yeah, very very expensive option"
      },
      {
        "index": 6,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nB"
      },
      {
        "index": 7,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: B\nRead the question :\nThe application is used exclusively by employees in a single physical location."
      },
      {
        "index": 8,
        "text": "Anonymous: ezzar 2Â years, 2Â months ago\nSelected Answer: B\nCloud spanner does everyhting cloud SQL already does. + It offers globality which we don't need and horizontal scaling that is mentionned nowhere"
      },
      {
        "index": 9,
        "text": "Anonymous: Dino0411 2Â years, 5Â months ago\nSelected Answer: C\nC. Select Cloud Spanner.\nCloud Spanner offers strong consistency, fast queries, and importantly, ACID guarantees for updates in multi-table transactions.\nCloud Spanner is well suited for large transactional databases that require horizontal scaling and offers relational database semantics.\nEven if the first version was PostgreSQL, Cloud Spanner is the best choice for this kind of application with strict requirements for ACID transactions.\nCloud SQL is also a relational database service, and while some database engines offer ACID transactions, it is not designed like Cloud Spanner for the strict requirements of multi-table transactional updates.\nReference link: Google Cloud - Cloud Spanner: https://cloud.google.com/spanner\nReference link: Google Cloud - ACID Transactions in Cloud Spanner: https://cloud.google.com/spanner/docs/transactions"
      },
      {
        "index": 10,
        "text": "Anonymous: Mo73w 2Â years, 8Â months ago\nSelected Answer: B\nthe best choice for this application is Cloud SQL for PostgreSQL. It offers the required strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. It is also a good choice for applications that are implemented in PostgreSQL and that you want to deploy to the cloud with minimal code changes."
      }
    ]
  },
  {
    "id": 99,
    "source": "examtopics",
    "question": "You are assigned to maintain a Google Kubernetes Engine (GKE) cluster named 'dev' that was deployed on Google Cloud. You want to manage the GKE configuration using the command line interface (CLI). You have just downloaded and installed the Cloud SDK. You want to ensure that future CLI commands by default address this specific cluster What should you do?",
    "options": {
      "A": "Use the command gcloud config set container/cluster dev.",
      "B": "Use the command gcloud container clusters update dev.",
      "C": "Create a file called gke.default in the ~/.gcloud folder that contains the cluster name.",
      "D": "Create a file called defaults.json in the ~/.gcloud folder that contains the cluster name."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: 73173v2 Highly Voted 2Â years, 10Â months ago\nSelected Answer: A\nTo set a default cluster for gcloud commands, run the following command:\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters"
      },
      {
        "index": 2,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 1Â month ago\nSelected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters#default_cluster_gcloud\nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "index": 3,
        "text": "Anonymous: Jonassamr 1Â year, 2Â months ago\nSelected Answer: A\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=fr"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: A\nAnswer = A"
      },
      {
        "index": 5,
        "text": "Anonymous: snkhatri 2Â years, 10Â months ago\nSelected Answer: A\nA looks right to me"
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 3Â years, 1Â month ago\nA is right\nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "index": 7,
        "text": "Anonymous: ggupton1 3Â years, 2Â months ago\nSelected Answer: A\nSet a default cluster forgcloud\nTo set a default cluster for commands gcloud, run the following command:\nPer https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=fr\ngcloud config set container/cluster CLUSTER_NAME"
      },
      {
        "index": 8,
        "text": "Anonymous: Akash7 3Â years, 2Â months ago\nAnswer is A,\nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=en"
      },
      {
        "index": 9,
        "text": "Anonymous: MadMikedD 3Â years, 2Â months ago\nSelected Answer: A\nTo set a default cluster for gcloud commands, run the following command:\ngcloud config set container/cluster CLUSTER_NAME cheeseburger12388 3Â years, 2Â months ago\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters#default_cluster_kubectl"
      },
      {
        "index": 10,
        "text": "Anonymous: aswinachu 3Â years, 2Â months ago\nSelected Answer: B\nCorrect Ans B\nhttps://cloud.google.com/sdk/gcloud/reference/container/clusters/update"
      }
    ]
  },
  {
    "id": 100,
    "source": "examtopics",
    "question": "The sales team has a project named Sales Data Digest that has the ID acme-data-digest. You need to set up similar Google Cloud resources for the marketing team but their resources must be organized independently of the sales team. What should you do?",
    "options": {
      "A": "Grant the Project Editor role to the Marketing team for acme-data-digest.",
      "B": "Create a Project Lien on acme-data-digest and then grant the Project Editor role to the Marketing team.",
      "C": "Create another project with the ID acme-marketing-data-digest for the Marketing team and deploy the resources there.",
      "D": "Create a new project named Marketing Data Digest and use the ID acme-data-digest. Grant the Project Editor role to the Marketing team."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: gcpj Highly Voted 3Â years ago\nSelected Answer: C\nAnswer should be C because the resources for the marketing team should be independent from the Sales team. Resources are tied and separated by projects."
      },
      {
        "index": 2,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: C\nOption D is not allowed in Google Cloud, this option suggests using the same project ID (acme-data-digest) that is already assigned to the sales team's project. Since project IDs must be globally unique, this is not possible. Attempting to create a new project with an existing ID would result in an error."
      },
      {
        "index": 3,
        "text": "Anonymous: accd3fd 1Â year, 3Â months ago\nSelected Answer: C\nCreating a separate project for the Marketing team allows you to organize their resources independently of the Sales team, which is a best practice for resource management and access control.\nGranting the Project Editor role to the Marketing team for the Sales team's project (acme-data-digest) would give them unnecessary access to Sales team resources (option A).\nCreating a Project Lien (option B) is not relevant in this scenario, as it's used to prevent resource deletion, not to manage access or organization.\nUsing the same ID (acme-data-digest) for a new project (option D) could lead to confusion and conflicts, and is not a recommended practice.\nBy creating a separate project for the Marketing team, you can ensure clear organization, access control, and resource management for both teams."
      },
      {
        "index": 4,
        "text": "Anonymous: kautela13 1Â year, 4Â months ago\nC is the answer"
      },
      {
        "index": 5,
        "text": "Anonymous: datozzxx 1Â year, 6Â months ago\nSelected Answer: C\nc = 100%"
      },
      {
        "index": 6,
        "text": "Anonymous: cooldude26 1Â year, 8Â months ago\nSelected Answer: C\nC. Create another project with the ID acme-marketing-data-digest for the Marketing team and deploy the resources there.\nExplanation:\nOption C is the correct choice for organizing resources independently for the marketing team. By creating a separate project (acme-marketing-data-digest), you ensure that the marketing team's resources are isolated from the sales team's resources. This approach provides a clean and distinct organizational structure for each team.\nOptions A, B, and D involve using the same project (acme-data-digest) for both teams, which could lead to potential conflicts and lack of resource isolation. Option B suggests using a project lien, but liens are typically used to prevent the deletion of projects and don't provide the organizational separation needed for independent teams."
      },
      {
        "index": 7,
        "text": "Anonymous: ezzar 1Â year, 9Â months ago\nSelected Answer: C\nonly C makes sense"
      },
      {
        "index": 8,
        "text": "Anonymous: fdelacortina 1Â year, 9Â months ago\nWhy not D?"
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: C\nAnswer = C"
      },
      {
        "index": 10,
        "text": "Anonymous: Dmosh 2Â years ago\nSelected Answer: C\nNo more a technical exam ;("
      }
    ]
  },
  {
    "id": 101,
    "source": "examtopics",
    "question": "You have deployed multiple Linux instances on Compute Engine. You plan on adding more instances in the coming weeks. You want to be able to access all of these instances through your SSH client over the internet without having to configure specific access on the existing and new instances. You do not want the\nCompute Engine instances to have a public IP. What should you do?",
    "options": {
      "A": "Configure Cloud Identity-Aware Proxy for HTTPS resources.",
      "B": "Configure Cloud Identity-Aware Proxy for SSH and TCP resources",
      "C": "Create an SSH keypair and store the public key as a project-wide SSH Key.",
      "D": "Create an SSH keypair and store the private key as a project-wide SSH Key."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Akash7 Highly Voted 2Â years, 8Â months ago\nB is correct as question say no public IP on the instance. Akash7 2Â years, 8Â months ago\nUse IAP TCP to enable access to VM instances that do not have external IP addresses or do not permit direct access over the internet.\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding"
      },
      {
        "index": 2,
        "text": "Anonymous: Untamables Highly Voted 2Â years, 2Â months ago\nSelected Answer: B\nAbsolutely B\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_ssh_connections"
      },
      {
        "index": 3,
        "text": "Anonymous: Vovtchick Most Recent 1Â year, 2Â months ago\nAnswer B\nhttps://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts"
      },
      {
        "index": 4,
        "text": "Anonymous: jimmydice 1Â year, 2Â months ago\nB - Cloud Identity-Aware Proxy (IAP) allows you to set up secure access to your VM instances without the need to expose them to the public internet. By using IAP for SSH and TCP resources, you can manage access to the instances through a central point (IAP), which serves as a secure way to access your resources without the need for public IP addresses.\nIAP allows you to set up access controls based on user identities and their permissions, rather than relying on specific IP addresses or public keys configured on individual instances. This streamlines access management and enhances security, providing centralized control over SSH access to your Compute Engine instances."
      },
      {
        "index": 5,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: Gautam_Thampy 2Â years, 4Â months ago\nSelected Answer: B\nb is right"
      },
      {
        "index": 7,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: B\nB looks right"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nB is correct, With TCP forwarding, IAP can protect SSH and RDP access to your VMs hosted on Google Cloud. Your VM instances don't even need public IP addresses."
      },
      {
        "index": 9,
        "text": "Anonymous: Rutu_98 2Â years, 8Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: lixamec 2Â years, 8Â months ago\nSelected Answer: B\nI think it is B\nhttps://medium.com/google-cloud/how-to-ssh-into-your-gce-machine-without-a-public-ip-4d78bd23309e"
      }
    ]
  },
  {
    "id": 102,
    "source": "examtopics",
    "question": "You have created an application that is packaged into a Docker image. You want to deploy the Docker image as a workload on Google Kubernetes Engine. What should you do?",
    "options": {
      "A": "Upload the image to Cloud Storage and create a Kubernetes Service referencing the image.",
      "B": "Upload the image to Cloud Storage and create a Kubernetes Deployment referencing the image.",
      "C": "Upload the image to Container Registry and create a Kubernetes Service referencing the image.",
      "D": "Upload the image to Container Registry and create a Kubernetes Deployment referencing the image."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Aninina Highly Voted 2Â years, 2Â months ago\nSelected Answer: D\nA deployment is responsible for keeping a set of pods running. A service is responsible for enabling network access to a set of pods."
      },
      {
        "index": 2,
        "text": "Anonymous: kaes Highly Voted 1Â year, 7Â months ago\nSelected Answer: D\nKeep in mind, in the new exam, it would be rather \"Artifact Registry\""
      },
      {
        "index": 3,
        "text": "Anonymous: Priyanka109 Most Recent 2Â years, 3Â months ago\nUpload your docker image on container registry then give a ref while creating deployment. So D!"
      },
      {
        "index": 4,
        "text": "Anonymous: rimjhim09 2Â years, 4Â months ago\nSelected Answer: D\nI also vote for D. I passed my exam today and this question was there. nurai 2Â years, 1Â month ago\nYou said passed the exam, How do I know which answer to take? Voting and the actual answer never match. You passed the exam using this question? Should I use community answer or most voted? Pls, help. I have an exam in 3 days.Thanks nanakn 2Â years, 1Â month ago\nhow was your exam? what's your answer to this question that you asked, now that you have given it."
      },
      {
        "index": 5,
        "text": "Anonymous: Gautam_Thampy 2Â years, 4Â months ago\nits D , A and B are obviously incorrect"
      },
      {
        "index": 6,
        "text": "Anonymous: pkmdb66 2Â years, 4Â months ago\nSelected Answer: D\nItâ€™s D"
      },
      {
        "index": 7,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: D\nD is right"
      },
      {
        "index": 8,
        "text": "Anonymous: Sakhi1234 2Â years, 4Â months ago\nSelected Answer: D\nI have hands on expereince to answer this question."
      },
      {
        "index": 9,
        "text": "Anonymous: learn_GCP 2Â years, 4Â months ago\nSelected Answer: D\nD is the Answer"
      },
      {
        "index": 10,
        "text": "Anonymous: Bootshale 2Â years, 7Â months ago\nSelected Answer: D\nD - not even a debate!"
      }
    ]
  },
  {
    "id": 103,
    "source": "examtopics",
    "question": "You are using Data Studio to visualize a table from your data warehouse that is built on top of BigQuery. Data is appended to the data warehouse during the day.\nAt night, the daily summary is recalculated by overwriting the table. You just noticed that the charts in Data Studio are broken, and you want to analyze the problem. What should you do?",
    "options": {
      "A": "Review the Error Reporting page in the Cloud Console to find any errors.",
      "B": "Use the BigQuery interface to review the nightly job and look for any errors.",
      "C": "Use Cloud Debugger to find out why the data was not refreshed correctly.",
      "D": "In Cloud Logging, create a filter for your Data Studio report."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JoniMONI Highly Voted 2Â years, 12Â months ago\nSelected Answer: B\nB. Use the BigQuery interface to review the nightly job and look for any errors.\nSince the problem is related to the data in the data warehouse, it would be useful to check the status of the nightly job that recalculates the data and overwrites the table. By reviewing the job in the BigQuery interface, you can see if it completed successfully and if there were any errors that may have caused the charts in Data Studio to break. Reviewing the Error Reporting page in the Cloud Console, using Cloud Debugger and creating a filter in Cloud Logging may not be directly related to the problem with the data."
      },
      {
        "index": 2,
        "text": "Anonymous: hylee Highly Voted 2Â years, 1Â month ago\nfor those who says 'C' is the answer :\nCloud Debugger was deprecated on May 16, 2022 and the service was shut down on May 31, 2023. You can continue to use the open source Snapshot Debugger. Snapshot Debugger was archived on September 7, 2023, so it is not receiving bug fixes or security patches. Snapshot Debugger remains available for use. You can also fork the repository and maintain your own version.\nCloud Debugger was deprecated on May 16, 2022 and the service was shut down on May 31, 2023. You can continue to use the open source Snapshot Debugger. Snapshot Debugger was archived on September 7, 2023, so it is not receiving bug fixes or security patches. Snapshot Debugger remains available for use. You can also fork the repository and maintain your own version.\nhttps://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation"
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 4Â months ago\nSo which on is the correct answer?"
      },
      {
        "index": 4,
        "text": "Anonymous: master9 1Â year, 4Â months ago\nSelected Answer: B\nSince the issue involves a table in BigQuery, which is overwritten nightly with recalculated data, it's most likely that something went wrong during this nightly job. Reviewing the job in the BigQuery interface allows you to check for errors, job failures, or any issues during the table overwrite process, which could be causing the charts in Data Studio to break."
      },
      {
        "index": 5,
        "text": "Anonymous: carlalap 2Â years, 1Â month ago\nAnswer: C\nCloud Debugger provides targeted debugging capabilities for Data Studio, allowing you to focus on the specific report or charts that are experiencing issues."
      },
      {
        "index": 6,
        "text": "Anonymous: thanab 2Â years, 2Â months ago\nSelected Answer: B\nYou should use the BigQuery interface to review the nightly job and look for any errors.\nThe reason for this is that the nightly job is responsible for recalculating the daily summary by overwriting the table. If there are any errors in the nightly job, it could cause the charts in Data Studio to be broken. By reviewing the nightly job, you can identify and fix any errors that may be causing the problem.\nThe other options are not as likely to be helpful in this situation. The Error Reporting page in the Cloud Console is not likely to be helpful because it will only show errors that have been reported by other users. The Cloud Debugger is not likely to be helpful because it is used to debug code, not to troubleshoot problems with data. The Cloud Logging filter is not likely to be helpful because it is used to filter logs, not to troubleshoot problems with data."
      },
      {
        "index": 7,
        "text": "Anonymous: __rajan__ 2Â years, 2Â months ago\nSelected Answer: B\nB. Use the BigQuery interface to review the nightly job and look for any errors.\nSince the charts in Data Studio are broken, and the data is being appended to the data warehouse during the day and the daily summary is being recalculated at night by overwriting the table, the most likely cause of the problem is an error in the nightly job."
      },
      {
        "index": 8,
        "text": "Anonymous: ovokpus 2Â years, 3Â months ago\nSelected Answer: B\nThe other options are less likely to provide the information you need for this specific situation:\nA. \"Review the Error Reporting page in the Cloud Console\" is more about application errors in code, rather than issues with BigQuery job executions.\nC. \"Use Cloud Debugger\" is not applicable here as it's used for debugging applications written in languages like Java, Python, etc., and doesn't work with BigQuery SQL or data processing tasks.\nD. \"In Cloud Logging, create a filter for your Data Studio report\" might not be helpful because, while Cloud Logging does capture a wide variety of logs, it doesn't provide the direct, detailed job execution information that the BigQuery interface does. Furthermore, Data Studio reporting errors may not be related to the underlying data processing job."
      },
      {
        "index": 9,
        "text": "Anonymous: ovokpus 2Â years, 3Â months ago\nSelected Answer: B\nB. Use the BigQuery interface to review the nightly job and look for any errors.\nHereâ€™s why this approach is appropriate:\nJob Information: BigQuery logs information about every job executed, including data loads or query jobs. If the nightly job that refreshes your data warehouse table encountered an error, this would be captured and could be viewed in the job's information in the BigQuery console.\nError Details: If the job failed or encountered issues, the BigQuery interface would provide details about the error, which can help you understand if there were problems with the query syntax, the data, or any other aspect of the operation.\nImmediate Feedback: Reviewing the job's execution details can give you immediate insights without needing to wait for logs or errors to propagate through other systems, which can be time-consuming and might not provide the direct feedback you need."
      },
      {
        "index": 10,
        "text": "Anonymous: on2it 2Â years, 6Â months ago\nSelected Answer: B\nIt needs to be B because it is the only way to proper investigate the issue."
      }
    ]
  },
  {
    "id": 104,
    "source": "examtopics",
    "question": "You have been asked to set up the billing configuration for a new Google Cloud customer. Your customer wants to group resources that share common IAM policies. What should you do?",
    "options": {
      "A": "Use labels to group resources that share common IAM policies.",
      "B": "Use folders to group resources that share common IAM policies.",
      "C": "Set up a proper billing account structure to group IAM policies.",
      "D": "Set up a proper project naming structure to group IAM policies."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: thimai Highly Voted 2Â years, 4Â months ago\nSelected Answer: B\nB for me\n\"Folders are used to group resources that share common IAM policies\"\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders"
      },
      {
        "index": 2,
        "text": "Anonymous: Charumathi Highly Voted 2Â years, 3Â months ago\nSelected Answer: B\nB is correct Answer,\nFolders are nodes in the Cloud Platform Resource Hierarchy. A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders are used to group resources that share common IAM policies. While a folder can contain multiple folders or resources, a given folder or resource can have exactly one parent.\nhttps://cloud.google.com/resource-manager/docs/creating-managing-folders"
      },
      {
        "index": 3,
        "text": "Anonymous: jimmydice Most Recent 1Â year, 2Â months ago\nB: Folders in Google Cloud provide a way to organize resources hierarchically and apply common IAM (Identity and Access Management) policies at the folder level. This structure allows you to group resources together based on organizational needs, such as by department, teams, environments, or projects, and apply IAM policies to the entire group of resources within that folder.\nBy utilizing folders, you can effectively manage and enforce consistent access controls, policies, and permissions across multiple resources within the same folder, thereby fulfilling the customer's requirement of grouping resources with common IAM policies."
      },
      {
        "index": 4,
        "text": "Anonymous: ExamsFR 1Â year, 4Â months ago\nSelected Answer: B\nB is correct Answer,"
      },
      {
        "index": 5,
        "text": "Anonymous: diasporabro 2Â years, 3Â months ago\nSelected Answer: B\n\"Folders are used to group resources that share common IAM policies.\""
      },
      {
        "index": 6,
        "text": "Anonymous: zellck 2Â years, 4Â months ago\nSelected Answer: B\nB is the answer.\nhttps://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam"
      },
      {
        "index": 7,
        "text": "Anonymous: RockingRohit6 2Â years, 4Â months ago\nFolders are used to group resources that share common IAM policies"
      },
      {
        "index": 8,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: B\nB seems right to me"
      }
    ]
  },
  {
    "id": 105,
    "source": "examtopics",
    "question": "You have been asked to create robust Virtual Private Network (VPN) connectivity between a new Virtual Private Cloud (VPC) and a remote site. Key requirements include dynamic routing, a shared address space of 10.19.0.1/22, and no overprovisioning of tunnels during a failover event. You want to follow Google- recommended practices to set up a high availability Cloud VPN. What should you do?",
    "options": {
      "A": "Use a custom mode VPC network, configure static routes, and use active/passive routing.",
      "B": "Use an automatic mode VPC network, configure static routes, and use active/active routing.",
      "C": "Use a custom mode VPC network, use Cloud Router border gateway protocol (BGP) routes, and use active/passive routing.",
      "D": "Use an automatic mode VPC network, use Cloud Router border gateway protocol (BGP) routes, and configure policy-based routing."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: theBestStudent Highly Voted 2Â years, 3Â months ago\nSelected Answer: C\nwe need custom mode vpc so subnets are not created automatically (the ip range is mentioned in the question) also we will need active/passive HA VPN (as it is not mentioned we will have to use more than one HA VPN gateway).\nLinks : https://cloud.google.com/network-connectivity/docs/vpn/concepts/best-practices\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#active\nhttps://cloud.google.com/vpc/docs/vpc#subnet-ranges theBestStudent 2Â years, 3Â months ago\nAlso for dynamic routing we need HA VPN\nLink: https://cloud.google.com/network-connectivity/docs/vpn/concepts/choosing-networks-routing#dynamic-routing"
      },
      {
        "index": 2,
        "text": "Anonymous: Charumathi Highly Voted 2Â years, 3Â months ago\nSelected Answer: C\nC . Choose a Cloud VPN gateway that uses dynamic routing and the Border Gateway Protocol (BGP). Google recommends using HA VPN and deploying on-premises devices that support BGP.\nChoose the appropriate tunnel configuration\nChoose the appropriate tunnel configuration based on the number of HA VPN gateways:\nIf you have a single HA VPN gateway, use an active/passive tunnel configuration.\nIf you have more than one HA VPN gateway, use an active/active tunnel configuration.\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/best-practices"
      },
      {
        "index": 3,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: C\nThe best option is:C.\nUse a custom mode VPC network, use Cloud Router border gateway protocol (BGP) routes, and use active/passive routing.\nReason:\nâ€¢ Custom mode VPC network: Provides granular control over IP address ranges and subnets, essential for handling the shared address space of 10.19.0.1/22.\nâ€¢ Cloud Router with BGP: Enables dynamic routing, ensuring efficient route updates and avoiding manual intervention when changes occur.\nâ€¢ Active/passive routing: Ensures no overprovisioning of tunnels during failover events, meeting the key requirement of robust connectivity."
      },
      {
        "index": 4,
        "text": "Anonymous: __rajan__ 1Â year, 2Â months ago\nSelected Answer: C\nC is correct."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: C\nC is the correct answer as we need to make sure that the subnets are not being created automatically"
      },
      {
        "index": 6,
        "text": "Anonymous: ale_brd_111 2Â years, 3Â months ago\nSelected Answer: C\nc is the correct one"
      },
      {
        "index": 7,
        "text": "Anonymous: Arulkumar 2Â years, 3Â months ago\nSelected Answer: C\nGoogle Cloud Router\nOn Google Cloud, dynamic routing can be established using Cloud Router. It exchanges network topology information through Border Gateway Protocol (BGP). Cloud Router advertises subnets from its VPC network to another router or gateway via BGP. This is great for setting up VPN between the cloud and on-prem, as topology changes automatically propagate with no manual intervention and higher redundancy for your systems.\nYou now have:\nDiscovery of remote networks\nMaintaining up-to-date routing information\nChoosing the best path to destination networks\nAbility to find a new best path if the current path is no longer available\nAnd a great side effect can be lower latency because Cloud Router learns routes through BGP which allows for optimal data paths to reach its destination, whether that be another network or a VPN gateway to on-premise. Cloud Router is also how Dedicated Interconnect can give you 10 gbp/s bandwidth between your cloud VPC and your peered on-premise data center."
      },
      {
        "index": 8,
        "text": "Anonymous: manjtrade2 2Â years, 4Â months ago\nSelected Answer: C\nC might be right"
      },
      {
        "index": 9,
        "text": "Anonymous: snkhatri 2Â years, 4Â months ago\nSelected Answer: C\nI think it should be C as there is too much customisation."
      }
    ]
  },
  {
    "id": 106,
    "source": "examtopics",
    "question": "You are running multiple microservices in a Kubernetes Engine cluster. One microservice is rendering images. The microservice responsible for the image rendering requires a large amount of CPU time compared to the memory it requires. The other microservices are workloads that are optimized for n1-standard machine types. You need to optimize your cluster so that all workloads are using resources as efficiently as possible. What should you do?",
    "options": {
      "A": "Assign the pods of the image rendering microservice a higher pod priority than the other microservices.",
      "B": "Create a node pool with compute-optimized machine type nodes for the image rendering microservice. Use the node pool with general-purpose machine type nodes for the other microservices.",
      "C": "Use the node pool with general-purpose machine type nodes for the image rendering microservice. Create a node pool with compute-optimized machine type nodes for the other microservices.",
      "D": "Configure the required amount of CPU and memory in the resource requests specification of the image rendering microservice deployment. Keep the resource requests for the other microservices at the default."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gautam_Thampy Highly Voted 1Â year, 4Â months ago\nSelected Answer: B\nC is not correct coz general purpose machine types will not suffice for image rendering.\nB is the most suitable answer."
      },
      {
        "index": 2,
        "text": "Anonymous: calm_fox Most Recent 1Â year, 1Â month ago\nSelected Answer: B\nB is right"
      },
      {
        "index": 3,
        "text": "Anonymous: diasporabro 1Â year, 3Â months ago\nSelected Answer: B\nB looks like the right choice here"
      },
      {
        "index": 4,
        "text": "Anonymous: learn_GCP 1Â year, 3Â months ago\nSelected Answer: B\nB. is the Answer"
      },
      {
        "index": 5,
        "text": "Anonymous: adarsh4503 1Â year, 3Â months ago\nI agree B is the answer."
      },
      {
        "index": 6,
        "text": "Anonymous: zellck 1Â year, 4Â months ago\nSelected Answer: B\nB is the answer."
      },
      {
        "index": 7,
        "text": "Anonymous: osanchez 1Â year, 4Â months ago\nB is correct"
      }
    ]
  },
  {
    "id": 107,
    "source": "examtopics",
    "question": "Your organization has three existing Google Cloud projects. You need to bill the Marketing department for only their Google Cloud services for a new initiative within their group. What should you do?",
    "options": {
      "A": "1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud Project for the Marketing department. 2. Link the new project to a Marketing Billing Account.",
      "B": "1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project.",
      "C": "1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Link the new project to a Marketing Billing Account.",
      "D": "1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: gcpreviewer Highly Voted 3Â years, 3Â months ago\nSelected Answer: A\nI understand that the question implies the creation of a new project, however neither of the roles listed have that functionality. If you chose B you are choosing an answer that has a direct contradiction because the Billing Account Admin does not have the permissions to create a new project. Thus, I think it is better to assume the new initiative/project is already created or being created by someone else and your job is simply to link the project to the account which you do have the appropriate permissions to perform.\nA is my choice. sukouto 1Â year, 11Â months ago\nWorth noting that an Organization Administrator doesn't have permissions to deal with billing, so I think C/D are no good:\nhttps://cloud.google.com/iam/docs/understanding-roles#resourcemanager.organizationAdmin"
      },
      {
        "index": 2,
        "text": "Anonymous: moitsu Highly Voted 3Â years, 4Â months ago\nSelected Answer: B\nBetween A& B, Billing Administrator IAM role is either at the organisation level not project level. Hence A is out. C & D doesn't make sense. Bajeerao 1Â year, 1Â month ago\nYou can control viewing permissions at different levels for different users or roles by setting access permissions at the Cloud Billing account or project level.\nhence i feel its B Gautam_Thampy 3Â years, 4Â months ago\nThe billing account administrator role can also be given at the project level. A is correct. Refer to this doc: https://cloud.google.com/billing/docs/how-to/billing-access Gautam_Thampy 3Â years, 3Â months ago\nCorrection I meant the billing account admin role can be given at the organisation or the billing account level. AdelElagawany 2Â years, 2Â months ago\nOption A is incorrect because the billing account admin scope is either the \"Organization level\" or the \"Billing account level\" AdelElagawany 2Â years, 2Â months ago\nOption A is incorrect because the billing account admin scope is either the \"Organization level\" or the \"Billing account level\" NOT the project level"
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua Most Recent 11Â months, 3Â weeks ago\nSelected Answer: C\nA. is incorrect because the Billing Administrator role is not tied to a specific project but rather to the billing account or organization. Additionally, it does not mention creating a new project, which is necessary for isolating the Marketing department's resources.\nC. To create a new project and link it to a billing account, you need the Organization Administrator role or the Billing Administrator role. The Organization Administrator role ensures you have the necessary permissions to manage projects and billing at the organizational level.\nCreate a new Google Cloud project specifically for the Marketing department. This ensures that resources and services used for their initiative are isolated and can be billed separately.\nLink the new project to a dedicated Marketing Billing Account. This ensures that all costs associated with the Marketing department's initiative are billed accurately and separately from other departments."
      },
      {
        "index": 4,
        "text": "Anonymous: kamee15 1Â year ago\nSelected Answer: A\nI believe the answer is A\nStep No.1: Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud Project for the Marketing Department.\nStep No.2: Link the new project to the Marketing Billing Account.\nREASON NO.1: Billing Administrator role: This role is sufficient to manage billing settings and accounts, which is all that is required for this scenario.\nREASON NO. 2: â€¢ Linking the project to a Marketing Billing Account: Ensures that the costs for the Marketing departmentâ€™s new initiative are tracked and billed separately without impacting other projects."
      },
      {
        "index": 5,
        "text": "Anonymous: har508206 1Â year, 11Â months ago\nAccording to gcp docs - https://cloud.google.com/iam/docs/understanding-roles#resourcemanager.organizationAdmin. Org Admin does not have this permission -> resourcemanager.projects.create, necessary to create project. So C and D are out"
      },
      {
        "index": 6,
        "text": "Anonymous: sivakarthick16 1Â year, 12Â months ago\nSelected Answer: C\nBy assigning the Organization Administrator IAM role, you will have the necessary permissions to manage the organization's Google Cloud resources.\nCreating a new project for the Marketing department ensures that their services are isolated and billed separately.\nLinking the new project to a Marketing Billing Account allows you to track and manage the billing specifically for the Marketing department's initiatives.\nSetting default key-value project labels to department:marketing for all services in this project (as mentioned in option D) is not necessary for billing purposes, but it can be helpful for organizing and categorizing resources within the project."
      },
      {
        "index": 7,
        "text": "Anonymous: sinh 2Â years ago\nSelected Answer: C\nit's C."
      },
      {
        "index": 8,
        "text": "Anonymous: nudiiiir 2Â years ago\nSelected Answer: C\nit's C"
      },
      {
        "index": 9,
        "text": "Anonymous: ogerber 2Â years, 1Â month ago\nSelected Answer: C\nIts C."
      },
      {
        "index": 10,
        "text": "Anonymous: carlalap 2Â years, 1Â month ago\nAnswer is C.\nBilling Administrator IAM: https://cloud.google.com/iam/docs/job-functions/billing\nOrganization Administrator IAM: https://cloud.google.com/iam/docs/understanding-roles\nBilling Administrator IAM\nThe Billing Administrator IAM role allows users to manage billing-related tasks for a specific project. These tasks include:\n-Viewing and managing billing accounts\n-Setting billing alerts\n-Configuring payment methods\n-Monitoring billing activity\n-Organization Administrator IAM\nThe Organization Administrator IAM role allows users to manage overall Google Cloud usage and resources within an organization. These tasks include:\n-Creating and deleting projects\n-Managing users and groups\n-Setting up IAM policies\n-Enabling and configuring Cloud services"
      }
    ]
  },
  {
    "id": 108,
    "source": "examtopics",
    "question": "You deployed an application on a managed instance group in Compute Engine. The application accepts Transmission Control Protocol (TCP) traffic on port 389 and requires you to preserve the IP address of the client who is making a request. You want to expose the application to the internet by using a load balancer. What should you do?",
    "options": {
      "A": "Expose the application by using an external TCP Network Load Balancer.",
      "B": "Expose the application by using a TCP Proxy Load Balancer.",
      "C": "Expose the application by using an SSL Proxy Load Balancer.",
      "D": "Expose the application by using an internal TCP Network Load Balancer."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: PiperMe Highly Voted 1Â year, 4Â months ago\nSelected Answer: A\nThose saying B are incorrect:\n- External TCP Network Load Balancers DO preserve the client's IP address. This is a core feature of this type of load balancer in Google Cloud.\n- While TCP Proxy Load Balancers also support client IP preservation, their primary strength lies in additional Layer 7 capabilities.\n- In the absence of requirements for advanced traffic manipulation at the application layer, the External TCP Network Load Balancer remains the best choice."
      },
      {
        "index": 2,
        "text": "Anonymous: Cynthia2023 Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nTCP Network Load Balancer: This type of load balancer operates at the network layer (Layer 4 of the OSI model). It is designed for routing TCP traffic and is well-suited for scenarios where you need to maintain the original source IP address of the client. This is crucial in your case since the application requires the preservation of the client's IP address."
      },
      {
        "index": 3,
        "text": "Anonymous: yodaforce Most Recent 10Â months ago\nSelected Answer: A\nThe external TCP Network Load Balancer is the best option when you need to preserve the clientâ€™s original IP address, as it operates at Layer 4 (TCP/UDP) and does not proxy traffic.\nSince the application is running on port 389 (LDAP over TCP) and requires direct client IP visibility, a Layer 4 load balancer is required."
      },
      {
        "index": 4,
        "text": "Anonymous: c2e9cb4 1Â year, 6Â months ago\nSelected Answer: A\nHow to preserve client IP in a Network Load Balancer TCP :\nhttps://cloud.google.com/load-balancing/docs/tcp/setting-up-tcp#proxy-protocol"
      },
      {
        "index": 5,
        "text": "Anonymous: nudiiiir 1Â year, 6Â months ago\nSelected Answer: B\nA. External TCP Network Load Balancer: While it handles TCP traffic, it doesn't inherently preserve client IP addresses.\nC. SSL Proxy Load Balancer: This is primarily intended for encrypted SSL traffic, not general TCP traffic.\nD. Internal TCP Network Load Balancer: This is for internal traffic within a VPC, not for exposing applications to the internet. PiperMe 1Â year, 4Â months ago\nIncorrect. External TCP Network Load Balancers DO preserve the client's IP address."
      },
      {
        "index": 6,
        "text": "Anonymous: ogerber 1Â year, 7Â months ago\nIt is A,\nNote: Proxy-based load balancers send connections to the backends from different GFE or Envoy IP addresses. If you're using a form of authentication that relies on keeping track of the IP address that opened the first connection, and expects that same IP address to open the second connection, you might not want to use a proxy load balancer. Proxy load balancers don't preserve client IP addresses by default. This type of authentication is more compatible with the passthrough load balancers. For proxy load balancers such as the internal and external Application Load Balancers, we recommend that you use Identity-Aware Proxy (IAP) as your authentication method instead.\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer#:~:text=Proxy%20load%20balancers%20do%20not%20preserve%20client%20IP"
      },
      {
        "index": 7,
        "text": "Anonymous: carlalap 1Â year, 7Â months ago\nAnswer is: A\nExternal proxy Network Load Balancers let you use a single IP address for all users worldwide.\nhttps://cloud.google.com/load-balancing/docs/tcp"
      },
      {
        "index": 8,
        "text": "Anonymous: joao_01 1Â year, 10Â months ago\nIts A, for sure"
      },
      {
        "index": 9,
        "text": "Anonymous: shreykul 1Â year, 12Â months ago\nSelected Answer: A\nhttps://cloud.google.com/load-balancing/docs/choosing-load-balancer#:~:text=Proxy%20load%20balancers%20do%20not%20preserve%20client%20IP"
      },
      {
        "index": 10,
        "text": "Anonymous: geeroylenkins 2Â years ago\nSelected Answer: A\nI am going with A as the client IP needs to be preserved. Not sure with on2it votes once for A and once for B with the same comment especially because you need a *pass-through* load balancer to preserve the client IP as stated here: https://cloud.google.com/load-balancing/docs/choosing-load-balancer#proxy-pass-through\n\"You'd choose a passthrough Network Load Balancer to preserve client source IP addresses\""
      }
    ]
  },
  {
    "id": 109,
    "source": "examtopics",
    "question": "You are building a multi-player gaming application that will store game information in a database. As the popularity of the application increases, you are concerned about delivering consistent performance. You need to ensure an optimal gaming performance for global users, without increasing the management complexity. What should you do?",
    "options": {
      "A": "Use Cloud SQL database with cross-region replication to store game statistics in the EU, US, and APAC regions.",
      "B": "Use Cloud Spanner to store user data mapped to the game statistics.",
      "C": "Use BigQuery to store game statistics with a Redis on Memorystore instance in the front to provide global consistency.",
      "D": "Store game statistics in a Bigtable database partitioned by username."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: gpais Highly Voted 1Â year, 11Â months ago\nSelected Answer: B\nhttps://cloud.google.com/solutions/databases/games"
      },
      {
        "index": 2,
        "text": "Anonymous: tatyavinchu Highly Voted 1Â year, 11Â months ago\nglobal users = Cloud Spanner\nCorrect Answer is B"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nOption B, leveraging Cloud Spanner, provides a powerful solution specifically designed for globally distributed, consistently performant applications while keeping operational complexity low â€“ making it the ideal choice for the multi-player gaming scenario.\nBigtable is well-suited for massive scale, but its NoSQL nature might require more data modeling effort compared to Cloud Spanner for gaming-related data."
      },
      {
        "index": 4,
        "text": "Anonymous: DanBar 1Â year, 5Â months ago\nSelected Answer: D\nBigtable"
      },
      {
        "index": 5,
        "text": "Anonymous: nudiiiir 1Â year, 6Â months ago\nSelected Answer: D\nit's D because in this specific case cause Bigtable scales seamlessly to handle massive amounts of data and high read/write throughput,\nideal for multiplayer gaming applications"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: B\nSpanner should meet expectation"
      },
      {
        "index": 7,
        "text": "Anonymous: MrJkr 2Â years ago\nSelected Answer: B\nAmong the options provided, the better answer for ensuring optimal gaming performance for global users without increasing management complexity would be option B\nCloud Spanner is a globally distributed, horizontally scalable database service provided by Google Cloud Platform. It offers strong consistency guarantees, high availability, and automatic scaling.\nIt offers the necessary features to ensure optimal gaming performance, global scalability, strong consistency, and automatic scaling, making it a suitable choice for storing user data mapped to game statistics."
      }
    ]
  },
  {
    "id": 110,
    "source": "examtopics",
    "question": "You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use?",
    "options": {
      "A": "Cloud SQL",
      "B": "Firestore",
      "C": "Cloud Spanner",
      "D": "Bigtable"
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: mapcio123 Highly Voted 2Â years, 6Â months ago\nSelected Answer: C\nspanner- relational and global"
      },
      {
        "index": 2,
        "text": "Anonymous: Ciupaz Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nThe key phrase is \"with minimum configurations changes\". Cloud Spanner is the best choice."
      },
      {
        "index": 3,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: C\nand scales horizontally"
      },
      {
        "index": 4,
        "text": "Anonymous: Husni_adam 2Â years, 6Â months ago\nSelected Answer: C\nCloud Spanner because Relational database, scale across regions for workloads that have more stringent availability requirements, Handles large amounts of data and for high transactional consistency\nhttps://cloud.google.com/blog/topics/developers-practitioners/databases-google-cloud-part-2-options-glance/"
      },
      {
        "index": 5,
        "text": "Anonymous: _F4LLEN_ 2Â years, 6Â months ago\nSpanner"
      },
      {
        "index": 6,
        "text": "Anonymous: gpais 2Â years, 6Â months ago\nI vote option C"
      }
    ]
  },
  {
    "id": 111,
    "source": "examtopics",
    "question": "You have one GCP account running in your default region and zone and another account running in a non-default region and zone. You want to start a new\nCompute Engine instance in these two Google Cloud Platform accounts using the command line interface. What should you do?",
    "options": {
      "A": "Create two configurations using gcloud config configurations create [NAME]. Run gcloud config configurations activate [NAME] to switch between accounts when running the commands to start the Compute Engine instances.",
      "B": "Create two configurations using gcloud config configurations create [NAME]. Run gcloud configurations list to start the Compute Engine instances.",
      "C": "Activate two configurations using gcloud configurations activate [NAME]. Run gcloud config list to start the Compute Engine instances.",
      "D": "Activate two configurations using gcloud configurations activate [NAME]. Run gcloud configurations list to start the Compute Engine instances."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: leba Highly Voted 5Â years, 8Â months ago\nCorrect answer is A as you can create different configurations for each account and create compute instances in each account by activating the respective account.Refer GCP documentation - Configurations Create &amp; Activate Options B, C &amp; D are wrong as gcloud config configurations list does not help create instances. It would only lists existing named configurations."
      },
      {
        "index": 2,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\nA is the correct option"
      },
      {
        "index": 3,
        "text": "Anonymous: Urticas Most Recent 1Â day, 23Â hours ago\nSelected Answer: A\nWhen you need to manage multiple Google Cloud accounts, regions, or zones using the gcloud CLI, the recommended approach is to use named configurations.\nEach configuration can store:\nAccount credentials Project ID Default region Default zone\nThis allows you to easily switch contexts without reâ€‘specifying flags every time."
      },
      {
        "index": 4,
        "text": "Anonymous: svij87 1Â month ago\nSelected Answer: A\nA is correct option"
      },
      {
        "index": 5,
        "text": "Anonymous: Deepthi12 5Â months, 3Â weeks ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: Shybi 6Â months, 3Â weeks ago\nSelected Answer: A\nOption A. Keywords - Activate and list."
      },
      {
        "index": 7,
        "text": "Anonymous: 4ad26ef 1Â year, 4Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 9,
        "text": "Anonymous: YourCloudGuru 2Â years, 3Â months ago\nSelected Answer: A\nThe correct answer is A\nThis option allows you to create and activate different configurations for your GCP accounts. This way, you can easily switch between accounts and run commands without having to re-enter your credentials.\nThe other options are not as good:\n* Option B does not specify how to switch between accounts when running the commands to start the Compute Engine instances.\n* Option C does not specify how to create configurations for your GCP accounts.\n* Option D does not specify how to start the Compute Engine instances."
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nYes A seems to be more correct"
      }
    ]
  },
  {
    "id": 112,
    "source": "examtopics",
    "question": "You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do?",
    "options": {
      "A": "Use granular logging statements within a Deployment Manager template authored in Python.",
      "B": "Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.",
      "C": "Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.",
      "D": "Execute the Deployment Manager template using the ×’â‚¬\"-preview option in the same project, and observe the state of interdependent resources."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: YashBindlish Highly Voted 4Â years, 8Â months ago\nCorrect answer is D as Deployment Manager provides the preview feature to check on what resources would be created"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 1Â year, 11Â months ago\nSelected Answer: D\nAnswer D is the most appropriate choice for getting rapid feedback on changes to a Deployment Manager template.\nThe preview command in Deployment Manager creates a preview deployment of the resources defined in the configuration, without actually creating or modifying any resources. This allows you to quickly test and validate changes to the template before committing them to the project. During the preview, you can observe the state of interdependent resources and ensure that their dependencies are properly met. This provides rapid feedback on your changes, without actually creating any resources or incurring any costs. jogoldberg 4Â months, 2Â weeks ago\nWhat is the \"×’â‚¬\"-preview option\" mentioned in the question?"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: D\nAnswer is D as Deployment Manager provides the preview feature to review without creating/modifying the resources."
      },
      {
        "index": 4,
        "text": "Anonymous: Shybi 6Â months, 3Â weeks ago\nSelected Answer: D\nOption D. Rapid feedback is the keywords"
      },
      {
        "index": 5,
        "text": "Anonymous: tmpcs 1Â year, 1Â month ago\nD is right based on this part of doc: https://cloud.google.com/deployment-manager/docs/deployments/updating-deployments#optional_preview_an_updated_configuration"
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is D."
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: D\nThe correct option is D\nThe `--preview` option will preview the changes that will be made to the deployment without actually making them. This allows you to see how the changes will affect the deployment and to identify any potential problems.\nThe other options are not as good:\n* Option A is not as good, because it requires you to add logging statements to your Deployment Manager template. This can be time-consuming and error-prone.\n* Option B is not as good, because it requires you to monitor the Stackdriver Logging page of the GCP Console. This can be difficult to do, especially for complex deployments.\n* Option C is not as good, because it requires you to create a separate project with the same configuration. This can be time-consuming and expensive."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nD is the right answer aas deployment manager provides the preview option to check the templates"
      },
      {
        "index": 9,
        "text": "Anonymous: ExamsFR 1Â year, 6Â months ago\nSelected Answer: D\nD is right"
      },
      {
        "index": 10,
        "text": "Anonymous: Neha_Pallavi 1Â year, 6Â months ago\nD is correct. Execute the Deployment Manager template using the ×’â‚¬\"-preview option in the same project, and observe the state of interdependent resources."
      }
    ]
  },
  {
    "id": 113,
    "source": "examtopics",
    "question": "You are building a pipeline to process time-series data. Which Google Cloud Platform services should you put in boxes 1,2,3, and 4?",
    "options": {
      "A": "Cloud Pub/Sub, Cloud Dataflow, Cloud Datastore, BigQuery",
      "B": "Firebase Messages, Cloud Pub/Sub, Cloud Spanner, BigQuery",
      "C": "Cloud Pub/Sub, Cloud Storage, BigQuery, Cloud Bigtable",
      "D": "Cloud Pub/Sub, Cloud Dataflow, Cloud Bigtable, BigQuery"
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cloudenthu01 Highly Voted 4Â years, 6Â months ago\nWithout a doubt D.\nWhenever we want to process timeseries data look for BigTable.\nAlso you want to perform analystics in Box 4 ..look for BigQuery\nOnly D provides this option. vlodia 4Â years, 6Â months ago\nSpeaker also looks like an IoT device so D not A adedj99 4Â years, 1Â month ago\nare we considering bigtable as storage in here , since they expecting some storage"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 1Â year, 11Â months ago\nSelected Answer: D\nThe correct process for building a pipeline to process time-series data. Here's how each of the components is used:"
      },
      {
        "index": 1,
        "text": "Cloud Pub/Sub: receives and distributes time-series data from different sources."
      },
      {
        "index": 2,
        "text": "Cloud Dataflow: processes the data by applying transformations and analytics."
      },
      {
        "index": 3,
        "text": "Cloud Bigtable: stores and manages the processed data as a NoSQL database."
      },
      {
        "index": 4,
        "text": "BigQuery: provides a SQL-like interface to analyze the data and extract insights.\nBy combining these components, you can create a scalable and reliable pipeline to process and analyze time-series data in real time. kenrichy 1Â year, 10Â months ago\nMany thanks Buruguduy.. for the extensive explanation you always give to your choice of answers. It's really helpful to understand the concept."
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: D\nD. Datastore is an old name - new name is firestore in datastore mode. Processing time series data is best done in BigTable which supports timeseries small data in realtime and particulalry for IoT\n(non relational)"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nCorrect answer D"
      },
      {
        "index": 5,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: D\nThe correct answer is D\nThis diagram shows a typical pipeline for processing time-series data:"
      },
      {
        "index": 1,
        "text": "**Cloud Pub/Sub:** A messaging service that allows you to send and receive messages between independent applications."
      }
    ]
  },
  {
    "id": 114,
    "source": "examtopics",
    "question": "You have a project for your App Engine application that serves a development environment. The required testing has succeeded and you want to create a new project to serve as your production environment. What should you do?",
    "options": {
      "A": "Use gcloud to create the new project, and then deploy your application to the new project.",
      "B": "Use gcloud to create the new project and to copy the deployed application to the new project.",
      "C": "Create a Deployment Manager configuration file that copies the current App Engine deployment into a new project.",
      "D": "Deploy your application again using gcloud and specify the project parameter with the new project name to create the new project."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 4Â years, 10Â months ago\nCorrect is A.\nOption B is wrong as the option to use gcloud app cp does not exist.\nOption C is wrong as Deployment Manager does not copy the application, but allows you to specify all the resources needed for your application in a declarative format using yaml\nOption D is wrong as gcloud app deploy would not create a new project. The project should be created before usage manu202020 4Â years, 6Â months ago\nyou're missing one thing. D isn't about using deployment manager to copy the configuration, instead, using the configuration file to copy the configuration from test project. AdelElagawany 1Â year, 3Â months ago\nA is correct since the documentation here explicitly mentioned the roles of external editors https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "index": 2,
        "text": "Anonymous: leba Highly Voted 4Â years, 8Â months ago\nCorrect answer is A as gcloud can be used to create a new project and the gcloud app deploy can point to the new project.Refer GCP documentation - GCloud App Deploy.\nOption B is wrong as the option to use gcloud app cp does not exist\n.Option C is wrong as Deployment Manager does not copy the application, but allows you to specify all the resources needed for your application in a declarative format using yaml\nOption D is wrong as gcloud app deploy would not create a new project. The project should be created before usage."
      },
      {
        "index": 3,
        "text": "Anonymous: Urticas Most Recent 1Â day, 23Â hours ago\nSelected Answer: A\nWhen you want to promote an App Engine application from a development environment to a production environment, the recommended and correct approach is to treat the production setup as a separate project and deploy the app again."
      },
      {
        "index": 4,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA. You cannot copy app engine applications. You need to create the new prod project and then will need to gcloud app deploy --project <new proj name> with all the source to create a fesh instal of the same app."
      },
      {
        "index": 5,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA: Use gcloud to create the new project, and then deploy your application to the new project. You need to CREATE a new project. You cant copy an app using gcloud."
      },
      {
        "index": 6,
        "text": "Anonymous: kdvn 9Â months ago\nSelected Answer: A\nThe correct answer is: A. Use gcloud to create the new project, and then deploy your application to the new project.\nExplanation:\nTo move from a development environment to a production environment in Google App Engine, you canâ€™t directly copy deployments between projects."
      },
      {
        "index": 7,
        "text": "Anonymous: Trundul 1Â year, 1Â month ago\nSelected Answer: C\nthe correct answer is C"
      },
      {
        "index": 8,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: A\nThe correct answer is A.\nThis is the simplest and most straightforward way to create a new production environment. It is also the most efficient way, because it does not require you to copy the deployed application to the new project.\nThe other options are not as good:\n* Option B is not as good, because it requires you to copy the deployed application to the new project. This can be time-consuming and error-prone.\n* Option C is not as good, because it requires you to create a Deployment Manager configuration file. This can be complex and time-consuming.\n* Option D is not as good, because it does not allow you to create a new project."
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\noption A seems more correct as in B copy command don't exist"
      },
      {
        "index": 10,
        "text": "Anonymous: Bb_master 1Â year, 9Â months ago\nSelected Answer: C\nAs this is test on cloud infra then definitely the question is about creating instance with all the configuration which was present in the development environment. I think C is correct because we can copy the existing configuration from deployment manager to the new project."
      }
    ]
  },
  {
    "id": 115,
    "source": "examtopics",
    "question": "You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.",
      "B": "Add the auditors group to two new custom IAM roles.",
      "C": "Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.",
      "D": "Add the auditor user accounts to two new custom IAM roles."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\nCorrect is A.\nAs per google best practices it is recommended to use predefined roles and create groups to control access to multiple users with same responsibility droogie 5Â years, 6Â months ago\nYou assume Auditors Group = External Auditors only. Auditors Group may contain both Internal and External Auditors. robor97 5Â years, 1Â month ago\nThe question literally says - External Auditors adeice 4Â years, 10Â months ago\nI can create External group and Internal group Auditors"
      },
      {
        "index": 2,
        "text": "Anonymous: JavierCorrea Highly Voted 5Â years, 5Â months ago\nCorrect answer is A as per:\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors ArtistS 2Â years, 3Â months ago\nvery useful\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application"
      },
      {
        "index": 3,
        "text": "Anonymous: Urticas Most Recent 1Â day, 23Â hours ago\nSelected Answer: A\nA follows Googleâ€™s recommended approach: groups + predefined roles + least privilege"
      },
      {
        "index": 4,
        "text": "Anonymous: svij87 1Â month ago\nSelected Answer: A\nA is the answer"
      },
      {
        "index": 5,
        "text": "Anonymous: tabnaz 10Â months, 3Â weeks ago\nSelected Answer: A\nCorrect is A."
      },
      {
        "index": 6,
        "text": "Anonymous: Cloudmoh 11Â months, 1Â week ago\nSelected Answer: A\nBased on best practices, A group should be created, and both auditors should be added and predefined 'logging.viewer' and 'bigQuery.dataViewer roles will be granted."
      },
      {
        "index": 7,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: A\nI will go A"
      },
      {
        "index": 8,
        "text": "Anonymous: Seleth 1Â year, 5Â months ago\nSelected Answer: A\nThe organization creates a Google group for these external auditors and adds the current auditor to the group. This group is monitored and is typically granted access to the dashboard application.\nhttps://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors"
      },
      {
        "index": 9,
        "text": "Anonymous: garg.vnay 1Â year, 5Â months ago\nCorrect Answer is A. you should add a role to the group of users instead of adding particular users in IAM"
      },
      {
        "index": 10,
        "text": "Anonymous: andreiboaghe95 1Â year, 7Â months ago\nSelected Answer: A\ncorrect answer is A"
      }
    ]
  },
  {
    "id": 116,
    "source": "examtopics",
    "question": "You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow\nGoogle-recommended practices. What should you do?",
    "options": {
      "A": "Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.",
      "B": "Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.",
      "C": "Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.",
      "D": "Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\nAs per as the least privileage recommended by google, C is the correct Option, A is incorrect because the scope doesnt exist. B incorrect because it will give him full of control johnconnor 3Â years, 6Â months ago\nCheck here, it is A-> https://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/authentication Bedmed 3Â years ago\nIn the Document, it includes https://www.googleapis.com/auth/devstorage.read_write scope CVGCP 2Â years, 7Â months ago\nThere is no scope called write-only, as per the reference document. karim1321 2Â years, 7Â months ago\nIn the Document, 'write -only' does not exist. Just read-only robor97 5Â years, 1Â month ago\nThe scope does exist - https://download.huihoo.com/google/gdgdevkit/DVD1/developers.google.com/compute/docs/api/how-tos/authorization.html gielda211 3Â years, 9Â months ago\nit doesn't exist. show us this on official google website peter77 4Â years, 4Â months ago\nNo it doesn't. You have read-only, read-write, full-control and others... but \"write-only\" is not a thing.\nhttps://cloud.google.com/storage/docs/authentication"
      },
      {
        "index": 2,
        "text": "Anonymous: XRiddlerX Highly Voted 5Â years, 6Â months ago\nIn reviewing this, it looks to be a multiple answer question. According to Best Practices in this Google Doc (https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices) you grant the instance the scope and the permissions are determined by the IAM roles of the service account. In this case, you would grant the instance the scope and the role (storage.objectCreator) to the service account.\nAns B and C\nRole from GCP Console:\nID = roles/storage.objectCreator\nRole launch stage = General Availability\nDescription = Access to create objects in GCS.\n3 assigned permissions\nresourcemanager.projects.get\nresourcemanager.projects.list\nstorage.objects.create nickyshil 3Â years, 5Â months ago\nThere are many access scopes available to choose from, but a best practice is to set the cloud-platform access scope, which is an OAuth scope for most Google Cloud services, and then control the service account's access by granting it IAM roles..you have an app that reads and writes files on Cloud Storage, it must first authenticate to the Cloud Storage API. You can create an instance with the cloud-platform scope and attach a service account to the instance\nhttps://cloud.google.com/compute/docs/access/service-accounts ryumada 3Â years, 5Â months ago\nReading the second point of the best practice. You should grant your VM the https://www.googleapis.com/auth/cloud-platform scope to allow access to most of Google Cloud APIs.\nSo, that the IAM permissions are completely determined by the IAM roles you granted to the service account.\nThe conclusion is you should not mess up with the VM scopes to grant access to Google Services, instead you should grant the access via IAM roles of the service account you attached to the VM.\nhttps://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: C\nC is correct answer as we don't want user to have admin access only creator access so that they can't perform admin action on storage."
      },
      {
        "index": 4,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: C\nC. As you want to use predefined roles. and Admin give more permissions than asked for. ie just need to write permissions so storage.objectCreator only is needed."
      },
      {
        "index": 5,
        "text": "Anonymous: Hanu17 1Â year ago\nSelected Answer: B\nThe reason why A is not an answer.\nThe Activity log in the GCP Console is part of the Cloud Audit Logs but focuses on high-level admin activities, not specific data access or detailed operations like viewing files or adding metadata labels Hanu17 1Â year ago\nYou have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?"
      },
      {
        "index": 6,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nC because of 'storage.objectCreator'"
      },
      {
        "index": 7,
        "text": "Anonymous: andreiboaghe95 1Â year, 7Â months ago\nSelected Answer: C\nCorrect answer is C"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nCorrect answer is D"
      },
      {
        "index": 9,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: C\nstorage.objectCreator contains sufficient privileges to do the job & so admin is not required"
      },
      {
        "index": 10,
        "text": "Anonymous: YourCloudGuru 2Â years, 3Â months ago\nSelected Answer: C\nThe correct answer is C.\nThe other options are not accurate and go against the principle of giving least required access.\nA is incorrect as there is no role as write_only\nB is not a good option as it gives full control of google cloud services where as we are looking for write data into a particular cloud storage bucket\nD. is not a good option as it gives full control over objects\nSources:\nhttps://cloud.google.com/storage/docs/authentication\nhttps://cloud.google.com/storage/docs/access-control/iam-roles"
      }
    ]
  },
  {
    "id": 117,
    "source": "examtopics",
    "question": "You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?",
    "options": {
      "A": "Using the GCP Console, filter the Activity log to view the information.",
      "B": "Using the GCP Console, filter the Stackdriver log to view the information.",
      "C": "View the bucket in the Storage section of the GCP Console.",
      "D": "Create a trace in Stackdriver to view the information."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: iamgcp Highly Voted 5Â years, 8Â months ago\nA is correct. As mentioned in the question, data access logging is enabled. I tried to download a file from a bucket and was able to view this information in Activity tab in console RegisFTM 4Â years, 1Â month ago\nI did all the configuration enabling data access logging but I still not able to see the logs when uploading or downloading a file. Does someone here has done it with a different result? ryumada 3Â years, 5Â months ago\nI agree with liyux21 and vito9630. In this reference link below says:\nIn the Activity page, where the identity performing logged actions is redacted from the audit log entry, User (anonymized) is displayed.\nBeacause of this, I think you can't verify the addition of metadata labels through Activity Logs.\nhttps://cloud.google.com/logging/docs/audit#view-activity MEHDIGRB 3Â years, 3Â months ago\nactivity log is deprecated:\nhttps://cloud.google.com/compute/docs/logging/activity-logs barathgdkrish 3Â years, 1Â month ago\nYou need to see here, https://cloud.google.com/compute/docs/logging/audit-logging. Admin activity audit logs. Rog_4444 2Â years, 10Â months ago\nYes, it is deprecated. However, it became the audit log which is exactly what this question is referring to. Option A is correct in my opinion. vito9630 5Â years, 7Â months ago\ndata access logging don't provide information about addition of metada, so B is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: eliteone11 Highly Voted 5Â years, 1Â month ago\nAnswer is A. Activity log does indeed show information about metadata.\nI agree with Eshkrkrkr based on https://cloud.google.com/storage/docs/audit-logs Admin Activity logs: Entries for operations that modify the configuration or metadata of a project, bucket, or object. injarapu 3Â years, 10Â months ago\n'Admin activity logs' capture metadata modification, but its different from 'Data Access logging', right ?"
      },
      {
        "index": 3,
        "text": "Anonymous: Urticas Most Recent 1Â day, 23Â hours ago\nSelected Answer: B\nCloud Logging (formerly Stackdriver Logging) is the single centralized place where Cloud Audit Logsâ€”including Data Access logsâ€”are stored and queried."
      },
      {
        "index": 4,
        "text": "Anonymous: Vismaya 2Â weeks, 6Â days ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: SKSINDIAN 3Â weeks, 1Â day ago\nSelected Answer: B\n\"Stackdriver\" is the legacy name for what is now called Google Cloud Observability (which includes Cloud Logging and Cloud Monitoring). Stackdriver contains the both the activity and data logs."
      },
      {
        "index": 6,
        "text": "Anonymous: Naveen00001 4Â months, 1Â week ago\nSelected Answer: A\nB & D not going to br the answer beacuse the stackdriver does not exist.\nC you can not see the metadata & logs .\nso , remaining A is the correct ANswer."
      },
      {
        "index": 7,
        "text": "Anonymous: vdh_06 8Â months, 3Â weeks ago\nSelected Answer: A\nThe correct answer is A. The Activity Log is the place where Data Access Logs (including file views) and Admin Activity Logs (like metadata label changes) are stored.\nExplanation:\nThe Activity Log captures both Admin Activity (e.g., adding metadata labels) and Data Access Logs (e.g., file viewing activity in Cloud Storage).\nThe Activity Log is where you find Cloud Storage Data Access Logs, which is exactly what you need to track who viewed files and modified labels in your buckets.\nYou can filter by user, resource (bucket), and activity type to find both the metadata changes and file viewing activities.\nConclusion: Correct Answer â€” The Activity Log is the correct place for both administrative actions and data access, so you can query the detailed information directly."
      },
      {
        "index": 8,
        "text": "Anonymous: jeyam1990 11Â months, 2Â weeks ago\nSelected Answer: B\nThe correct answer is:\nB. Using the GCP Console, filter the Stackdriver log to view the information.\nExplanation:\nThe Activity log in the GCP Console is limited to Admin Activity Logs, which show administrative actions like adding metadata labels. It does not include Data Access Logs, which are required to verify file viewing activity.\nThe Stackdriver log (now referred to as Cloud Logging) provides access to both Admin Activity Logs and Data Access Logs, allowing you to view both types of actions (adding metadata labels and viewing files). By filtering the logs in Cloud Logging, you can get the required information for the user efficiently.\nAnswers provided by ChatGPT 1826c27 11Â months, 1Â week ago\nmr chatgtp - stackdriver is no longer in GCP"
      },
      {
        "index": 9,
        "text": "Anonymous: speksy 1Â year ago\nSelected Answer: B\nStackdriver Logging (now called Google Cloud Logging) captures detailed logs for activities within Google Cloud, including bucket metadata changes and file access activities for Cloud Storage bucket"
      },
      {
        "index": 10,
        "text": "Anonymous: Hanu17 1Â year ago\nSelected Answer: B\nThe reason why A is not an answer. The Activity log in the GCP Console is part of the Cloud Audit Logs but focuses on high-level admin activities, not specific data access or detailed operations like viewing files or adding metadata labels"
      }
    ]
  },
  {
    "id": 118,
    "source": "examtopics",
    "question": "You are the project owner of a GCP project and want to delegate control to colleagues to manage buckets and files in Cloud Storage. You want to follow Google- recommended practices. Which IAM roles should you grant your colleagues?",
    "options": {
      "A": "Project Editor",
      "B": "Storage Admin",
      "C": "Storage Object Admin",
      "D": "Storage Object Creator"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 4Â months ago\nCorrect Answer is (B):\nStorage Admin (roles/storage.admin) Grants full control of buckets and objects.\nWhen applied to an individual bucket, control applies only to the specified bucket and objects within the bucket.\nfirebase.projects.get\nresourcemanager.projects.get\nresourcemanager.projects.list\nstorage.buckets.*\nstorage.objects.* iambatmanadarkknight 3Â years, 3Â months ago\nwhy not storage object admin? TenshiD 3Â years, 2Â months ago\nBecause the objet admin don't have control over buckets and you need it Raz0r 3Â years ago\nExactly, you want to give someone right to edit storages not just objects. Google does this kind of answers to confuse us. dang1986 2Â years, 11Â months ago\nQuestion states \"Buckets and Objects\""
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 4Â years, 9Â months ago\nB is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: B\nAdmin role for full access of storage."
      },
      {
        "index": 4,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: B\nThe correct answer is B\nThis role allows users to create, manage, and delete buckets and files in Cloud Storage. It also allows users to set permissions on buckets and files.\nThe other options are not as good:\nA gives users too much power, as it allows them to manage all resources in a project, including Cloud Storage buckets and files\nC gives users too much power, as it allows them to manage all objects in a bucket, including the permissions on those objects\nD does not give users enough power, as it does not allow them to manage buckets or set permissions on buckets and objects\nSteps to grant Storage Admin IAM role:\n1 Go to the Google Cloud Console\n2 Click on the IAM & Admin menu\n3 Click on the Roles tab\n4 Click on the Storage Admin role\n5 Click on the Add members button\n6 Type the email addresses of your colleagues in the Members field\n7 Click on the Add button"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nB is more correct, as it give you the both access"
      },
      {
        "index": 6,
        "text": "Anonymous: Neha_Pallavi 1Â year, 5Â months ago\nB.\nStorage Admin (roles/storage.admin) - Grants full control of buckets and objects.\nhttps://cloud.google.com/storage/docs/access-control/iam-roles"
      },
      {
        "index": 7,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: B\nCorrect option B"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: B\nAnswer B, \"Storage Admin,\" is the correct answer because it grants permissions to manage Cloud Storage resources at the project level, including creating and deleting buckets, changing bucket settings, and assigning permissions to buckets and their contents. This role also includes the permissions of the \"Storage Object Admin\" and \"Storage Object Creator\" roles, which allow managing objects and uploading new ones.\nAnswer A, \"Project Editor,\" is a higher-level role that includes permissions to manage not only Cloud Storage but also other GCP services in the project. Granting this role may not be appropriate if the colleagues only need to manage Cloud Storage resources.\nAnswers C and D may not be sufficient if the colleagues need to create or delete buckets or change their settings."
      },
      {
        "index": 9,
        "text": "Anonymous: kkozlow2 2Â years, 1Â month ago\nSelected Answer: B\nStorage Admin (roles/storage.admin)\nGrants full control of buckets and objects.\nWhen applied to an individual bucket, control applies only to the specified bucket and objects within the bucket.\nWhile\nStorage Object Admin (roles/storage.objectAdmin)\nGrants full control over objects, including listing, creating, viewing, and deleting objects."
      },
      {
        "index": 10,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nB. Storage Admin"
      }
    ]
  },
  {
    "id": 119,
    "source": "examtopics",
    "question": "You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do?",
    "options": {
      "A": "Create a signed URL with a four-hour expiration and share the URL with the company.",
      "B": "Set object access to 'public' and use object lifecycle management to remove the object after four hours.",
      "C": "Configure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.",
      "D": "Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JJ_ME Highly Voted 4Â years, 9Â months ago\nA.\nSigned URLs are used to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account.\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 3Â months ago\nA is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: A\nA is correct answer"
      },
      {
        "index": 4,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA. Signed URL. this creates credentials to access the file for a time period. You are then completely responsible to distribute the URL to whom needs its. Not advisable for most usecases. Howerver it is the correct answer to this question. This will fail most corporate security assessments."
      },
      {
        "index": 5,
        "text": "Anonymous: Cloudmoh 11Â months, 1Â week ago\nSelected Answer: A\nBased on Google best practice and Doc, Signed URLs give time-limited access to a specific Cloud Storage resource. Anyone in possession of the signed URL can use it while it's active, regardless of whether they have a Google account or not."
      },
      {
        "index": 6,
        "text": "Anonymous: SteveXs 1Â year ago\nSelected Answer: A\nSigned URLs give time-limited access to a specific Cloud Storage resource. Anyone in possession of the signed URL can use it while it's active, regardless of whether they have a Google account."
      },
      {
        "index": 7,
        "text": "Anonymous: JB28 1Â year, 7Â months ago\nOption A"
      },
      {
        "index": 8,
        "text": "Anonymous: nmnm22 1Â year, 8Â months ago\nsigned URL for timed access"
      },
      {
        "index": 9,
        "text": "Anonymous: YourCloudGuru 1Â year, 9Â months ago\nSelected Answer: A\nThe correct answer is A\nTo do this, follow these steps:\n1 Go to the Google Cloud Console\n2 Click on the Storage menu\n3 Click on the Browser tab\n4 Click on the name of the bucket containing the object that you want to share\n5 Click on the name of the object that you want to share\n6 Click on the Create signed URL button\n7 In the Expires field, enter 4\n8 Click on the Create button\nThe company will be able to access the object using the signed URL for four hours. After four hours, the signed URL will expire and the company will no longer be able to access the object.\nThe other options are not as secure or efficient:\nB not as secure because it makes the object accessible to anyone who has the URL of the object\nC requires you to configure the storage bucket as a static website and to delete the object after four hours\nD requires you to create a new Cloud Storage bucket and to copy the object to that bucket\nhttps://cloud.google.com/storage/docs/access-control/signed-urls"
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nA seems more legit"
      }
    ]
  },
  {
    "id": 120,
    "source": "examtopics",
    "question": "You are creating a Google Kubernetes Engine (GKE) cluster with a cluster autoscaler feature enabled. You need to make sure that each node of the cluster will run a monitoring pod that sends container metrics to a third-party monitoring solution. What should you do?",
    "options": {
      "A": "Deploy the monitoring pod in a StatefulSet object.",
      "B": "Deploy the monitoring pod in a DaemonSet object.",
      "C": "Reference the monitoring pod in a Deployment object.",
      "D": "Reference the monitoring pod in a cluster initializer at the GKE cluster creation time."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JackGlemins Highly Voted 4Â years, 10Â months ago\nB is right: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\nSome typical uses of a DaemonSet are:\nrunning a cluster storage daemon on every node\nrunning a logs collection daemon on every node\nrunning a node monitoring daemon on every node"
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 9Â months ago\nB is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: B\nDefinately Daemon set. forces every node to setup an object. Stateful is about persistent file storage, referencing object will not create it at every node."
      },
      {
        "index": 4,
        "text": "Anonymous: Cloudmoh 11Â months, 1Â week ago\nSelected Answer: B\nin K8S, Deploying and monitoring the pod in a DaemonSet object."
      },
      {
        "index": 5,
        "text": "Anonymous: ACEqa 1Â year, 4Â months ago\nThe correct answer is B"
      },
      {
        "index": 6,
        "text": "Anonymous: JB28 2Â years, 1Â month ago\nOption B"
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 2Â years, 3Â months ago\nSelected Answer: B\nThe correct answer is B\nDaemonSets ensure that one running instance of the pod is scheduled on every node in the cluster. This means that even if the cluster autoscaler scales the cluster up or down, the monitoring pod will continue to run on each node.\nThe other options are not as good:\nA This is not necessary for the monitoring pod, and it can make it more difficult to scale the cluster\nC This is not necessary for the monitoring pod, and it can make it more difficult to scale the cluster\nD This is not the best way to deploy the monitoring pod, because it makes it difficult to update the monitoring pod and it does not ensure that the monitoring pod is running on every node in the cluster.\nhttps://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nB seems more correcnt"
      },
      {
        "index": 9,
        "text": "Anonymous: senatori 2Â years, 10Â months ago\nSelected Answer: B\ndaemonset- makes sure that a fixed number of pods is running in every moment in every node of the cluster"
      },
      {
        "index": 10,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: B\nAnswer B, Deploy the monitoring pod in a DaemonSet object, is the correct answer.\nA DaemonSet ensures that all (or some) nodes in a cluster run a copy of a specific Pod. By deploying the monitoring pod in a DaemonSet object, a copy of the pod will run on each node of the cluster. This ensures that the metrics for all containers running on each node will be sent to the third-party monitoring solution.\nA. StatefulSet is used for stateful applications that require unique network identifiers, stable storage, and ordered deployment and scaling.\nC. A Deployment object is used to manage a set of replica Pods in a declarative way.\nD. Cluster initializers are deprecated and no longer recommended for use in Kubernetes."
      }
    ]
  },
  {
    "id": 121,
    "source": "examtopics",
    "question": "Your company has multiple projects linked to a single billing account in Google Cloud. You need to visualize the costs with specific metrics that should be dynamically calculated based on company-specific criteria. You want to automate the process. What should you do?",
    "options": {
      "A": "In the Google Cloud console, visualize the costs related to the projects in the Reports section.",
      "B": "In the Google Cloud console, visualize the costs related to the projects in the Cost breakdown section.",
      "C": "In the Google Cloud console, use the export functionality of the Cost table. Create a Looker Studio dashboard on top of the CSV export.",
      "D": "Configure Cloud Billing data export to BigQuery for the billing account. Create a Looker Studio dashboard on top of the BigQuery export."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 1Â year, 10Â months ago\nSelected Answer: D\nCloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage, cost estimates, and pricing data) automatically throughout the day to a BigQuery dataset that you specify. Then you can access your Cloud Billing data from BigQuery for detailed analysis, or use a tool like Looker Studio to visualize your data. scanner2 1Â year, 10Â months ago\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      },
      {
        "index": 2,
        "text": "Anonymous: MrJkr Highly Voted 2Â years ago\nSelected Answer: D\nOption D closely aligns with the requirements mentioned in the question.\nBy configuring Cloud Billing data export to BigQuery, you can automate the process of exporting billing data to a BigQuery dataset. You can then use Looker Studio, a data visualization and exploration platform, to create a dashboard on top of the BigQuery export. This allows you to visualize costs with specific metrics that can be dynamically calculated based on company-specific criteria."
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: D\nThe key here is that the cost visualization goes beyond the standard metrics, implying a need to manipulate and aggregate the raw data which makes D the best solution"
      },
      {
        "index": 4,
        "text": "Anonymous: mzchelino 1Â year, 10Â months ago\nSelected Answer: D\nD!\nKeys here are \"dynamically calculated\" and \"automate\"."
      },
      {
        "index": 5,
        "text": "Anonymous: Shubha1 1Â year, 11Â months ago\nwhy not A? can someone pls explain Linhtinh603 1Â year, 7Â months ago\nAnswer A is using default report with basic charts and trends, dose not customize as company-specific criteria. Therefore, answer D is correct."
      },
      {
        "index": 6,
        "text": "Anonymous: Husni_adam 2Â years ago\nSelected Answer: D\nD is the right answer because you can automate process with export billing data to bigquery and then using the bigquery export to create the Looker studio dashboard Husni_adam 2Â years ago\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery"
      }
    ]
  },
  {
    "id": 122,
    "source": "examtopics",
    "question": "You have an application that runs on Compute Engine VM instances in a custom Virtual Private Cloud (VPC). Your companyâ€™s security policies only allow the use of internal IP addresses on VM instances and do not let VM instances connect to the internet. You need to ensure that the application can access a file hosted in a Cloud Storage bucket within your project. What should you do?",
    "options": {
      "A": "Enable Private Service Access on the Cloud Storage Bucket.",
      "B": "Add storage.googleapis.com to the list of restricted services in a VPC Service Controls perimeter and add your project to the list of protected projects.",
      "C": "Enable Private Google Access on the subnet within the custom VPC.",
      "D": "Deploy a Cloud NAT instance and route the traffic to the dedicated IP address of the Cloud Storage bucket."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 1Â year, 4Â months ago\nSelected Answer: C\nCloud Storage is not a supported service for Private Service Access. Hence, A cannot be the answer.\nhttps://cloud.google.com/vpc/docs/private-services-access#private-services-supported-services\nVM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. If you disable Private Google Access, the VM instances can no longer reach Google APIs and services; they can only send traffic within the VPC network.\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "index": 2,
        "text": "Anonymous: juliorevk Highly Voted 1Â year, 5Â months ago\nSelected Answer: C\nPrivate Google Access lets you connect VM instances to GCP services without external IP addresses and only internal. A is wrong because even though Private Services Access lets you also access GCP and other services through internal IPs, it also allows the VMs to have external IPs.\nhttps://cloud.google.com/vpc/docs/private-google-access"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nC is the correct Answer as Private Google Access allows you to the connect on the internal networks, A is incorrect becuause Cloud Storage bucket dont have such services to connect to Private Acesss`"
      },
      {
        "index": 4,
        "text": "Anonymous: FJ82 1Â year, 6Â months ago\nSelected Answer: C\nPrivate Google Access is a VPC feature"
      },
      {
        "index": 5,
        "text": "Anonymous: TomFoot 1Â year, 6Â months ago\nSelected Answer: C\nC allows access to Google services & API's"
      },
      {
        "index": 6,
        "text": "Anonymous: georgesouzafarias 1Â year, 7Â months ago\nSelected Answer: C\nRight answer."
      }
    ]
  },
  {
    "id": 123,
    "source": "examtopics",
    "question": "Your company completed the acquisition of a startup and is now merging the IT systems of both companies. The startup had a production Google Cloud project in their organization. You need to move this project into your organization and ensure that the project is billed to your organization. You want to accomplish this task with minimal effort. What should you do?",
    "options": {
      "A": "Use the projects.move method to move the project to your organization. Update the billing account of the project to that of your organization.",
      "B": "Ensure that you have an Organization Administrator Identity and Access Management (IAM) role assigned to you in both organizations. Navigate to the Resource Manager in the startupâ€™s Google Cloud organization, and drag the project to your company's organization.",
      "C": "Create a Private Catalog for the Google Cloud Marketplace, and upload the resources of the startup's production project to the Catalog. Share the Catalog with your organization, and deploy the resources in your companyâ€™s project.",
      "D": "Create an infrastructure-as-code template for all resources in the project by using Terraform, and deploy that template to a new project in your organization. Delete the project from the startupâ€™s Google Cloud organization."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MrJkr Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nOption A is correct as it suggests using the \"projects.move\" method provided by Google Cloud to move the project from the startup's organization to your organization. This method allows you to transfer the ownership and control of a project to another organization. By moving the project, you can ensure that it is under your organization's management.\nWhile the other options contain elements that may be relevant in certain scenarios, they do not directly address the requirement of moving the project and ensuring billing to your organization."
      },
      {
        "index": 2,
        "text": "Anonymous: Vismaya Most Recent 1Â week, 4Â days ago\nSelected Answer: A\nOption A"
      },
      {
        "index": 3,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nI vote for D. Why is that?\nOption A says project.move method - This is complicated, it requires you to have specific IAM permissions set up correctly in both organizations (like Project Mover role), and you must also ensure that any dependencies or policies are managed properly.\nThe new command is much easier\ngcloud beta projects move PROJECT_ID --organization=ORGANIZATION_ID or --folder=FOLDER_ID\nbut option A doesn't say to use\ngcloud beta project move\nSo for me, the answer is D - IaC/Terraform can accomplish this task with minimal effort (compared to the \"projects.move\" method)"
      },
      {
        "index": 4,
        "text": "Anonymous: 0verK0alafied 1Â year, 2Â months ago\nSelected Answer: A\ngcloud beta project move is the command with the flag of --organization= or --folder= with the IDs."
      },
      {
        "index": 5,
        "text": "Anonymous: VijKall 1Â year, 2Â months ago\nSelected Answer: A\nOption A.\nHere is the command - gcloud beta projects move"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is correct Answer you can use the project move method to move the project to your organization"
      },
      {
        "index": 7,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nHere, A is the only choice because neither B nor C nor D be the answer.\nYou can move a project within an organization. But you need to migrate a project across organizations.\nhttps://cloud.google.com/resource-manager/docs/project-migration\nhttps://medium.com/google-cloud/migrating-a-project-from-one-organization-to-another-gcp-4b37a86dd9e6"
      },
      {
        "index": 8,
        "text": "Anonymous: georgesouzafarias 1Â year, 7Â months ago\nSelected Answer: A\nDon't overthink it.\nhttps://cloud.google.com/resource-manager/docs/project-migration-checklist"
      }
    ]
  },
  {
    "id": 124,
    "source": "examtopics",
    "question": "All development (dev) teams in your organization are located in the United States. Each dev team has its own Google Cloud project. You want to restrict access so that each dev team can only create cloud resources in the United States (US). What should you do?",
    "options": {
      "A": "Create a folder to contain all the dev projects. Create an organization policy to limit resources in US locations.",
      "B": "Create an organization to contain all the dev projects. Create an Identity and Access Management (IAM) policy to limit the resources in US regions.",
      "C": "Create an Identity and Access Management (IAM) policy to restrict the resources locations in the US. Apply the policy to all dev projects.",
      "D": "Create an Identity and Access Management (IAM) policy to restrict the resources locations in all dev projects. Apply the policy to all dev roles."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Vovtchick Highly Voted 2Â years, 2Â months ago\nAnswer A\nAn organization policy configures a single constraint that restricts one or more Google Cloud services. The organization policy is set on an organization, folder, or project resource to enforce the constraint on that resource and any child resources.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/overview"
      },
      {
        "index": 2,
        "text": "Anonymous: ezzar Highly Voted 2Â years, 2Â months ago\nSelected Answer: A\nit's A : IAM is about WHO does WHAT\nOrganization policy is only about WHAT"
      },
      {
        "index": 3,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: A\nB. Create an organization and IAM policy: IAM policies control permissions but do not enforce resource location restrictions. Organization policies are required for location constraints.\nC. IAM policy to restrict resources in US regions: IAM policies cannot enforce location-based restrictions.\nD. IAM policy applied to dev roles: Similar to C, IAM policies control access permissions, not resource locations. so the answer is A"
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nYou can limit the physical location of a new resource with the Organization Policy Service resource locations constraint.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/defining-locations"
      },
      {
        "index": 5,
        "text": "Anonymous: joao_01 2Â years, 3Â months ago\nSelected Answer: A\nIts A.\n\"Organization Policy\" does not indicate that it will be ONLY applied to a organization, it can be applied to any resource within a organization to restrict and add conditions. This policy focus on WHAT and not WHO (IAM). So, since in this case we want to restrict to VMs in US, its clearly the option A.\nLink: https://cloud.google.com/resource-manager/docs/organization-policy/overview\n\"Identity and Access Management focuses on who, and lets the administrator authorize who can take action on specific resources based on permissions.\"\n\"Organization Policy focuses on what, and lets the administrator set restrictions on specific resources to determine how they can be configured.\""
      },
      {
        "index": 6,
        "text": "Anonymous: ExamsFR 2Â years, 4Â months ago\nSelected Answer: A\nhttps://cloud.google.com/resource-manager/docs/organization-policy/defining-locations?hl=fr#gcloud"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer as you can just add the projects to the same folder and just apply the Organization policy"
      },
      {
        "index": 8,
        "text": "Anonymous: DocOck 2Â years, 4Â months ago\nThe organization policy is set on an organization, folder, or project resource to enforce the constraint on that resource and any child resources. An organization policy contains one or more rules that specify how, and whether, to enforce the constraint.\nA"
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: A\nRestricting resources creation in specific locations can be achieved by Organization policy. The Organization Policy Service gives you centralized and programmatic control over your organization's cloud resources.\nYou can use \"Google Cloud Platform - Resource Location Restriction\" Organization policy constraint for the solution asked in the question.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/overview\nhttps://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints scanner2 2Â years, 4Â months ago\nOption A is mentioning to create a folder that contains all dev projects."
      },
      {
        "index": 10,
        "text": "Anonymous: LONGBOW_RA 2Â years, 4Â months ago\nSelected Answer: C\nhttps://cloud.google.com/resource-manager/docs/access-control-folders\nA is wrong, The documentation says it needs a folder admin role to apply folder's IAM policy to limit resources access. A said organization level policies. LONGBOW_RA 2Â years, 4Â months ago\nOK, I changed my mind. A folder contains projects. Then mange the folder's IAM policies maybe can be considered as \"organization level\" .\nWTF, I feel I am getting an English certification."
      }
    ]
  },
  {
    "id": 125,
    "source": "examtopics",
    "question": "You are configuring Cloud DNS. You want to create DNS records to point home.mydomain.com, mydomain.com, and www.mydomain.com to the IP address of your Google Cloud load balancer. What should you do?",
    "options": {
      "A": "Create one CNAME record to point mydomain.com to the load balancer, and create two A records to point WWW and HOME to mydomain.com respectively.",
      "B": "Create one CNAME record to point mydomain.com to the load balancer, and create two AAAA records to point WWW and HOME to mydomain.com respectively.",
      "C": "Create one A record to point mydomain.com to the load balancer, and create two CNAME records to point WWW and HOME to mydomain.com respectively.",
      "D": "Create one A record to point mydomain.com to the load balancer, and create two NS records to point WWW and HOME to mydomain.com respectively."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: C\nRecord name | Type | Value\nmydomain.com | A | load balancer IP\nwww.mydomain.com | CNAME | mydomain.com\nhome.mydomain.com | CNAME | mydomain.com\nhttps://cloud.google.com/dns/docs/records-overview#supported_dns_record_types"
      },
      {
        "index": 2,
        "text": "Anonymous: MrJkr Highly Voted 2Â years, 6Â months ago\nSelected Answer: C\nOption A suggests creating one CNAME record to point mydomain.com to the load balancer, which is incorrect because CNAME records cannot coexist with other record types on the same domain/subdomain. In this case, you need to use an A record instead.\nOption B suggests creating two AAAA records, which are used for IPv6 addresses. Unless you specifically have an IPv6 address for your load balancer, using AAAA records would not be appropriate.\nOption D suggests creating two NS records, which are used for specifying the authoritative name servers for a domain. NS records are not used to point subdomains to IP addresses or load balancers.\nTherefore, option C is the correct answer, as it correctly suggests creating one A record to point mydomain.com to the load balancer, and two CNAME records to point WWW and HOME to mydomain.com respectively."
      },
      {
        "index": 3,
        "text": "Anonymous: yehia2221 Most Recent 1Â year, 5Â months ago\nSelected Answer: C\nSimply, you can't create cname for thr root domain (mydomain.com). When you refe to other GCP services, it is a best practice to use cname/alais for the other records"
      },
      {
        "index": 4,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: C\nOption C strikes the right balance between direct mapping with an A record for the root domain and using CNAME aliases for subdomains. This aligns with DNS best practices and simplifies management."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC is the correct answer, as for this you need to create One A record and two CNAME records"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: C\nA record is always for load balancer IP, then CNAME"
      },
      {
        "index": 7,
        "text": "Anonymous: juliorevk 2Â years, 5Â months ago\nSelected Answer: C\nC because the A record which points a domain name to an IPv4 address is used for the main domain (mydomain.com), the other two for www and home are aliases (CNAME) that can be pointed to the main domain of mydomain.com."
      },
      {
        "index": 8,
        "text": "Anonymous: georgesouzafarias 2Â years, 7Â months ago\nSelected Answer: C\nYou can only associate A(IP) record to a domain.\nhttps://cloud.google.com/dns/docs/set-up-dns-records-domain-name#create_a_record_to_point_the_domain_to_an_external_ip_address"
      }
    ]
  },
  {
    "id": 126,
    "source": "examtopics",
    "question": "You have two subnets (subnet-a and subnet-b) in the default VPC. Your database servers are running in subnet-a. Your application servers and web servers are running in subnet-b. You want to configure a firewall rule that only allows database traffic from the application servers to the database servers. What should you do?",
    "options": {
      "A": "â€¢ Create service accounts sa-app and sa-db.",
      "B": "â€¢ Create network tags app-server and db-server.",
      "C": "â€¢ Create a service account sa-app and a network tag db-server.",
      "D": "â€¢ Create a network tag app-server and service account sa-db."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\nBoth service accounts and network tags can be used for creating a Cloud Firewall rule. The prime word is \"to allow network traffic from app server to database server\" which is achievable by inbound/ingress rule and not egress rule.\nhttps://cloud.google.com/firewall/docs/firewalls#rule_assignment"
      },
      {
        "index": 2,
        "text": "Anonymous: Captain1212 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\nAnswer A is correct as question demands traffic to application to database which can be only be achieved by the ingreess rule"
      },
      {
        "index": 3,
        "text": "Anonymous: BRDA Most Recent 1Â month, 1Â week ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 4,
        "text": "Anonymous: peterwheat 7Â months, 3Â weeks ago\nSelected Answer: A\nwhy not B? Due to you are not able to specify network tags as source filter for egress rules."
      },
      {
        "index": 5,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: A\nA IS CORRECT"
      },
      {
        "index": 6,
        "text": "Anonymous: Ciupaz 1Â year, 4Â months ago\nSelected Answer: B\nBy specifying the source and target tags in the firewall rule, you can ensure that only traffic from instances with the app-server tag can reach instances with the db-server tag."
      },
      {
        "index": 7,
        "text": "Anonymous: MariaDoss 2Â years, 1Â month ago\nSelected Answer: A\nthe answer is A"
      },
      {
        "index": 8,
        "text": "Anonymous: GP123123 2Â years, 1Â month ago\nfor \"allow\" rules, restrict them to only allow specific virtual machines by specifying the virtual machine's service account\nRefer to https://cloud.google.com/firewall/docs/firewalls GP123123 2Â years, 1Â month ago\nAnswer is A"
      },
      {
        "index": 9,
        "text": "Anonymous: carlalap 2Â years, 2Â months ago\nAnswer is B.\nNetwork tags can be simpler to manage, especially if the access control is based on broader categories (like application servers and database servers) rather than individual instances.\nIf you need fine-grained control and identity-based access, go with service accounts. If you prefer simplicity and broader categorization, network tags may be a suitable choice. carlalap 2Â years, 2Â months ago\nAlthough what was previously stated is correct, I must correct it. The correct answer would be option A. Creating an ingress firewall rule on the subnet where your database servers are located is the appropriate approach. This rule would control incoming traffic to the database servers, ensuring that only traffic from the specified application servers (identified by network tags or service accounts) is allowed."
      },
      {
        "index": 10,
        "text": "Anonymous: tatyavinchu 2Â years, 5Â months ago\nCorrect Answer is A"
      }
    ]
  },
  {
    "id": 127,
    "source": "examtopics",
    "question": "Your team wants to deploy a specific content management system (CMS) solution to Google Cloud. You need a quick and easy way to deploy and install the solution. What should you do?",
    "options": {
      "A": "Search for the CMS solution in Google Cloud Marketplace. Use gcloud CLI to deploy the solution.",
      "B": "Search for the CMS solution in Google Cloud Marketplace. Deploy the solution directly from Cloud Marketplace.",
      "C": "Search for the CMS solution in Google Cloud Marketplace. Use Terraform and the Cloud Marketplace ID to deploy the solution with the appropriate parameters.",
      "D": "Use the installation guide of the CMS provider. Perform the installation through your configuration management system."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: juliorevk Highly Voted 1Â year, 5Â months ago\nSelected Answer: B\nFastest and easiest way to deploy a solution straight from the marketplace"
      },
      {
        "index": 2,
        "text": "Anonymous: kamee15 Most Recent 1Â year ago\nSelected Answer: B\nQuick=Fast=Marketplace, so B is answer"
      },
      {
        "index": 3,
        "text": "Anonymous: VijKall 1Â year, 2Â months ago\nSelected Answer: B\nOption B, so says all and they are all correct."
      },
      {
        "index": 4,
        "text": "Anonymous: on2it 1Â year, 6Â months ago\nSelected Answer: B\nIndeed directly from Cloud Marketplace"
      },
      {
        "index": 5,
        "text": "Anonymous: _F4LLEN_ 1Â year, 6Â months ago\nSelected Answer: B\nWe can deploy it directly from Cloud Marketplace."
      },
      {
        "index": 6,
        "text": "Anonymous: TomFoot 1Â year, 6Â months ago\nSelected Answer: B\nI think B would be simple and fast."
      },
      {
        "index": 7,
        "text": "Anonymous: georgesouzafarias 1Â year, 7Â months ago\nSelected Answer: B\nKey Words: Quick and Easy"
      }
    ]
  },
  {
    "id": 128,
    "source": "examtopics",
    "question": "You are working for a startup that was officially registered as a business 6 months ago. As your customer base grows, your use of Google Cloud increases. You want to allow all engineers to create new projects without asking them for their credit card information. What should you do?",
    "options": {
      "A": "Create a Billing account, associate a payment method with it, and provide all project creators with permission to associate that billing account with their projects.",
      "B": "Grant all engineers permission to create their own billing accounts for each new project.",
      "C": "Apply for monthly invoiced billing, and have a single invoice for the project paid by the finance team.",
      "D": "Create a billing account, associate it with a monthly purchase order (PO), and send the PO to Google Cloud."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MrJkr Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nOption A is the better answer for the given scenario. It allows you to centralize billing and payment management while providing flexibility to project creators. By creating a billing account and associating a payment method with it, you establish a central source for billing and payment for all projects.\nGranting project creators permission to associate the billing account with their projects ensures that they can create projects without the need for their individual credit card information. This approach streamlines the process and avoids the hassle of collecting credit card details from each engineer.\nAdditionally, this option allows for easy monitoring and management of project costs through a single billing account, making it simpler to track expenses and allocate resources effectively."
      },
      {
        "index": 2,
        "text": "Anonymous: DevOps_ad Most Recent 7Â months, 2Â weeks ago\nSelected Answer: B\nwithout asking them for their credit card information\nOption B sounds better since consolidated billing is not asked for. Every project needs its own billing account."
      },
      {
        "index": 3,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: A\nCloud Billing accounts pay for Google Cloud projects and Google Maps Platform projects.\nYou might be eligible to switch your account type to monthly invoiced billing if your business meets certain requirements. These requirements include, but aren't limited to the following:\n- You must be registered as a business for a minimum of one year.\n- You expect to spend a minimum of $40,000 a year on Google Cloud.\n- Invoiced billing must be available in your country.\nhttps://cloud.google.com/billing/docs/how-to/invoiced-billing\nSo A is the correct answer here."
      },
      {
        "index": 4,
        "text": "Anonymous: _F4LLEN_ 1Â year, 6Â months ago\nSelected Answer: A\nA seems the best among the rest."
      },
      {
        "index": 5,
        "text": "Anonymous: gpais 1Â year, 6Â months ago\nSelected Answer: A\nIt's option A"
      },
      {
        "index": 6,
        "text": "Anonymous: georgesouzafarias 1Â year, 7Â months ago\nSelected Answer: A\nDon't overthink."
      }
    ]
  },
  {
    "id": 129,
    "source": "examtopics",
    "question": "Your continuous integration and delivery (CI/CD) server canâ€™t execute Google Cloud actions in a specific project because of permission issues. You need to validate whether the used service account has the appropriate roles in the specific project.\n\nWhat should you do?",
    "options": {
      "A": "Open the Google Cloud console, and check the Identity and Access Management (IAM) roles assigned to the service account at the project or inherited from the folder or organization levels.",
      "B": "Open the Google Cloud console, and check the organization policies.",
      "C": "Open the Google Cloud console, and run a query to determine which resources this service account can access.",
      "D": "Open the Google Cloud console, and run a query of the audit logs to find permission denied errors for this service account."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: mufuuuu Highly Voted 2Â years, 1Â month ago\nSelected Answer: A\nIts not a rocket science. its straightforward question"
      },
      {
        "index": 2,
        "text": "Anonymous: 33d6a28 Most Recent 1Â year, 2Â months ago\nSelected Answer: D\nA seems right, but I want to know why D is not good?"
      },
      {
        "index": 3,
        "text": "Anonymous: Rectifoace23 2Â years, 2Â months ago\nSelected Answer: A\nA is the answer"
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nA it's really straightforward"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct Answer"
      },
      {
        "index": 6,
        "text": "Anonymous: happydays 2Â years, 5Â months ago\nSelected Answer: A\nIt's A from GPT"
      },
      {
        "index": 7,
        "text": "Anonymous: TungTrinhSenacor 2Â years, 5Â months ago\nSelected Answer: A\nI go for A"
      },
      {
        "index": 8,
        "text": "Anonymous: ankitb4u 2Â years, 5Â months ago\nSelected Answer: C\nC should be the correct answer here TungTrinhSenacor 2Â years, 5Â months ago\nWhy not A?"
      }
    ]
  },
  {
    "id": 130,
    "source": "examtopics",
    "question": "Your team is using Linux instances on Google Cloud. You need to ensure that your team logs in to these instances in the most secure and cost efficient way. What should you do?",
    "options": {
      "A": "Attach a public IP to the instances and allow incoming connections from the internet on port 22 for SSH.",
      "B": "Use the gcloud compute ssh command with the --tunnel-through-iap flag. Allow ingress traffic from the IP range 35.235.240.0/20 on port 22.",
      "C": "Use a third party tool to provide remote access to the instances.",
      "D": "Create a bastion host with public internet access. Create the SSH tunnel to the instance through the bastion host."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Captain1212 Highly Voted 2Â years, 4Â months ago\nSelected Answer: B\nCommon sense B is the correct answer , must safer from using the third party apps or the public id addresses"
      },
      {
        "index": 2,
        "text": "Anonymous: LaCubanita Most Recent 1Â year, 3Â months ago\nAccording to Gemini: In Google Cloud Platform (GCP), Identity-Aware Proxy (IAP) is a more secure alternative to bastion hosts for accessing private resources. IAP encrypts SSH connections end-to-end, so it can't inspect the contents of the session. IAP also provides access controls to reduce the risk of unauthorized access and data breaches. https://cloud.google.com/compute/docs/connect/ssh-best-practices/network-access#use-a-bastion-host"
      },
      {
        "index": 3,
        "text": "Anonymous: Ciupaz 1Â year, 4Â months ago\nSelected Answer: B\nWhy the others are not correct?\nBastion Host: While a bastion host can provide remote access, it introduces additional complexity and potential security risks.\nThird-Party Tools: Using third-party tools may add costs and introduce dependencies."
      },
      {
        "index": 4,
        "text": "Anonymous: Rajkumar21 1Â year, 10Â months ago\nOne General Question: Most of the cases the Answer provided for each questions in Exam Topic Differs from the Answer comes as a result as part of discussion.\nJust worried, since appearing ACE exam-Should we go with Answers what the group of people says (with highest percentage opted answer)? 123kiki1626 1Â year, 8Â months ago\nGo with majority, the website might have the wrong answer but discussion and majority people mostly know the right answer."
      },
      {
        "index": 5,
        "text": "Anonymous: mufuuuu 2Â years, 1Â month ago\nSelected Answer: B\nB is correct Rahul001 2Â years, 1Â month ago\nI will be appearing for Ace in the upcoming week is this 255 questions will be enough to pass the exam tlopsm 2Â years, 1Â month ago\nPlease give us feedback when you are done Rahul001 2Â years, 1Â month ago\nYes it is enough to clear, 6-7 questions may come from outside exam topics but you can expect atleast 40 to come out of 246 questions."
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: B\nYou can use Bastion if\n\"You have a specific use case, like session recording, and you can't use IAP.\"\nhttps://cloud.google.com/compute/docs/connect/ssh-internal-ip Ahmed_Y 2Â years, 4Â months ago\nThanks for that link but I think it is C, Although totally agree that Bastion comes 2nd in that table, no way all the user would have IP within this range 35.235.240.0/20! itsimranmalik 2Â years, 4Â months ago\n\"allows ingress traffic from the IP range `35.235.240.0/20`. This range contains all IP addresses that IAP uses for TCP forwarding\"\nhttps://cloud.google.com/iap/docs/using-tcp-forwarding#create-firewall-rule"
      },
      {
        "index": 7,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: D\nBut the question states \"You need to ensure that your team logs in to these instances in the most secure and cost efficient way\"\nBastion is more secure than IAP but I'm not sure is more cost effective...\nHard to choose"
      },
      {
        "index": 8,
        "text": "Anonymous: juliorevk 2Â years, 5Â months ago\nUnderstood about IAP being a secure way to SSH but where did the \"Allow ingress traffic from the IP range 35.235.240.0/20 on port 22.\" come from and how does that fit in? The question had no details about it and the IP range seemed to come out of nowhere. Linhtinh603 2Â years, 1Â month ago"
      },
      {
        "index": 9,
        "text": "Anonymous: Husni_adam 2Â years, 6Â months ago\nSelected Answer: B\nhttps://cloud.google.com/compute/docs/connect/ssh-using-iap#gcloud\naccording the documentation the correct answer is B"
      },
      {
        "index": 10,
        "text": "Anonymous: techsteph 2Â years, 6Â months ago\nSelected Answer: B\nhttps://cloud.google.com/compute/docs/connect/ssh-using-iap#gcloud"
      }
    ]
  },
  {
    "id": 131,
    "source": "examtopics",
    "question": "An external member of your team needs list access to compute images and disks in one of your projects. You want to follow Google-recommended practices when you grant the required permissions to this user. What should you do?",
    "options": {
      "A": "Create a custom role, and add all the required compute.disks.list and compute.images.list permissions as includedPermissions. Grant the custom role to the user at the project level.",
      "B": "Create a custom role based on the Compute Image User role. Add the compute.disks.list to the includedPermissions field. Grant the custom role to the user at the project level.",
      "C": "Create a custom role based on the Compute Storage Admin role. Exclude unnecessary permissions from the custom role. Grant the custom role to the user at the project level.",
      "D": "Grant the Compute Storage Admin role at the project level."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: demoro86 Highly Voted 1Â year, 4Â months ago\nSelected Answer: A\nI have successfully created a custom role with compute.disks.list and compute.image.list permissions. I have also tried creating it based on the Compute Storage Admin role. However, you still need to select compute.disks.list and compute.image.list individually; all permissions are unchecked by default. So A fits fine."
      },
      {
        "index": 2,
        "text": "Anonymous: rahulrauki Most Recent 1Â year, 3Â months ago\nSelected Answer: A\nYou can't give B because, Image user will be able to use the Image to create resources. Only give list access"
      },
      {
        "index": 3,
        "text": "Anonymous: joao_01 1Â year, 4Â months ago\nIts A. Give user ONLY the required permission."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct answer , just create the custom role add all the required permissoons , give to the user"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: A\nThe key word is \"needs list access\", so only A meets this requirement"
      },
      {
        "index": 6,
        "text": "Anonymous: juliorevk 1Â year, 5Â months ago\nSelected Answer: A\nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support - Both compute.disks.list and compute.images.list are available as permissions for custom roles. Makes more sense to make a new custom role than going off an admin one then adjusting it."
      },
      {
        "index": 7,
        "text": "Anonymous: shreykul 1Â year, 5Â months ago\nSelected Answer: B\nOption B allows you to create a custom role that is based on the existing Compute Image User role, which already includes the necessary permissions for accessing compute images. Then, you add the compute.disks.list permission to the custom role's includedPermissions field to grant the user list access to compute disks as well. This ensures that the user has precisely the permissions needed for their specific tasks and nothing more, following the principle of least privilege."
      },
      {
        "index": 8,
        "text": "Anonymous: shreykul 1Â year, 6Â months ago\nSelected Answer: A\nhttps://cloud.google.com/sdk/gcloud/reference/compute/images/list\nhttps://cloud.google.com/compute/docs/reference/rest/v1/disks/list"
      },
      {
        "index": 9,
        "text": "Anonymous: fatanu88 1Â year, 6Â months ago\nAnswer is B: Compute image user role provide permission to list and read images without having other permissions on the image. Granting this role at the project level gives users the ability to list all images in the project and create resources, such as instances and persistent disks, based on images in the project. Adding the compute.disks.list then meet all the question requirements"
      },
      {
        "index": 10,
        "text": "Anonymous: FJ82 1Â year, 6Â months ago\nSelected Answer: C\nTried this, could not find those permissions when I tried to create custom role directly, you need to create from the role techsteph 1Â year, 6Â months ago\nYou're right, changing my answer to C."
      }
    ]
  },
  {
    "id": 132,
    "source": "examtopics",
    "question": "You are running a web application on Cloud Run for a few hundred users. Some of your users complain that the initial web page of the application takes much longer to load than the following pages. You want to follow Googleâ€™s recommendations to mitigate the issue. What should you do?",
    "options": {
      "A": "Set the minimum number of instances for your Cloud Run service to 3.",
      "B": "Set the concurrency number to 1 for your Cloud Run service.",
      "C": "Set the maximum number of instances for your Cloud Run service to 100.",
      "D": "Update your web application to use the protocol HTTP/2 instead of HTTP/1.1."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ovokpus Highly Voted 1Â year, 3Â months ago\nSelected Answer: A\nThis is a typical cold start problem. Cold starts happen when a serverless platform like Cloud Run needs to start a new instance to handle a request because no suitable instances are available. This startup time can cause a delay, which is noticeable to users, especially on the first page load."
      },
      {
        "index": 2,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 1Â week ago\nSelected Answer: A\nSet Minimum number of active instances > 0"
      },
      {
        "index": 3,
        "text": "Anonymous: VijKall 1Â year, 2Â months ago\nSelected Answer: A\nBoth A and D looks good.\nBut going with A as it is worded as some users are impacted during startup, http/2 would help resolve more than what issue is worded here."
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 1Â year, 4Â months ago\nIts A. Look at this link: https://cloud.google.com/run/docs/tips/general#optimize_performance\nYou'll see that one of Google's recommendations for improve performance and reduce \"cold starts\", is to set a minimum of instances."
      },
      {
        "index": 5,
        "text": "Anonymous: DannSecurity 1Â year, 4Â months ago\nI checked the google recommendations for cloud run but they never mention HTTP/2. I am going with A"
      },
      {
        "index": 6,
        "text": "Anonymous: Cherrycardo 1Â year, 5Â months ago\nSelected Answer: A\nhttps://cloud.google.com/functions/docs/configuring/min-instances\nEven though the initial # of instances present in the VM is not stated, setting a min amount of instances \"can further help you avoid cold starts and reduce application latency\"."
      },
      {
        "index": 7,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: A\nmin. # of instances should solve latency issue"
      },
      {
        "index": 8,
        "text": "Anonymous: qannik 1Â year, 5Â months ago\nSelected Answer: A\nI chose A too.\nThe question should be related to GCP products and how can you best configure them.\nI shouldn't be forced to change my app to use HTTP/2 qannik 1Â year, 5Â months ago\nhttps://cloud.google.com/blog/topics/developers-practitioners/3-ways-optimize-cloud-run-response-times"
      },
      {
        "index": 9,
        "text": "Anonymous: Vzero2333 1Â year, 5Â months ago\nI chosse A\nnotice ''some of user'' and ''initial web page slowly than other page''\nbecause the some of user login the web without load and the cloud run scale the instance to 0 so that the users had to waiting the startup new instance sometimes."
      },
      {
        "index": 10,
        "text": "Anonymous: geeroylenkins 1Â year, 6Â months ago\nSelected Answer: D\nhttps://www.cloudflare.com/learning/performance/http2-vs-http1.1/\nI'm going with D too.\nB wouldn't help: https://cloud.google.com/run/docs/about-concurrency#concurrency-1\nC wouldn't help - setting a max won't increase speed ever.\nA would not necessarily help - there's no indication that the initial page is taking much longer just because there are too few instances. However, D would improve how things load, as per the first link I posted."
      }
    ]
  },
  {
    "id": 133,
    "source": "examtopics",
    "question": "You are building a data lake on Google Cloud for your Internet of Things (IoT) application. The IoT application has millions of sensors that are constantly streaming structured and unstructured data to your backend in the cloud. You want to build a highly available and resilient architecture based on Google-recommended practices. What should you do?",
    "options": {
      "A": "Stream data to Pub/Sub, and use Dataflow to send data to Cloud Storage.",
      "B": "Stream data to Pub/Sub, and use Storage Transfer Service to send data to BigQuery.",
      "C": "Stream data to Dataflow, and use Dataprep by Trifacta to send data to Bigtable.",
      "D": "Stream data to Dataflow, and use Storage Transfer Service to send data to BigQuery."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: shreykul Highly Voted 1Â year, 5Â months ago\nSelected Answer: A\nA. Streaming data to Pub/Sub allows you to decouple the ingestion of data from the processing and storage, providing a scalable and reliable message queue that can handle the high volume of data coming from millions of sensors.\nUsing Dataflow to consume data from Pub/Sub and send it to Cloud Storage allows for real-time data processing and storage. Dataflow is a fully managed service for processing data in real-time or batch mode, making it an ideal choice for handling the constant stream of data from IoT sensors.\nStoring data in Cloud Storage offers high durability and availability, providing a robust foundation for building a data lake. Cloud Storage is a scalable object storage service that can handle large volumes of structured and unstructured data, making it well-suited for the IoT application's data requirements."
      },
      {
        "index": 2,
        "text": "Anonymous: VijKall Highly Voted 1Â year, 2Â months ago\nSelected Answer: A\nPub/Sub , Dataflow and BigTable would have been idle solution, but since Cloud Storage is the only option with that combo, I will go with A. nmnm22 1Â year, 2Â months ago\nideal*"
      },
      {
        "index": 3,
        "text": "Anonymous: joao_01 Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nIts A, for sure."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nA is the correct answer a there is both unstuctured and strucutder data"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: A\naccording to https://cloud.google.com/architecture/optimized-large-scale-analytics-ingestion"
      }
    ]
  },
  {
    "id": 134,
    "source": "examtopics",
    "question": "You are running out of primary internal IP addresses in a subnet for a custom mode VPC. The subnet has the IP range 10.0.0.0/20, and the IP addresses are primarily used by virtual machines in the project. You need to provide more IP addresses for the virtual machines. What should you do?",
    "options": {
      "A": "Add a secondary IP range 10.1.0.0/20 to the subnet.",
      "B": "Change the subnet IP range from 10.0.0.0/20 to 10.0.0.0/18.",
      "C": "Change the subnet IP range from 10.0.0.0/20 to 10.0.0.0/22.",
      "D": "Convert the subnet IP range from IPv4 to IPv6."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: joao_01 Highly Voted 1Â year, 10Â months ago\nSelected Answer: A\nThis one is tricky. First i was going with B, then i did some search. Option A and B can indeed add more IPs. However, i think the option is A because between those 2 options the option A we will add IPs without changing the any ours VMs configurations that we currently have. If we choose B might need to change our current VMs configuration in order to reflect the new IP range expanded. You guys understand what i mean?\nLink: https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\n\"If you expand the primary IPv4 range of a subnet, you might need to modify other configurations that are assuming this IP address range\" AdelElagawany 1Â year, 8Â months ago\nWhat are the configurations needed?\nFirst of all the CIDR Range 10.0.0.0/18 Include the CIDR range 10.0.0.0/20 and this is a mandatory step for adjusting the Primary IP CIDR Range so No change needed on the machine level. sj209 1Â year, 8Â months ago\nyou will need to update the gateway IP of all the servers in 10.0.0.0/20, while changing to 10.0.0.0/18. So adding a new subnet makes sense. sj209 1Â year, 8Â months ago\ni was wrong, GW does not change. B seems correct tlopsm 1Â year, 7Â months ago\nBut the subnet mask will change ccpmad 1Â year, 1Â month ago\nYou are wrong, with new subnet range, we dont have to reflect anything to existing vms."
      },
      {
        "index": 2,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nB is the correct answer\nWhen you expand a subnet's IP range in Google Cloud from /20 (255.255.240.0) to /18 (255.255.192.0), the existing virtual machines (VMs) will not experience any disruption, and their current configurations will remain intact.\nThe subnet mask is updated automatically by Google Cloud. There are no interruptions to network connectivity for running VMs.\nThis is impossible to do in traditional Networking(without outage). only in Google cloud :-)"
      },
      {
        "index": 3,
        "text": "Anonymous: IshwarChandra 1Â year, 3Â months ago\nSelected Answer: B\nPurpose of Secondary IP Range: \"Adding a secondary IP range allows you to assign additional IP addresses to instances in a subnet without changing the subnet's primary IP range. This can be useful when you want to segregate traffic or allocate specific IP addresses to certain types of instances or workloads within the same subnet.\"\nIn the question, no where the logical separation of vm or traffic segregation is mentioned so by expanding th eprimary ip range will increase the available ips so Option B is correct."
      },
      {
        "index": 4,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: B\nOption B offers a practical and scalable solution to address the shortage of IP addresses by enlarging the subnet's range in the most efficient way.\nOption A, while technically feasible, managing multiple IP ranges within a subnet adds complexity and can potentially lead to routing issues. Even with a secondary IP range, you'll likely need to either:"
      },
      {
        "index": 1,
        "text": "Configure VMs with multiple network interfaces, each assigned an IP from a different range. This adds management overhead."
      },
      {
        "index": 2,
        "text": "Re-configure existing VMs with IPs from the new secondary range, potentially causing downtime or requiring complex IP address changes."
      },
      {
        "index": 5,
        "text": "Anonymous: leoalvarezh 1Â year, 5Â months ago\nSelected Answer: A\nI think that if we go with option B is OK but we need to configure the new mask on VMs\nIf we go with option A, for me is OK but VMs are not in the same LAN, we need to configure connectivity but nothing related with that requirement in the question...so maybe A is more accurate"
      },
      {
        "index": 6,
        "text": "Anonymous: Raghav2001 1Â year, 5Â months ago\nB can not be the answer we can use B if the IP address are primarily used by Interfaces or services with in VM"
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: A\nA. Add a secondary IP range 10.1.0.0/20 to the subnet:\nâ€¢ This option involves adding a secondary IP range to the existing subnet. This can provide additional IP addresses without changing the existing primary IP range.\nB. Change the subnet IP range from 10.0.0.0/20 to 10.0.0.0/18:\nâ€¢ This involves expanding the current subnet's CIDR range to a larger block (from /20 to /18). This expansion will significantly increase the number of available IP addresses.\nâ€¢ However, changing the CIDR block of an existing subnet is not straightforward in GCP. It typically requires creating a new subnet with the desired range and migrating resources, which can be complex and disruptive."
      },
      {
        "index": 8,
        "text": "Anonymous: tesix79748 1Â year, 7Â months ago\nSelected Answer: B\nAll subnets have a primary CIDR range, which is the range of internal IP addresses that define the subnet. Each VM instance gets its primary internal IP address from this range. You can also allocate alias IP ranges from that primary range, or you can add a secondary range to the subnet and allocate alias IP ranges from the secondary range. Use of alias IP ranges does not require secondary subnet ranges. These secondary subnet ranges merely provide an organizational tool.\nhttps://cloud.google.com/vpc/docs/alias-ip"
      }
    ]
  },
  {
    "id": 135,
    "source": "examtopics",
    "question": "Your company requires all developers to have the same permissions, regardless of the Google Cloud project they are working on. Your companyâ€™s security policy also restricts developer permissions to Compute Engine, Cloud Functions, and Cloud SQL. You want to implement the security policy with minimal effort. What should you do?",
    "options": {
      "A": "â€¢ Create a custom role with Compute Engine, Cloud Functions, and Cloud SQL permissions in one project within the Google Cloud organization.",
      "B": "â€¢ Add all developers to a Google group in Google Groups for Workspace.",
      "C": "â€¢ Add all developers to a Google group in Cloud Identity.",
      "D": "â€¢ Add all developers to a Google group in Cloud Identity."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: D\nD combines the security of a custom role tailored to the company's policy with the ease of management provided by organization-level assignment to a Cloud Identity group."
      },
      {
        "index": 2,
        "text": "Anonymous: leoalvarezh 1Â year, 5Â months ago\nSelected Answer: D\nBest practise is to use predefined roles but in this case we need to apply some restrictions about our company's security policy so I think D is the valid response."
      },
      {
        "index": 3,
        "text": "Anonymous: joao_01 1Â year, 10Â months ago\nSelected Answer: D\nI vote for D"
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 1Â year, 10Â months ago\nI vote for D"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nd is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: D\nPermissions provided at the Organization level are inherited to the folder level and project level."
      },
      {
        "index": 7,
        "text": "Anonymous: gpais 1Â year, 11Â months ago\nSelected Answer: C\nUse predefined roles: Use predefined roles, such as â€œEditorâ€ or â€œViewerâ€, instead of creating custom roles. This makes it easier to understand the level of access associated with a role.Use custom roles: Create custom roles when predefined roles do not meet the specific needs of your organization.\nIn the link below:\nhttps://cloud.google.com/iam/docs/roles-overview#custom\nWhen to use custom roles\nIn most situations, you should be able to use predefined roles instead of custom roles. Predefined roles are maintained by Google, and are updated automatically when new permissions, features, or services are added to Google Cloud. In contrast, custom roles are not maintained by Google; when Google Cloud adds new permissions, features, or services, your custom roles will not be updated automatically."
      },
      {
        "index": 8,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: D\nonly D meets best practices"
      },
      {
        "index": 9,
        "text": "Anonymous: shreykul 1Â year, 12Â months ago\nSelected Answer: D\nhttps://www.cloudskillsboost.google/focuses/1035?parent=catalog#:~:text=custom%20role%20at%20the%20organization%20level"
      }
    ]
  },
  {
    "id": 136,
    "source": "examtopics",
    "question": "You are working for a hospital that stores its medical images in an on-premises data room. The hospital wants to use Cloud Storage for archival storage of these images. The hospital wants an automated process to upload any new medical images to Cloud Storage. You need to design and implement a solution. What should you do?",
    "options": {
      "A": "Create a Pub/Sub topic, and enable a Cloud Storage trigger for the Pub/Sub topic. Create an application that sends all medical images to the Pub/Sub topic.",
      "B": "Create a script that uses the gcloud storage command to synchronize the on-premises storage with Cloud Storage, Schedule the script as a cron job.",
      "C": "Create a Pub/Sub topic, and create a Cloud Function connected to the topic that writes data to Cloud Storage. Create an application that sends all medical images to the Pub/Sub topic.",
      "D": "In the Google Cloud console, go to Cloud Storage. Upload the relevant images to the appropriate bucket."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sinh Highly Voted 1Â year, 6Â months ago\nSame as No.168."
      },
      {
        "index": 2,
        "text": "Anonymous: shreykul Highly Voted 1Â year, 12Â months ago\nSelected Answer: C\nOption C is more robust and utilises the GCP functionalities correctly. qannik 1Â year, 11Â months ago\nYou can not send images to a Pub/Sub topic. peddyua 11Â months, 3Â weeks ago\nNothing saysaboutarealtime, it'sarchival solution,makes sense to upload once at night in batches, sync will work"
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua Most Recent 11Â months, 3Â weeks ago\nSelected Answer: B\nsimple automation with least efforts, and cheap"
      },
      {
        "index": 4,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nSelected Answer: B\nYou can use 'gsutil' or 'gcloud storage' to upload to Cloud Storage\n'gcloud storage' command is more advanced, it has more automation\nwhile 'gcloud storage' offers enhanced performance and usability features, it does not completely replace 'gsutil'. Users should choose based on their specific needs and the complexity of their tasks."
      },
      {
        "index": 5,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: B\nB makes the most sense. Using the gcloud storage command to synchronize offers a straightforward way to mirror the on-premises data with Cloud Storage. Cron jobs are a well-established mechanism for scheduling recurring tasks, ensuring the synchronization process runs according to the hospital's needs. This approach is relatively easy to set up and maintain, especially for on-premises systems where installing additional software might be restricted."
      },
      {
        "index": 6,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: B\nA. Pub/Sub topic with a Cloud Storage trigger:\nâ€¢ Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. However, creating a Cloud Storage trigger for a Pub/Sub topic isn't feasible as triggers usually work the other way around (Cloud Storage events triggering Pub/Sub messages).\nâ€¢ This option would not provide a direct or automated way to upload images from on-premises storage to Cloud Storage. Cynthia2023 1Â year, 6Â months ago\n. Script using gcloud storage command and cron job:\nâ€¢ This approach involves writing a script that synchronizes the on-premises storage with Cloud Storage using the gcloud storage command.\nâ€¢ Scheduling this script as a cron job would automate the process, allowing for regular uploads of new images without manual intervention.\nâ€¢ This is a straightforward approach and aligns well with the requirement of automated archival storage. Cynthia2023 1Â year, 6Â months ago\ngcloud storage Command: there is no gcloud storage command in the Google Cloud SDK. The gcloud tool does not directly provide a functionality for synchronizing files to Google Cloud Storage.\nCorrect Tool - gsutil: The correct tool to use for synchronizing files with Google Cloud Storage is gsutil, specifically the gsutil rsync command. gsutil is indeed part of the Google Cloud SDK, but it's a separate tool from gcloud. Cynthia2023 1Â year, 6Â months ago\nI still go with B, because it's the closest answer."
      },
      {
        "index": 7,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: A\nPer ChatGPT,\nOption A is the most appropriate solution for the hospital's requirement as it provides an automated, scalable, and event-driven approach for uploading new medical images from the on-premises data room to Cloud Storage. PiperMe 1Â year, 4Â months ago\nYou can't use ChatGCP with Google questions! Gemini all the way ;)\nThat being said, B is the best solution. It provides a reliable, scheduled, and easy-to-manage solution that aligns perfectly with the hospital's need to automate archival storage of medical images in Cloud Storage. Introducing Pub/Sub and an additional application adds complexity. While Pub/Sub can be useful for event-driven architectures, it's overkill for this basic synchronization. PiperMe 1Â year, 4Â months ago\nChatGPT... I've been writing GCP too many times today lol"
      },
      {
        "index": 8,
        "text": "Anonymous: ogerber 1Â year, 7Â months ago\nSelected Answer: B\nIts B, replicated question."
      },
      {
        "index": 9,
        "text": "Anonymous: hylee 1Â year, 7Â months ago\nSelected Answer: B\nHello, guys. the exact same question was Question. 91.\nand the answer was :\n- Create a script that uses the gsutil command line interface to synchronize the on-premises storage with Cloud Storage. Schedule the script as a cron job.\nSo I would vote for B"
      },
      {
        "index": 10,
        "text": "Anonymous: carlalap 1Â year, 8Â months ago\nThe answer is C. The hint is \"The hospital wants an automated process to upload any new medical images to Cloud Storage\". By creating a Cloud Function connected to the topic, you can write a serverless function that automatically executes when a new message is published to the topic. The Cloud Function can then write the data to Cloud Storage, which is a durable and cost-effective storage service for archival purposes."
      }
    ]
  },
  {
    "id": 137,
    "source": "examtopics",
    "question": "Your company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?",
    "options": {
      "A": "Bigtable",
      "B": "BigQuery",
      "C": "Cloud SQL",
      "D": "Firestore"
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: shmoeee Highly Voted 1Â year, 10Â months ago\nAnother duplicate question"
      },
      {
        "index": 2,
        "text": "Anonymous: Rahaf99 Highly Voted 2Â years, 1Â month ago\nSelected Answer: C\nCloud SQL is very similar to postgreSQL"
      },
      {
        "index": 3,
        "text": "Anonymous: FormacionCloud314 Most Recent 1Â year, 2Â months ago\nas already answered in question 188, the answer is CLoud SQL."
      },
      {
        "index": 4,
        "text": "Anonymous: carlalap 2Â years, 2Â months ago\nCloud SQL (supports PostgreSQL), so it should require minimal code changes.\nAnswer:\nC. Cloud SQL"
      },
      {
        "index": 5,
        "text": "Anonymous: xhilmi 2Â years, 4Â months ago\nSo what about this another question?\nYour company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?\nA. BigQuery\nB. Cloud SQL\nC. Cloud Spanner\nD. Cloud Datastore\nIt should be Cloud Spanner or Cloud SQL ? joao_01 2Â years, 3Â months ago\nCloud SQL"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: C\nACID and strong consistency are in C or D, but Firestore is for documents and in question we have \"multi-table updates\" so there left C rahulrauki 2Â years, 3Â months ago\nYou missed the main \"postgres\" part but yeah"
      },
      {
        "index": 7,
        "text": "Anonymous: pritampanda1988 2Â years, 5Â months ago\nSelected Answer: C\nCloud SQL is the most appropriate choice for deploying the application with the required characteristics while minimizing code changes and maintaining strong consistency, fast queries, and ACID guarantees for multi-table transactional updates."
      }
    ]
  },
  {
    "id": 138,
    "source": "examtopics",
    "question": "Your company runs one batch process in an on-premises server that takes around 30 hours to complete. The task runs monthly, can be performed offline, and must be restarted if interrupted. You want to migrate this workload to the cloud while minimizing cost. What should you do?",
    "options": {
      "A": "Create an Instance Template with Spot VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload.",
      "B": "Migrate the workload to a Compute Engine VM. Start and stop the instance as needed.",
      "C": "Migrate the workload to a Google Kubernetes Engine cluster with Spot nodes.",
      "D": "Migrate the workload to a Compute Engine Spot VM."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: shreykul Highly Voted 2Â years, 5Â months ago\nSelected Answer: B\nB. Migrating the workload to a Compute Engine VM and starting and stopping the instance as needed allows you to control when the task runs. This approach provides flexibility in terms of when to initiate the batch process, and it can be easily scheduled to run monthly. By stopping the instance when the task is not running, you can save on compute costs."
      },
      {
        "index": 2,
        "text": "Anonymous: longph8 Most Recent 10Â months ago\nSelected Answer: D\nSpot VMs (Option D) provide the lowest cost and fit the batch process's needs perfectly.\nRegular VMs (Option B) are flexible but cost significantly more without adding value for this use case."
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua 11Â months, 3Â weeks ago\nSelected Answer: D\nkey work 'performed offline' and runs '30 hours' nobody will be sitting and watching this for 30 hours, during night."
      },
      {
        "index": 4,
        "text": "Anonymous: wota 1Â year, 1Â month ago\nGemini recommands \"Spot VMs\", So Answer is D"
      },
      {
        "index": 5,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: D\nD is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: B\nB. is correct. shreykul provided you the correct answer, take it or leave it."
      },
      {
        "index": 7,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: A\nIt seems to me that answer is A.\nScale to zero - After completed batch processes, minimize cost.\nSpot VM - minimize cost.\nfailure resistant - do not need restart instance.\nYou already have a template - delete and deploy your Compute instances once a month easily and fast.\nCan you explain me unless I understand some thing."
      },
      {
        "index": 8,
        "text": "Anonymous: noopy 1Â year, 9Â months ago\nSelected Answer: A\nA is the most robust and cost-effective for your scenario. This setup leverages the cost savings of Spot VMs while also providing the ability to handle interruptions through a Managed Instance Group, ensuring the process is completed even if individual instances are preempted. Additionally, the automatic scaling based on CPU utilization helps manage resources efficiently, further reducing costs.\nThus, A not only minimizes costs by using Spot VMs but also enhances reliability and scalability through a Managed Instance Group, making it the most suitable choice for migrating your batch process to the cloud. ccpmad 1Â year, 8Â months ago\n\"runs one batch process\"\nSo, why would you like a instange group? it is only one batch process. And if it is interrupted, it has to be restarted...for me it is B. BuenaCloudDE 1Â year, 6Â months ago\nIt seems to me that answer is A.\nScale to zero - After completed batch processes, minimize cost.\nSpot VM - minimize cost.\nfailure resistant - do not need restart instance.\nYou already have a template - delete and deploy your Compute instances once a month easily and fast.\nCan you explain me unless I understand some thing."
      },
      {
        "index": 9,
        "text": "Anonymous: omunoz 1Â year, 9Â months ago\nI donÂ´t like this kind of questions... but I think is A..\nAn Instance Template with Spot VMs (not preemptible)... yomi95 1Â year, 3Â months ago\nBut will this 30hour job have to restart every time spot VM is replaced since they can preempt with 30 sec notice."
      },
      {
        "index": 10,
        "text": "Anonymous: pumajd 1Â year, 10Â months ago\nSelected Answer: B\nSame as 136 aviiciii 1Â year, 6Â months ago\nno, this question has different answer choices with the inclusion of spot vms Phat 10Â months ago\nGoogle renamed it to spot to align with industry terminilogy. 85c887f 9Â months, 3Â weeks ago\nIn 136 we had preemptible VMs, that are limited to 24 h, and this is not a case with a Spot VMS. So answers would not be the same."
      }
    ]
  },
  {
    "id": 139,
    "source": "examtopics",
    "question": "You are planning to migrate the following on-premises data management solutions to Google Cloud:\n\nâ€¢ One MySQL cluster for your main database\nâ€¢ Apache Kafka for your event streaming platform\nâ€¢ One Cloud SQL for PostgreSQL database for your analytical and reporting needs\n\nYou want to implement Google-recommended solutions for the migration. You need to ensure that the new solutions provide global scalability and require minimal operational and infrastructure management. What should you do?",
    "options": {
      "A": "Migrate from MySQL to Cloud SQL, from Kafka to Pub/Sub, and from Cloud SQL for PostgreSQL to BigQuery.",
      "B": "Migrate from MySQL to Cloud Spanner, from Kafka to Pub/Sub, and from Cloud SQL for PostgreSQL to BigQuery.",
      "C": "Migrate from MySQL to Cloud Spanner, from Kafka to Memorystore, and from Cloud SQL for PostgreSQL to Cloud SQL.",
      "D": "Migrate from MySQL to Cloud SQL, from Kafka to Memorystore, and from Cloud SQL for PostgreSQL to Cloud SQL."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: VijKall Highly Voted 1Â year, 8Â months ago\nSelected Answer: B\nNeeds Global scalability ---- Spanner is in, CloudSQL is out.\nKafka --> Pub/Sub & not memory store.\nPostgres --> BigQuery as it needs scalability and for analytics."
      },
      {
        "index": 2,
        "text": "Anonymous: kacper07 Most Recent 1Â year, 1Â month ago\nB is correct, even M$ copilot chose it :D"
      },
      {
        "index": 3,
        "text": "Anonymous: ExamsFR 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct answer."
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 1Â year, 10Â months ago\nSelected Answer: B\nIts B, makes sense"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nB is the correrct answer as question demands global scalibity for it cloud spanner , for PostgreSQL to BIG query"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nFor event streaming -> Cloud Pub/Sub is a good choice.\nhttps://cloud.google.com/pubsub/docs/overview\nDatabase for analytics and reporting -> BigQuery is a good choice.\nhttps://cloud.google.com/bigquery/docs/introduction\nFor MySQL, we have two options Cloud SQL or Cloud Spanner. Cloud SQL has MySQL flavored database, while Cloud Spanner provide Google Standard SQL (aka GoogleSQL) which supports standard SQL queries. In addition, the question is asking about \"global scalability and require minimal operational and infrastructure management\", where Cloud Spanner wins a score.\nHence, B is the correct answer.\nhttps://cloud.google.com/spanner\nhttps://cloud.google.com/spanner/docs/create-query-database-console#create-instance\nhttps://cloud.google.com/spanner/docs/create-query-database-console#create-database"
      },
      {
        "index": 7,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: B\nagree B"
      },
      {
        "index": 8,
        "text": "Anonymous: Speridian 1Â year, 11Â months ago\nB should be the answer. Cloud Spanner - Global support."
      },
      {
        "index": 9,
        "text": "Anonymous: ankitb4u 1Â year, 11Â months ago\nB should be the answer as cloud spanner provides scalability"
      }
    ]
  },
  {
    "id": 140,
    "source": "examtopics",
    "question": "During a recent audit of your existing Google Cloud resources, you discovered several users with email addresses outside of your Google Workspace domain. You want to ensure that your resources are only shared with users whose email addresses match your domain. You need to remove any mismatched users, and you want to avoid having to audit your resources to identify mismatched users. What should you do?",
    "options": {
      "A": "Create a Cloud Scheduler task to regularly scan your projects and delete mismatched users.",
      "B": "Create a Cloud Scheduler task to regularly scan your resources and delete mismatched users.",
      "C": "Set an organizational policy constraint to limit identities by domain to automatically remove mismatched users.",
      "D": "Set an organizational policy constraint to limit identities by domain, and then retroactively remove the existing mismatched users"
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: joao_01 Highly Voted 2Â years, 4Â months ago\nSelected Answer: D\nIts D. \"The domain restriction constraint is not retroactive. Once a domain restriction is set, this limitation will apply to IAM policy changes made from that point forward, and not to any previous changes.\". Link: https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nOrganization policies are not retroactive. If you need to force a change to your resource hierarchy that would violate an enforced constraint, you can disable the organization policy, make the change, and then enable the organization policy again."
      },
      {
        "index": 3,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: D\njoao_01 provided the appropriate answer, take it or leave it."
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: D\nD seems to be most appropriate. You can use organization policy constraint to limit the identities by domain. Once the organization policy is set, you can remove the leftover users that mismatched the conditions.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains"
      },
      {
        "index": 5,
        "text": "Anonymous: Cherrycardo 2Â years, 5Â months ago\nSelected Answer: D\nhttps://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints\nThis list constraint defines the set of domains that email addresses added to Essential Contacts can have.\nBy default, email addresses with any domain can be added to Essential Contacts.\nThe allowed/denied list must specify one or more domains of the form @example.com. If this constraint is active and configured with allowed values, only email addresses with a suffix matching one of the entries from the list of allowed domains can be added in Essential Contacts.\nThis constraint has no effect on updating or removing existing contacts.\nconstraints/essentialcontacts.allowedContactDomains"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: D\nIn order to define an organization policy, you choose a constraint, which is a particular type of restriction"
      },
      {
        "index": 7,
        "text": "Anonymous: juliorevk 2Â years, 5Â months ago\nSelected Answer: D\nhttps://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints - Domain restricted sharing\nIf this constraint is active, only principals that belong to the allowed customer IDs can be added to IAM policies. It doesn't specifically say, but I think it doesn't get rid of existing principals."
      },
      {
        "index": 8,
        "text": "Anonymous: Speridian 2Â years, 5Â months ago\nShould be D. Organization policy does not remove users automatically."
      }
    ]
  },
  {
    "id": 141,
    "source": "examtopics",
    "question": "Your application is running on Google Cloud in a managed instance group (MIG). You see errors in Cloud Logging for one VM that one of the processes is not responsive. You want to replace this VM in the MIG quickly. What should you do?",
    "options": {
      "A": "Use the gcloud compute instances update command with a REFRESH action for the VM.",
      "B": "Use the gcloud compute instance-groups managed recreate-instances command to recreate the VM.",
      "C": "Select the MIG from the Compute Engine console and, in the menu, select Replace VMs.",
      "D": "Update and apply the instance template of the MIG."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\ngcloud compute instance-groups managed recreate-instances"
      },
      {
        "index": 2,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: B\nThis allows you to quickly recreate the specific VM"
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: B\nAnswer is B.\nB: You can recreate one or more VMs\nC: Only option to replace each VMs in MIG\nCompare it:\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/rolling-action/replace"
      },
      {
        "index": 4,
        "text": "Anonymous: Leo212003 1Â year, 6Â months ago\nI think option C is correct, since gcloud CLI might take some extra time to reinitiate the instance, option B looks wrong"
      },
      {
        "index": 5,
        "text": "Anonymous: leoalvarezh 1Â year, 11Â months ago\nThe underlying virtual machine instances are deleted and recreated based on the latest instance template configured for the managed instance group."
      },
      {
        "index": 6,
        "text": "Anonymous: Rocky_Jatin 2Â years, 3Â months ago\nSelected Answer: B\nB hoga common sense"
      },
      {
        "index": 7,
        "text": "Anonymous: Ben_oso 2Â years, 3Â months ago\nSelected Answer: C\nI think its C, because the question talk of \"Replace the VM\".\nIn B you recreate the same VM, so i think the C is more sense."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB is the correct answer , as the question demands in MIG managed instance group. B"
      },
      {
        "index": 9,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: B\nYou can recreate specified instance(s) in a managed instance group.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances"
      },
      {
        "index": 10,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: B\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/recreate-instances"
      }
    ]
  },
  {
    "id": 142,
    "source": "examtopics",
    "question": "You want to permanently delete a Pub/Sub topic managed by Config Connector in your Google Cloud project. What should you do?",
    "options": {
      "A": "Use kubectl to create the label deleted-by-cnrm and to change its value to true for the topic resource.",
      "B": "Use kubectl to delete the topic resource.",
      "C": "Use gcloud CLI to delete the topic.",
      "D": "Use gcloud CLI to update the topic label managed-by-cnrm to false."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: nmnm22 Highly Voted 2Â years, 2Â months ago\ni hate these questions so much"
      },
      {
        "index": 2,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: B\nIf a resource is managed by the Config Connector, you can update/delete it through kubectl command.\nhttps://cloud.google.com/config-connector/docs/how-to/getting-started#before_you_begin"
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: B\nDeleting a resource\nUse kubectl delete to delete resources. For example, to delete the PubSubTopic you created earlier, run kubectl delete with your pubsub-topic.yaml file:\nkubectl delete -f pubsub-topic.yaml"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB is the right answer, if the resource is managed by the config connector you can just use the kubectl to use detele or update the topic resource"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: B\ncreated by kubectl should be removed by it"
      },
      {
        "index": 6,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: B\nhttps://cloud.google.com/config-connector/docs/how-to/getting-started#deleting_a_resource"
      },
      {
        "index": 7,
        "text": "Anonymous: Speridian 2Â years, 5Â months ago\nshould be C."
      },
      {
        "index": 8,
        "text": "Anonymous: happydays 2Â years, 5Â months ago\nSelected Answer: B\nTHIS IS B"
      },
      {
        "index": 9,
        "text": "Anonymous: gpais 2Â years, 5Â months ago\nSelected Answer: C\nhttps://cloud.google.com/sdk/gcloud/reference/pubsub/topics/delete qannik 2Â years, 5Â months ago\nRead the question. It's a Pub/Sub topic managed by Config Connector.\nhttps://cloud.google.com/config-connector/docs/how-to/getting-started#deleting_a_resource"
      }
    ]
  },
  {
    "id": 143,
    "source": "examtopics",
    "question": "Your company is using Google Workspace to manage employee accounts. Anticipated growth will increase the number of personnel from 100 employees to 1,000 employees within 2 years. Most employees will need access to your companyâ€™s Google Cloud account. The systems and processes will need to support 10x growth without performance degradation, unnecessary complexity, or security issues. What should you do?",
    "options": {
      "A": "Migrate the users to Active Directory. Connect the Human Resources system to Active Directory. Turn on Google Cloud Directory Sync (GCDS) for Cloud Identity. Turn on Identity Federation from Cloud Identity to Active Directory.",
      "B": "Organize the users in Cloud Identity into groups. Enforce multi-factor authentication in Cloud Identity.",
      "C": "Turn on identity federation between Cloud Identity and Google Workspace. Enforce multi-factor authentication for domain wide delegation.",
      "D": "Use a third-party identity provider service through federation. Synchronize the users from Google Workplace to the third-party provider in real time."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: 0verK0alafied Highly Voted 1Â year, 8Â months ago\nSelected Answer: B\nWhy would you need to federate the identities for Cloud ID and Workspace accounts? It's the same thing! I have a dev gcp org doing exactly this..."
      },
      {
        "index": 2,
        "text": "Anonymous: gpais Highly Voted 1Â year, 11Â months ago\nSelected Answer: C\nI think C is the best"
      },
      {
        "index": 3,
        "text": "Anonymous: ritvikk49 Most Recent 1Â year ago\nSelected Answer: C\nB is incorrect because organizing user accounts within Cloud Identity and enforcing multi-factor authentication (MFA) alone is a limited approach for handling large-scale growth. While Cloud Identity groups and MFA are important for managing user permissions and enhancing security, this option lacks a robust identity management solution to handle the projected tenfold user increase. It does not address scalability challenges or provide a way to efficiently manage identities across Google Cloud and other services the company might integrate as they expand. While it would add some security, it would not streamline identity management or account synchronization at the scale needed."
      },
      {
        "index": 4,
        "text": "Anonymous: ccpmad 1Â year, 2Â months ago\nSelected Answer: B\nGoogle Cloud Identity is Googleâ€™s identity provider (idP) that is used by both Workspace and Google Cloud.\nFirst, I thought it was C, but there is not identity federation between Cloud Identity and Workspace, because Cloud identity goes first and it is used by GCP, Workspace or other Google services.\nMoreover, By default, Cloud Identity Free includes 50 free licenses"
      },
      {
        "index": 5,
        "text": "Anonymous: sukouto 1Â year, 4Â months ago\nSelected Answer: B\nAccording to the following article, Google Workspace already allows access to Google Cloud platform, so there should be no need for \"identity federation between Cloud Identity and Google Workspace\" (which I cannot find documentation about). C is out.\nhttps://cloud.google.com/iam/docs/user-identities\nA and D are out because Google is not going to suggest you use some other identity provider, lol"
      },
      {
        "index": 6,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: C\nI see everyone is split here, but I like C best. Option C provides a streamlined and secure way to accommodate growth within the Google ecosystem. It prioritizes both ease of management and security.\nYou already use Workspace for user management... Adding Active Directory adds complexity and is unnecessary; A is out. While improving security and organization, it doesn't directly solve the issue of seamless access to GCP resources or address user management as you scale; B is outâ€¦. While some scenarios might justify it, you're already using Google Workspace for identity. This option adds complexity, cost, and an extra system to manage; D is outâ€¦."
      },
      {
        "index": 7,
        "text": "Anonymous: leoalvarezh 1Â year, 5Â months ago\nSelected Answer: C\nI think B helps to manage but the answer in this scenario is C due to allows for seamless integration between user accounts in both services"
      },
      {
        "index": 8,
        "text": "Anonymous: blackBeard33 1Â year, 5Â months ago\nSelected Answer: B\nI choose B."
      },
      {
        "index": 9,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: A\nPer ChatGPT,\nOption A is the most suitable choice as it leverages the scalability and centralization capabilities of Active Directory, integrates with existing systems, and ensures seamless user management and authentication across Google Workspace and Google Cloud, aligning with the requirements for anticipated growth without introducing unnecessary complexity or security issues. PiperMe 1Â year, 4Â months ago\nPlease stop using ChatGPT. Active Directory is Azure. This adds significant overhead in setup and management of a new directory system. Integrating it with both Workspace and Google Cloud increases complexity."
      },
      {
        "index": 10,
        "text": "Anonymous: carlalap 1Â year, 7Â months ago\nAnswer is A. It is talking about long term.\nThis option can support 10x growth without performance degradation, unnecessary complexity, or security issues, as it leverages the scalability, reliability, and security of Active Directory and Cloud Identity."
      }
    ]
  },
  {
    "id": 144,
    "source": "examtopics",
    "question": "You want to host your video encoding software on Compute Engine. Your user base is growing rapidly, and users need to be able to encode their videos at any time without interruption or CPU limitations. You must ensure that your encoding solution is highly available, and you want to follow Google-recommended practices to automate operations. What should you do?",
    "options": {
      "A": "Deploy your solution on multiple standalone Compute Engine instances, and increase the number of existing instances when CPU utilization on Cloud Monitoring reaches a certain threshold.",
      "B": "Deploy your solution on multiple standalone Compute Engine instances, and replace existing instances with high-CPU instances when CPU utilization on Cloud Monitoring reaches a certain threshold.",
      "C": "Deploy your solution to an instance group, and increase the number of available instances whenever you see high CPU utilization in Cloud Monitoring.",
      "D": "Deploy your solution to an instance group, and set the autoscaling based on CPU utilization."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 1Â year, 10Â months ago\nSelected Answer: D\nThe answer is D.\nYou can create a managed instance group with autoscaling enabled based on CPU utilization. This way appropriate number of instances can be added or removed based on the CPU metrics.\nhttps://cloud.google.com/compute/docs/instance-groups/create-mig-with-basic-autoscaling\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/set-autoscaling"
      },
      {
        "index": 2,
        "text": "Anonymous: Akinzoa Most Recent 1Â year, 7Â months ago\nD is the correct answer!"
      },
      {
        "index": 3,
        "text": "Anonymous: VijKall 1Â year, 8Â months ago\nSelected Answer: D\nAnswer is D.\nMIG with scaling using CPU utilization."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nD seems more correct, just set the autoscaling based on the CPU ulilization"
      },
      {
        "index": 5,
        "text": "Anonymous: Jerica_ 1Â year, 11Â months ago\nSelected Answer: D\nD seems most appropriate"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: D\ndefinitely D"
      },
      {
        "index": 7,
        "text": "Anonymous: gpais 1Â year, 11Â months ago\nSelected Answer: D\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-groups/managed/set-autoscaling"
      }
    ]
  },
  {
    "id": 145,
    "source": "examtopics",
    "question": "Your managed instance group raised an alert stating that new instance creation has failed to create new instances. You need to solve the instance creation problem. What should you do?",
    "options": {
      "A": "Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names.",
      "B": "Create an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template.",
      "C": "Verify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template.",
      "D": "Delete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\nC is eliminated because you cannot update or modify an existing instance template. It is immutable.\nhttps://cloud.google.com/compute/docs/instance-templates#how_to_update_instance_templates\nD is eliminated because you cannot delete an instance template if a managed instance group references it.\nhttps://cloud.google.com/compute/docs/instance-templates/get-list-delete-instance-templates#delete_an_instance_template\nB is eliminated because you cannot set different/custom name for persistent disk in an instance template.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-templates/create#--create-disk\nSo we left with A. Therefore A is the correct answer. Although, it should set the disks.autoDelete property to true for completeness. yodaforce 10Â months ago\nA is correct answer\nauto-delete If set to yes, the persistent disk is automatically deleted when the instance is deleted. However, if you detach the disk from the instance, deleting the instance doesn't delete the disk.\nThe default value is yes.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/instance-templates/create#--create-disk"
      },
      {
        "index": 2,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nC is corrct answer"
      },
      {
        "index": 3,
        "text": "Anonymous: Dewansh 2Â years, 1Â month ago\nSelected Answer: A\ninstance templates are immutable so can not modify or update"
      },
      {
        "index": 4,
        "text": "Anonymous: recluse7 2Â years, 2Â months ago\nSelected Answer: C\nWhen a managed instance group raises an alert indicating that new instance creation has failed, it's important to investigate the issue. This issue is typically related to the configuration of the instance template used by the group. Therefore, the first step is to verify that the instance template being utilized by the instance group contains valid syntax. This includes checking all settings such as machine type, boot disk, and any custom configurations. Valid syntax ensures that the instances created adhere to the specified configurations."
      },
      {
        "index": 5,
        "text": "Anonymous: Ell89 2Â years, 3Â months ago\nrepeated question"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: C\nExisting instance group has already template so no requirement to create new one. Problem is with persistent disk so delete it and configure autodelete solves problem now and in future 3arle 2Â years, 5Â months ago\ni was wrong, templates are immutable so you cannot update them, so option A is valid"
      },
      {
        "index": 7,
        "text": "Anonymous: happydays 2Â years, 5Â months ago\nSelected Answer: A\nTHIS IS A"
      }
    ]
  },
  {
    "id": 146,
    "source": "examtopics",
    "question": "You have created an application that is packaged into a Docker image. You want to deploy the Docker image as a workload on Google Kubernetes Engine. What should you do?",
    "options": {
      "A": "Upload the image to Cloud Storage and create a Kubernetes Service referencing the image.",
      "B": "Upload the image to Cloud Storage and create a Kubernetes Deployment referencing the image.",
      "C": "Upload the image to Artifact Registry and create a Kubernetes Service referencing the image.",
      "D": "Upload the image to Artifact Registry and create a Kubernetes Deployment referencing the image."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: D\nA and B are eliminated because Cloud Storage is not a preferred place for storing docker images.\nhttps://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images\nC is eliminated because Kubernetes Service is responsible for networking and connectivity between pods and external entities.\nKubernetes Deployment is responsible for deploying and managing an application (running in pods) on your GKE cluster.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/deploying-workloads-overview#stateless_applications"
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nD is the best answer."
      },
      {
        "index": 3,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: D\nbest choice"
      },
      {
        "index": 4,
        "text": "Anonymous: pritampanda1988 2Â years, 5Â months ago\nSelected Answer: D\nArtifact Registry is a fully managed container registry that integrates seamlessly with Google Kubernetes Engine and other Google Cloud services. By uploading the Docker image to Artifact Registry, you can create a Kubernetes Deployment that references the image stored in Artifact Registry. This ensures that Kubernetes can pull the image from a trusted and managed source, while the Deployment manages the deployment and scaling of the application pods based on the image."
      }
    ]
  },
  {
    "id": 147,
    "source": "examtopics",
    "question": "You are using Looker Studio to visualize a table from your data warehouse that is built on top of BigQuery. Data is appended to the data warehouse during the day. At night, the daily summary is recalculated by overwriting the table. You just noticed that the charts in Looker Studio are broken, and you want to analyze the problem. What should you do?",
    "options": {
      "A": "In Cloud Logging, create a filter for your Looker Studio report.",
      "B": "Use the open source CLI tool, Snapshot Debugger, to find out why the data was not refreshed correctly.",
      "C": "Review the Error Reporting page in the Google Cloud console to find any errors.",
      "D": "Use the BigQuery interface to review the nightly job and look for any errors."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: joao_01 Highly Voted 2Â years, 4Â months ago\nSelected Answer: D\nThis question appear before. I would go with D, altough the question before had opinions between the Cloud Logging and Bigquery options. Despite this i think that D is most appropriate for this case."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nD is the best answer."
      },
      {
        "index": 3,
        "text": "Anonymous: elviskimutai 2Â years, 3Â months ago\nD is the answer"
      },
      {
        "index": 4,
        "text": "Anonymous: Mallu_Mounika 2Â years, 5Â months ago\nSelected Answer: D\nD is right"
      },
      {
        "index": 5,
        "text": "Anonymous: happydays 2Â years, 5Â months ago\nSelected Answer: D\nD is correct"
      }
    ]
  },
  {
    "id": 148,
    "source": "examtopics",
    "question": "You have a batch workload that runs every night and uses a large number of virtual machines (VMs). It is fault-tolerant and can tolerate some of the VMs being terminated. The current cost of VMs is too high. What should you do?",
    "options": {
      "A": "Run a test using simulated maintenance events. If the test is successful, use Spot N2 Standard VMs when running future jobs.",
      "B": "Run a test using simulated maintenance events. If the test is successful, use N2 Standard VMs when running future jobs.",
      "C": "Run a test using a managed instance group. If the test is successful, use N2 Standard VMs in the managed instance group when running future jobs.",
      "D": "Run a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Ell89 Highly Voted 2Â years, 3Â months ago\nSelected Answer: A\nA - spot VMs for fault tolerant workloads"
      },
      {
        "index": 2,
        "text": "Anonymous: halifax Most Recent 1Â year, 1Â month ago\nSelected Answer: A\nWatch out, Spot VM, the new compute instance service from Google does not have a minimum or maximum runtime, this is unlike the previous offerings, preemptible VM, which can only run for 24 hours."
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nYou can use Spot VMs in your clusters and node pools to run stateless, batch, or fault-tolerant workloads that can tolerate disruptions caused by the ephemeral nature of Spot VMs."
      },
      {
        "index": 4,
        "text": "Anonymous: guru_ji 1Â year, 11Â months ago\nSelected Answer: A\nA is correct."
      },
      {
        "index": 5,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: A\nIts A, makes sense"
      },
      {
        "index": 6,
        "text": "Anonymous: ExamsFR 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer as Spot VMs are highly affordable"
      },
      {
        "index": 8,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: A\nHere, the keywords are batch workloads, large number of VMs, tolerate some VMs being terminated, and addressing cost of VMs. Spot VMs have significant discounts than on-demand VMs, therefore A is the correct answer.\nhttps://cloud.google.com/compute/docs/instances/spot"
      },
      {
        "index": 9,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: A\nSpot VMs are highly affordable compute instances suitable for batch jobs and fault-tolerant workloads. Spot VMs offer the same machine types, options, and performance as regular compute instances. If your applications are fault tolerant and can withstand possible instance preemptions, then Spot instances can reduce your Compute Engine costs by up to 91%!"
      },
      {
        "index": 10,
        "text": "Anonymous: gpais 2Â years, 5Â months ago\nSelected Answer: A\nA definitely Alberto06 2Â years, 5Â months ago\nPlease , put the reference, thanks."
      }
    ]
  },
  {
    "id": 149,
    "source": "examtopics",
    "question": "You created several resources in multiple Google Cloud projects. All projects are linked to different billing accounts. To better estimate future charges, you want to have a single visual representation of all costs incurred. You want to include new cost data as soon as possible. What should you do?",
    "options": {
      "A": "Fill all resources in the Pricing Calculator to get an estimate of the monthly cost.",
      "B": "Use the Reports view in the Cloud Billing Console to view the desired cost information.",
      "C": "Visit the Cost Table page to get a CSV export and visualize it using Looker Studio.",
      "D": "Configure Billing Data Export to BigQuery and visualize the data in Looker Studio."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ExamsFR Highly Voted 2Â years, 4Â months ago\nSelected Answer: D\nD is the correct answer\n\"single visual representation of all costs incurred\""
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nCloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage, cost estimates, and pricing data) automatically throughout the day to a BigQuery dataset that you specify. Then you can access your Cloud Billing data from BigQuery for detailed analysis, or use a tool like Looker Studio to visualize your data. You can also use this export method to export data to a JSON file."
      },
      {
        "index": 3,
        "text": "Anonymous: yomi95 1Â year, 3Â months ago\nAnswer D\nSame answer is for Q-201 but different question parameters (multiple projects, same billing account, dynamically calculated cost visualizations)"
      },
      {
        "index": 4,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: D\nVisualize costs = BQ and Looker"
      },
      {
        "index": 5,
        "text": "Anonymous: guru_ji 1Â year, 11Â months ago\nSelected Answer: D\nD is correct."
      },
      {
        "index": 6,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: D\nIts D, most appropriate"
      },
      {
        "index": 7,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: D\nA and C are straightaway eliminated.\nCloud Billing Reports displays a chart that plots usage costs for all projects linked to a Cloud Billing account, but projects should be linked to a single Cloud Billing account. So, C is also eliminated.\nhttps://cloud.google.com/billing/docs/how-to/reports\nYou can combine Cloud Billing data export to BigQuery with Looker Studio to stay up to date on your Google Cloud costs.\nhttps://cloud.google.com/billing/docs/how-to/visualize-data"
      },
      {
        "index": 8,
        "text": "Anonymous: gpais 2Â years, 5Â months ago\nSelected Answer: D\nWe want to aggregate the costs for multiple billing accounts"
      }
    ]
  },
  {
    "id": 150,
    "source": "examtopics",
    "question": "Your company has a large quantity of unstructured data in different file formats. You want to perform ETL transformations on the data. You need to make the data accessible on Google Cloud so it can be processed by a Dataflow job. What should you do?",
    "options": {
      "A": "Upload the data to BigQuery using the bq command line tool.",
      "B": "Upload the data to Cloud Storage using the gcloud storage command.",
      "C": "Upload the data into Cloud SQL using the import function in the Google Cloud console.",
      "D": "Upload the data into Cloud Spanner using the import function in the Google Cloud console."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: joao_01 1Â year, 4Â months ago\nSelected Answer: B\nIts B, non structure data"
      },
      {
        "index": 2,
        "text": "Anonymous: ExamsFR 1Â year, 4Â months ago\nSelected Answer: B\n\"unstructured data in different file formats\""
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nFor unstrucutrred data cloud striooage is the right answer"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: B\nThe question is asking about unstructured data. So, options A, C & E are eliminated.\nB is the correct answer."
      },
      {
        "index": 5,
        "text": "Anonymous: Cherrycardo 1Â year, 5Â months ago\nSelected Answer: B\nUnstructured is the keyword in this questions. All possible answers are structured, but Cloud Storage."
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: B\nonly B is for unstructured data"
      }
    ]
  },
  {
    "id": 151,
    "source": "examtopics",
    "question": "You have deployed an application on a single Compute Engine instance. The application writes logs to disk. Users start reporting errors with the application. You want to diagnose the problem. What should you do?",
    "options": {
      "A": "Navigate to Cloud Logging and view the application logs.",
      "B": "Configure a health check on the instance and set a â€œconsecutive successesâ€ Healthy threshold value of 1.",
      "C": "Connect to the instanceâ€™s serial console and read the application logs.",
      "D": "Install and configure the Ops agent and view the logs from Cloud Logging."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: qannik Highly Voted 1Â year, 11Â months ago\nSelected Answer: D\nI would go with D.\nBy default there is no logs agent installed on a compute instance.\nSo first you will have to install the Ops Agent and after a few minutes the logs will be visible in Cloud logging"
      },
      {
        "index": 2,
        "text": "Anonymous: 3arle Highly Voted 1Â year, 11Â months ago\nSelected Answer: D\nhttps://cloud.google.com/logging/docs/logging-gce-quickstart"
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 1Â week ago\nSelected Answer: D\nAs in [1], it is walking us through how to collect and view syslog logs collected from an Apache web server installed on a Compute Engine virtual machine instance (An application running on VM as this question says) by using the Ops Agent. You can use a process similar to the one in this quickstart to monitor other third-party applications.\nIn this quickstart, you do the following:"
      },
      {
        "index": 1,
        "text": "Create a Compute Engine VM instance and install the Ops Agent."
      },
      {
        "index": 2,
        "text": "Install an Apache web server."
      },
      {
        "index": 3,
        "text": "Configure the Ops Agent for the Apache web server."
      },
      {
        "index": 4,
        "text": "View your logs in the Logs Explorer."
      },
      {
        "index": 5,
        "text": "Create a log-based alert."
      },
      {
        "index": 6,
        "text": "Test your alert.\nWith that in mind, I will go for D"
      },
      {
        "index": 4,
        "text": "Anonymous: ccpmad 1Â year, 2Â months ago\nTelemetry is not application logs. Even if you install ops agent, you will not be able to consult the logs that the application writes to the instance disk. You have to enter the instance to inspect those logs, because those logs are not saved by the ops agent in gcp ccpmad 1Â year, 2Â months ago\nThe question is tricky, since the application starts to work poorly, that does not mean that we find the reason in the application logs, we have to look at the logs (telemetry) of the instance to determine if it is a problem of the GCP infrastructure, so first have the operations agent installed and sending metrics"
      }
    ]
  },
  {
    "id": 152,
    "source": "examtopics",
    "question": "You recently received a new Google Cloud project with an attached billing account where you will work. You need to create instances, set firewalls, and store data in Cloud Storage. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Use the gcloud CLI services enable cloudresourcemanager.googleapis.com command to enable all resources.",
      "B": "Use the gcloud services enable compute.googleapis.com command to enable Compute Engine and the gcloud services enable storage-api.googleapis.com command to enable the Cloud Storage APIs.",
      "C": "Open the Google Cloud console and enable all Google Cloud APIs from the API dashboard.",
      "D": "Open the Google Cloud console and run gcloud init --project in a Cloud Shell."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: VijKall Highly Voted 2Â years, 2Â months ago\nSelected Answer: B\nB is correct, need to enable API first thing before you can use any services.\nCan be done using Google Console or using CLI."
      },
      {
        "index": 2,
        "text": "Anonymous: yomi95 Most Recent 1Â year, 3Â months ago\nAnswer B.\nAs for Firewall, after enabling the compute engine api, firewall can be setup in CLI. The Compute Engine API encompasses the functionality for configuring network resources, including firewalls."
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nBut gcloud services enable storage-api.googleapis.com enabled by default. Maybe it another answer than B?"
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: B\nEnable correspondent APIs. Its B"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: B\nAt first you need to enable API"
      }
    ]
  },
  {
    "id": 153,
    "source": "examtopics",
    "question": "Your application development team has created Docker images for an application that will be deployed on Google Cloud. Your team does not want to manage the infrastructure associated with this application. You need to ensure that the application can scale automatically as it gains popularity. What should you do?",
    "options": {
      "A": "Create an instance template with the container image, and deploy a Managed Instance Group with Autoscaling.",
      "B": "Upload Docker images to Artifact Registry, and deploy the application on Google Kubernetes Engine using Standard mode.",
      "C": "Upload Docker images to the Cloud Storage, and deploy the application on Google Kubernetes Engine using Standard mode.",
      "D": "Upload Docker images to Artifact Registry, and deploy the application on Cloud Run."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Captain1212 Highly Voted 1Â year, 4Â months ago\nSelected Answer: D\nD is the correct answer , as question says your team dont want to manage the infrastrucutre associated withthe application this is offered by the cloudrun"
      },
      {
        "index": 2,
        "text": "Anonymous: joao_01 Highly Voted 1Â year, 4Â months ago\nSelected Answer: D\nIts D. GKE standard the nodes are managed by user. So D is correct."
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua Most Recent 11Â months, 3Â weeks ago\nSelected Answer: D\nBut don't forget..Cloud Run SLA Monthly Uptime Percentage to Customer of at least 99.95%, which results in 22min downtime per month lol"
      },
      {
        "index": 4,
        "text": "Anonymous: nnecode 1Â year, 4Â months ago\nSelected Answer: D\nD. Upload Docker images to Artifact Registry and deploy the application on Cloud Run."
      },
      {
        "index": 5,
        "text": "Anonymous: AkshayJangwal 1Â year, 4Â months ago\nKey Hint : Your team does not want to manage the infrastructure associated with this application\nThis is offered by Cloud Run, hence option D."
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: D\nAnswer is D.\nCloud Run is container-as-a-service offering from Google Cloud. You can deploy the containerized application directly on top of Google's infrastructure, and only when a request comes.\nhttps://cloud.google.com/run/docs/overview/what-is-cloud-run scanner2 1Â year, 4Â months ago\nYou don't need to worry about underlying infrastructure.\nA, C and E requires resources, virtual instance, GKE cluster provisioning and maintaining. so these are eliminated."
      },
      {
        "index": 7,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: D\nGKE Standard mode: You manage the underlying infrastructure, including configuring the individual nodes.\nInstance group - you manage the infrastructure as well\nso after elimination A,B,C stays D"
      },
      {
        "index": 8,
        "text": "Anonymous: happydays 1Â year, 5Â months ago\nSelected Answer: D\nIT'S D"
      },
      {
        "index": 9,
        "text": "Anonymous: gpais 1Â year, 5Â months ago\nSelected Answer: D\nOption D"
      }
    ]
  },
  {
    "id": 154,
    "source": "examtopics",
    "question": "You are migrating a business critical application from your local data center into Google Cloud. As part of your high-availability strategy, you want to ensure that any data used by the application will be immediately available if a zonal failure occurs. What should you do?",
    "options": {
      "A": "Store the application data on a zonal persistent disk. Create a snapshot schedule for the disk. If an outage occurs, create a new disk from the most recent snapshot and attach it to a new VM in another zone.",
      "B": "Store the application data on a zonal persistent disk. If an outage occurs, create an instance in another zone with this disk attached.",
      "C": "Store the application data on a regional persistent disk. Create a snapshot schedule for the disk. If an outage occurs, create a new disk from the most recent snapshot and attach it to a new VM in another zone.",
      "D": "Store the application data on a regional persistent disk. If an outage occurs, create an instance in another zone with this disk attached."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: carlalap Highly Voted 1Â year, 8Â months ago\nAnswer is D. When your are using regional persistent disks, the data is automatically replicated to two replicas without the requirement of maintaining application replication. There is no need to use a snapshot."
      },
      {
        "index": 2,
        "text": "Anonymous: wota Most Recent 1Â year, 1Â month ago\nSelected Answer: D\nRegional persistent disks provides active-active disk replication across two zones in same region. So the snapshots are not needed."
      },
      {
        "index": 3,
        "text": "Anonymous: VijKall 1Â year, 8Â months ago\nSelected Answer: C\nAnswer is C and not D.\nUse regional PD with regular snapshot for data redundancy.\nC works, but data will be stale, as there is no latest snapshot.\nA&B are out, as they are using regular PD and not regional PD. VijKall 1Â year, 8Â months ago\nI come back to correct his. Answer is D. Regional PD will be accessible even when zone fails and app can be brought up using same disk in different zone."
      },
      {
        "index": 4,
        "text": "Anonymous: recluse7 1Â year, 8Â months ago\nSelected Answer: C\nOption C provides the best combination of data redundancy and recovery speed, making it the ideal choice for high-availability scenarios. While other options like option A (zonal persistent disk with snapshots) or option D (regional disk with instances in other zones) can also work, they may not offer the same level of efficiency and data protection as option C. Option B (zonal disk without replication) is less desirable because it lacks data redundancy and necessitates manual intervention to restore data in case of a zonal failure."
      },
      {
        "index": 5,
        "text": "Anonymous: joao_01 1Â year, 10Â months ago\nSelected Answer: D\nI thought first it was C. Then i saw Google documentation and for sure the answer is D."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD is the correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: DannSecurity 1Â year, 10Â months ago\nC,\nIf you are designing robust systems or high availability services on Compute Engine, use regional Persistent Disk combined with other best practices such as backing up your data using snapshots."
      },
      {
        "index": 8,
        "text": "Anonymous: Speridian 1Â year, 11Â months ago\nIt should be D."
      },
      {
        "index": 9,
        "text": "Anonymous: gfalconia 1Â year, 11Â months ago\nSelected Answer: D\nD, C doesn't make sense because article explicitly says that: 'During the failover, the regional persistent disk that is synchronously replicated to the secondary zone is force attached to the standby VM by the application control plane, and all traffic is directed to that VM based on health check signals.'\nhttps://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk#failover"
      },
      {
        "index": 10,
        "text": "Anonymous: _cloudio_ 1Â year, 11Â months ago\nSelected Answer: D\nThe benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region."
      }
    ]
  },
  {
    "id": 155,
    "source": "examtopics",
    "question": "The DevOps group in your organization needs full control of Compute Engine resources in your development project. However, they should not have permission to create or update any other resources in the project. You want to follow Googleâ€™s recommendations for setting permissions for the DevOps group. What should you do?",
    "options": {
      "A": "Grant the basic role roles/viewer and the predefined role roles/compute.admin to the DevOps group.",
      "B": "Create an IAM policy and grant all compute.instanceAdmin.* permissions to the policy. Attach the policy to the DevOps group.",
      "C": "Create a custom role at the folder level and grant all compute.instanceAdmin.* permissions to the role. Grant the custom role to the DevOps group.",
      "D": "Grant the basic role roles/editor to the DevOps group."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: VijKall Highly Voted 2Â years, 2Â months ago\nSelected Answer: A\nAnswer is A.\nroles/viewer gives read only access on Project, so it does not create/update any resources.\nroles/compute.admin gives full access to Compute Engine resources."
      },
      {
        "index": 2,
        "text": "Anonymous: carlalap Highly Voted 2Â years, 2Â months ago\nAnswer is C."
      },
      {
        "index": 1,
        "text": "The DevOps group needs full control of Compute Engine resources in your development project. --> So, we grants permissions to create and update Compute Engine instances and their related resources, such as disks, images, and snapshots.\nA// Create a custom role at the folder level and grant all compute.instanceAdmin.* permissions to the role."
      },
      {
        "index": 2,
        "text": "They should not have permission to create or update any other resources in the project. --> We do not grant permissions to create or update any other resources in the project, such as Cloud Storage buckets, Cloud Functions, or BigQuery datasets.\nA// Grant the custom role to the DevOps group. carlalap 2Â years, 2Â months ago\nFurthermore, Google recommends using custom roles to grant the minimum set of permissions that users need to perform their tasks. vaibhavCodian 2Â years, 1Â month ago\ncompletely incorrect\nCompute Admin\n(roles/compute.admin)\nFull control of all Compute Engine resources.\nIf the user will be managing virtual machine instances that are configured to run as a service account, you must also grant the roles/iam.serviceAccountUser role. goalDigger 2Â years ago\nWe can only grant a custom role within the project or organization in which we created it. We cannot grant custom roles on other projects or organizations, or on resources within other projects or organizations. Note: We cannot define custom roles at the folder level. So, C cannot be the answer. ccpmad 1Â year, 7Â months ago\nok, yes, we can not create a custom role at folder level, but we can create the custom role at organization level, and then, go to IAM at folder level, and use that custom role that give permissions at folder level. I have just try it and works.\nMoreover, it is not possible A, because question says that Dev group has to have permissions in development project. I think question is not correctly written. Becuase A answer allow Dev Grop to create resources in any project in the organization.\nBut finally, knowing the question is not writteng correctly, in the exam, I think I will bet for A. ccpmad 1Â year, 7Â months ago\nYes, I have just read another time answer C. C is not possible because says that creation of the custom role is at folder level. That is not possible.\nIn real life, we would create the custom role at organization level, and the use it at folder level, so Dev group only have the permissions in their dev projecto.\nFor this question, in an exam, we have to pick A.\nThank you and good luck"
      },
      {
        "index": 3,
        "text": "Anonymous: josecouva Most Recent 3Â weeks ago\nSelected Answer: B\nGoogle's recommendation is the Principle of Least Privilege. The roles/viewer role is too broad and allows viewing of things other than VMs."
      },
      {
        "index": 4,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: A\nThe correct answer is A. Take it or leave it"
      },
      {
        "index": 5,
        "text": "Anonymous: ngeorgiev2 1Â year, 10Â months ago\nSelected Answer: A\n\"roles/compute.admin\" - Full control of all Compute Engine resources.\n\"roles/compute.instanceAdmin\" - If the user will be managing virtual machine instances that are configured to run as a service account, you must also grant the roles/iam.serviceAccountUser role.\nCorrect answer is definitely A"
      },
      {
        "index": 6,
        "text": "Anonymous: sinh 2Â years ago\nSelected Answer: B\nGoogle recommends using custom roles ccpmad 1Â year, 8Â months ago\nIAM policy is not for a project, is for organization, it is not B"
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: A\nA. Grant roles/viewer and roles/compute.admin:\nâ€¢ The roles/viewer role provides read-only access to most Google Cloud services\nâ€¢ The roles/compute.admin role gives full control over Compute Engine resources, which is appropriate for the DevOps group's needs."
      },
      {
        "index": 8,
        "text": "Anonymous: Peto12 2Â years, 1Â month ago\nSelected Answer: B\nThis one is very tricky, by my opinion correct answer is B.\nThis wildcard at the end is important \"grant all compute.instanceAdmin.*\" that means that you need to assign two policies that are already there:\n- roles/compute.instanceAdmin.v1\n- roles/compute.instanceAdmin (beta)\nSo if the user has compute.instanceAdmin.v1 he will have full compute access without adding the additional one \"roles/iam.serviceAccountUser\". Also another argument against answer A is the Google recommendations to use the basic roles only when there is no predefined roles, and this is valid for all kind of environments not just production. kuracpalac 1Â year, 10Â months ago\nI selected B as well due to the basic roles being mentioned in A, which Google says it's a no no as they are too broad. ccpmad 1Â year, 8Â months ago\niam policy is for organization, this question is for a project. So it is not B"
      }
    ]
  },
  {
    "id": 156,
    "source": "examtopics",
    "question": "Your team is running an on-premises ecommerce application. The application contains a complex set of microservices written in Python, and each microservice is running on Docker containers. Configurations are injected by using environment variables. You need to deploy your current application to a serverless Google Cloud cloud solution. What should you do?",
    "options": {
      "A": "Use your existing CI/CD pipeline. Use the generated Docker images and deploy them to Cloud Run. Update the configurations and the required endpoints.",
      "B": "Use your existing continuous integration and delivery (CI/CD) pipeline. Use the generated Docker images and deploy them to Cloud Function. Use the same configuration as on-premises.",
      "C": "Use the existing codebase and deploy each service as a separate Cloud Function. Update the configurations and the required endpoints.",
      "D": "Use your existing codebase and deploy each service as a separate Cloud Run. Use the same configurations as on-premises."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Captain1212 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer , use your existing CI/cd pipeline and update the configuratons and the required endpoints"
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: A\nCloud Run is better than Cloud Functions and we have to update the configurations."
      },
      {
        "index": 3,
        "text": "Anonymous: nmnm22 2Â years, 2Â months ago\nSelected Answer: A\nA seems the most effecient"
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: A\nI think its A"
      },
      {
        "index": 5,
        "text": "Anonymous: nnecode 2Â years, 4Â months ago\nSelected Answer: D\nI vote D"
      },
      {
        "index": 6,
        "text": "Anonymous: Cherrycardo 2Â years, 5Â months ago\nSelected Answer: D\nhttps://cloud.google.com/run/docs/configuring/services/environment-variables\n\"The environment variables defined in the container runtime contract are reserved and cannot be set. In particular, the PORT environment variable is injected inside your container by Cloud Run. You should not set it yourself.\"\nHence, by \"using the same configurations of on-premise\" you are just using the environment variables already present on the container. Cherrycardo 2Â years, 5Â months ago\nI was wrong. The right answer is A. The current approach to loadi Docker images into Artifact Registry (formerly Container Registry), is by using CI/CD Pipelines."
      },
      {
        "index": 7,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: A\napp was written for docker image, it likely should be rewritten for cloud run"
      },
      {
        "index": 8,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: A\nI think it's A.\nIt can't be D because you can not use the same configuration as on-premise."
      },
      {
        "index": 9,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: D\nI vote for D"
      },
      {
        "index": 10,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nI vote for D"
      }
    ]
  },
  {
    "id": 157,
    "source": "examtopics",
    "question": "You are running multiple microservices in a Kubernetes Engine cluster. One microservice is rendering images. The microservice responsible for the image rendering requires a large amount of CPU time compared to the memory it requires. The other microservices are workloads that are optimized for n2-standard machine types. You need to optimize your cluster so that all workloads are using resources as efficiently as possible. What should you do?",
    "options": {
      "A": "Assign the pods of the image rendering microservice a higher pod priority than the other microservices.",
      "B": "Create a node pool with compute-optimized machine type nodes for the image rendering microservice. Use the node pool with general-purpose machine type nodes for the other microservices.",
      "C": "Use the node pool with general-purpose machine type nodes for the image rendering microservice. Create a node pool with compute-optimized machine type nodes for the other microservices.",
      "D": "Configure the required amount of CPU and memory in the resource requests specification of the image rendering microservice deployment. Keep the resource requests for the other microservices at the default."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: joao_01 Highly Voted 1Â year, 10Â months ago\nSelected Answer: B\nIts B, question appear before"
      },
      {
        "index": 2,
        "text": "Anonymous: pzacariasf7 Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nrepeated question, the answer is B."
      },
      {
        "index": 3,
        "text": "Anonymous: sinh 1Â year, 6Â months ago\nSame as No.196."
      },
      {
        "index": 4,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: B\nrepeated question"
      },
      {
        "index": 5,
        "text": "Anonymous: qannik 1Â year, 11Â months ago\nSelected Answer: B\nB has logic"
      }
    ]
  },
  {
    "id": 158,
    "source": "examtopics",
    "question": "You are working in a team that has developed a new application that needs to be deployed on Kubernetes. The production application is business critical and should be optimized for reliability. You need to provision a Kubernetes cluster and want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Create a GKE Autopilot cluster. Enroll the cluster in the rapid release channel.",
      "B": "Create a GKE Autopilot cluster. Enroll the cluster in the stable release channel.",
      "C": "Create a zonal GKE standard cluster. Enroll the cluster in the stable release channel.",
      "D": "Create a regional GKE standard cluster. Enroll the cluster in the rapid release channel."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: nmnm22 Highly Voted 1Â year, 2Â months ago\n\"recommended practices\" --> Autopilot\n\"optimized for reliability\" --> Stable release"
      },
      {
        "index": 2,
        "text": "Anonymous: Captain1212 Highly Voted 1Â year, 4Â months ago\nSelected Answer: B\nAutopilot cluster is more relaible and gives more time to fix"
      },
      {
        "index": 3,
        "text": "Anonymous: hisafaj159 Most Recent 1Â year ago\nWhy not C. its saying business critical and should be optimized for reliability. So in case of any disaster it is more accurate, please correct me if i am wrong."
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 1Â year, 4Â months ago\nSelected Answer: B\nI selected B"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: B\nAutopilot is more reliable and stable release gives more time to fix issues in new version of GKE"
      },
      {
        "index": 6,
        "text": "Anonymous: qannik 1Â year, 5Â months ago\nSelected Answer: B\nAutopilot cluster is recommended by Google"
      }
    ]
  },
  {
    "id": 159,
    "source": "examtopics",
    "question": "You are responsible for a web application on Compute Engine. You want your support team to be notified automatically if users experience high latency for at least 5 minutes. You need a Google-recommended solution with no development cost. What should you do?",
    "options": {
      "A": "Export Cloud Monitoring metrics to BigQuery and use a Looker Studio dashboard to monitor your web applicationâ€™s latency.",
      "B": "Create an alert policy to send a notification when the HTTP response latency exceeds the specified threshold.",
      "C": "Implement an App Engine service which invokes the Cloud Monitoring API and sends a notification in case of anomalies.",
      "D": "Use the Cloud Monitoring dashboard to observe latency and take the necessary actions when the response latency exceeds the specified threshold."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: rahulrauki Highly Voted 1Â year, 9Â months ago\nSelected Answer: B\n\"No development cost\" : BigQuery and GAE out\n\"Automatically\" : Option D out (\"take necessary actions\")\nLeft with option B"
      },
      {
        "index": 2,
        "text": "Anonymous: Jayz1992 Most Recent 1Â year ago\nSelected Answer: C\nSet up an Alerting Policy:\nGo to the Cloud Monitoring Console.\nNavigate to Alerting under the \"Monitoring\" section.\nCreate a new alerting policy.\nChoose a metric that reflects the latency you want to monitor (e.g., http/request_latencies).\nSet the threshold for high latency (e.g., a latency value that signifies an issue).\nDefine the duration of the condition (e.g., 5 minutes). This will ensure the alert triggers only if the latency remains high for a sustained period.\nSet up Notification Channels:\nUnder the alerting policy, configure a notification channel to send alerts to your support team (e.g., via email, SMS, or through a Google Cloud Pub/Sub topic if you want more advanced options).\nYou can configure multiple notification channels to ensure the support team is notified promptly.\nso if we have to send notifications (email, msg etc ) to entire team, then I think C is right option"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: B\nB. Cloud Monitoring's alerting policies are designed specifically for this scenario"
      },
      {
        "index": 4,
        "text": "Anonymous: ExamsFR 1Â year, 10Â months ago\nSelected Answer: B\n\"You need a Google-recommended solution with no development cost\""
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: B\nhttps://cloud.google.com/monitoring/alerts#alerting-example"
      },
      {
        "index": 6,
        "text": "Anonymous: gpais 1Â year, 11Â months ago\nSelected Answer: B\nB seems to be the best answer"
      }
    ]
  },
  {
    "id": 160,
    "source": "examtopics",
    "question": "You have an on-premises data analytics set of binaries that processes data files in memory for about 45 minutes every midnight. The sizes of those data files range from 1 gigabyte to 16 gigabytes. You want to migrate this application to Google Cloud with minimal effort and cost. What should you do?",
    "options": {
      "A": "Create a container for the set of binaries. Use Cloud Scheduler to start a Cloud Run job for the container.",
      "B": "Create a container for the set of binaries. Deploy the container to Google Kubernetes Engine (GKE) and use the Kubernetes scheduler to start the application.",
      "C": "Upload the code to Cloud Functions. Use Cloud Scheduler to start the application.",
      "D": "Lift and shift to a VM on Compute Engine. Use an instance schedule to start and stop the instance."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: taylz876 Highly Voted 1Â year, 9Â months ago\nSelected Answer: D\nHere's why option D is the most appropriate:\n->Compute Engine: Compute Engine provides virtual machines (VMs) that closely resemble traditional on-premises servers. It allows you to migrate your existing application as-is to the Google Cloud platform.\n->Instance Scheduling: You can schedule the VM instance to start and stop at specific times, such as midnight, to align with your existing processing schedule. This ensures that the application runs at the required time, similar to the on-premises setup.\n->Minimal Effort and Cost: The \"lift and shift\" approach minimizes the need for code modifications or containerization, reducing migration complexity. It also allows you to use the same binaries and configurations as your on-premises setup, saving development effort. You only pay for the VM's compute resources when it's running, making it cost-effective."
      },
      {
        "index": 2,
        "text": "Anonymous: joao_01 Highly Voted 1Â year, 10Â months ago\nSelected Answer: D\nThis one is a tough one. Ill consider this in my answers:\nCost --> Both are more the same (the process will run at the same frequency)\nEffort --> Create the image in A takes more effort then to option D.\nWith this in mind ill choose D. (before i was choosing A). VijKall 1Â year, 8Â months ago\nCost will reduce as D option is starting VM at midnight for the job and stop after completion."
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua Most Recent 11Â months, 3Â weeks ago\nSelected Answer: D\nPretty easy, containers designed for something light, 1gb-16gb is not light\nminimal effort -> lift and shift"
      },
      {
        "index": 4,
        "text": "Anonymous: halifax 1Â year, 1Â month ago\nD - perfect solution, but expensive!\nCloud run maximum execution limit time 60 mins (very close to the 45 mins..risky!)\nThe maximum execution limit time for the cloud function is 9 mins (less than 45)"
      },
      {
        "index": 5,
        "text": "Anonymous: kuracpalac 1Â year, 4Â months ago\nFor my 2c I think I would have selected A, but D is a possibility.\nA because in the long run Cloud Run should cost less than D I believe, as it would take less CPU time through a period of time if data is from 1-16GB.\nD could be because it potentially requires \"less\" effort compared to CR."
      },
      {
        "index": 6,
        "text": "Anonymous: carlalap 1Â year, 7Â months ago\nI think the answer is C.\nCloud Run is ideal for applications that have short running times and variable workloads, like this data analytics application. Also, Lifting and shifting the application to a VM on Compute Engine would require to manage the VM by a person, including tasks like patching the operating system, managing security updates, and scaling the VM. This is more effort than using Cloud Run. carlalap 1Â year, 7Â months ago\nSorry, I must correct, I think it's A."
      },
      {
        "index": 7,
        "text": "Anonymous: VijKall 1Â year, 8Â months ago\nSelected Answer: D\nI vote for D."
      },
      {
        "index": 8,
        "text": "Anonymous: SinghAnc 1Â year, 9Â months ago\nSelected Answer: D\nD is the correct answer."
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD is the correct answer as it requires the minimoal effort"
      },
      {
        "index": 10,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: A\nA is correct answer."
      }
    ]
  },
  {
    "id": 161,
    "source": "examtopics",
    "question": "You used the gcloud container clusters command to create two Google Cloud Kubernetes (GKE) clusters: prod-cluster and dev-cluster.\n\nâ€¢ prod-cluster is a standard cluster.\nâ€¢ dev-cluster is an auto-pilot cluster.\n\nWhen you run the kubectl get nodes command, you only see the nodes from prod-cluster. Which commands should you run to check the node status for dev-cluster?",
    "options": {
      "A": "gcloud container clusters get-credentials dev-cluster",
      "B": "gcloud container clusters update -generate-password dev-cluster kubectl get nodes",
      "C": "kubectl config set-context dev-cluster",
      "D": "kubectl config set-credentials dev-cluster"
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: pritampanda1988 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\ngcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine"
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: A\ngcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine.\nhttps://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials"
      },
      {
        "index": 3,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: A\nIts the A"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer as it , updated the config file"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: A\ngcloud container clusters get-credentials updates a kubeconfig file with appropriate credentials and endpoint information to point kubectl at a specific cluster in Google Kubernetes Engine"
      },
      {
        "index": 6,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: A\nThe gcloud container clusters get-credentials command sets the Kubernetes context to the specified cluster (in this case, dev-cluster). This ensures that the subsequent kubectl commands will be executed against the dev-cluster.\nAfter setting the context, the kubectl get nodes command is used to retrieve the node status for the dev-cluster, showing the list of nodes in the cluster."
      },
      {
        "index": 7,
        "text": "Anonymous: Speridian 2Â years, 5Â months ago\nIt should be A"
      }
    ]
  },
  {
    "id": 162,
    "source": "examtopics",
    "question": "You recently discovered that your developers are using many service account keys during their development process. While you work on a long term improvement, you need to quickly implement a process to enforce short-lived service account credentials in your company. You have the following requirements:\n\nâ€¢ All service accounts that require a key should be created in a centralized project called pj-sa.\nâ€¢ Service account keys should only be valid for one day.\n\nYou need a Google-recommended solution that minimizes cost. What should you do?",
    "options": {
      "A": "Implement a Cloud Run job to rotate all service account keys periodically in pj-sa. Enforce an org policy to deny service account key creation with an exception to pj-sa.",
      "B": "Implement a Kubernetes CronJob to rotate all service account keys periodically. Disable attachment of service accounts to resources in all projects with an exception to pj-sa.",
      "C": "Enforce an org policy constraint allowing the lifetime of service account keys to be 24 hours. Enforce an org policy constraint denying service account key creation with an exception on pj-sa.",
      "D": "Enforce a DENY org policy constraint over the lifetime of service account keys for 24 hours. Disable attachment of service accounts to resources in all projects with an exception to pj-sa."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: qannik Highly Voted 1Â year, 5Â months ago\nSelected Answer: C\nYou can use an org policy to enforce a 24-hour lifetime for service account keys.\nYou can use an org policy to deny service account key creation, with an exception for the pj-sa project.\nThis is a Google-recommended solution and it is relatively inexpensive."
      },
      {
        "index": 2,
        "text": "Anonymous: joao_01 Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nIts C, makes sense"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nc is the coorect answer"
      },
      {
        "index": 4,
        "text": "Anonymous: scanner2 1Â year, 4Â months ago\nSelected Answer: C\nC is correct.\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#limit_key_expiry\nhttps://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation"
      },
      {
        "index": 5,
        "text": "Anonymous: 3arle 1Â year, 5Â months ago\nSelected Answer: C\nit should be C"
      },
      {
        "index": 6,
        "text": "Anonymous: niedobry 1Â year, 5Â months ago\nAnswer is C. Constraint: constraints/iam.serviceAccountKeyExpiryHours does not accept DENY values so D can not be correct."
      }
    ]
  },
  {
    "id": 163,
    "source": "examtopics",
    "question": "Your company is running a three-tier web application on virtual machines that use a MySQL database. You need to create an estimated total cost of cloud infrastructure to run this application on Google Cloud instances and Cloud SQL. What should you do?",
    "options": {
      "A": "Create a Google spreadsheet with multiple Google Cloud resource combinations. On a separate sheet, import the current Google Cloud prices and use these prices for the calculations within formulas.",
      "B": "Use the Google Cloud Pricing Calculator and select the Cloud Operations template to define your web application with as much detail as possible.",
      "C": "Implement a similar architecture on Google Cloud, and run a reasonable load test on a smaller scale. Check the billing information, and calculate the estimated costs based on the real load your system usually handles.",
      "D": "Use the Google Cloud Pricing Calculator to determine the cost of every Google Cloud resource you expect to use. Use similar size instances for the web server, and use your current on-premises machines as a comparison for Cloud SQL."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: rsvd Highly Voted 2Â years, 5Â months ago\nSelected Answer: D\nThere is no such thing called \"Cloud Operations template\" namanthony 9Â months, 3Â weeks ago\nAccess the Google Cloud Pricing Calculator. Click the â€œ+ Add to estimateâ€ button. In the search box, type â€œCloud Operationsâ€. Or more specifically: type each part like â€œLoggingâ€, â€œMonitoringâ€, â€œError Reportingâ€, or â€œTraceâ€. You will see items such as: Cloud Logging Cloud Monitoring Cloud Trace Cloud Error Reporting"
      },
      {
        "index": 2,
        "text": "Anonymous: qannik Highly Voted 2Â years, 5Â months ago\nSelected Answer: D\nGoogle Cloud Pricing Calculator, is the recommended approach for creating an estimated total cost of cloud infrastructure. By selecting the relevant Google Cloud resources (such as instances for web servers and Cloud SQL for the database), and specifying similar sizes and configurations, you can obtain a more accurate estimation of the costs."
      },
      {
        "index": 3,
        "text": "Anonymous: meh_33 Most Recent 1Â year, 5Â months ago\nSelected Answer: D\nWhere is Raaad for such a tough questions no comment . D"
      },
      {
        "index": 4,
        "text": "Anonymous: sukouto 1Â year, 10Â months ago\nNote to all: there is no such thing as a \"Cloud Operations Template\" => B is out."
      },
      {
        "index": 5,
        "text": "Anonymous: VijKall 2Â years, 2Â months ago\nSelected Answer: D\nGoogle Cloud Pricing Calculator is the recommended approach for cost estimation and you provide resources similar to what you see in on-premises for Web servers and add Cloud SQL as a managed service."
      },
      {
        "index": 6,
        "text": "Anonymous: ArtistS 2Â years, 2Â months ago\nD is correct. It can give u a more accurate figure."
      },
      {
        "index": 7,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: D\nIts D, try to simulate using what we have"
      },
      {
        "index": 8,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: D\nD is correct."
      },
      {
        "index": 9,
        "text": "Anonymous: ptapia_el 2Â years, 5Â months ago\nes la D"
      },
      {
        "index": 10,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: D\nit's D"
      }
    ]
  },
  {
    "id": 164,
    "source": "examtopics",
    "question": "You have a Bigtable instance that consists of three nodes that store personally identifiable information (PII) data. You need to log all read or write operations, including any metadata or configuration reads of this database table, in your companyâ€™s Security Information and Event Management (SIEM) system. What should you do?",
    "options": {
      "A": "â€¢ Navigate to Cloud Monitoring in the Google Cloud console, and create a custom monitoring job for the Bigtable instance to track all changes.",
      "B": "â€¢ Navigate to the Audit Logs page in the Google Cloud console, and enable Admin Write logs for the Bigtable instance.",
      "C": "â€¢ Navigate to the Audit Logs page in the Google Cloud console, and enable Data Read, Data Write and Admin Read logs for the Bigtable instance.",
      "D": "â€¢ Install the Ops Agent on the Bigtable instance during configuration."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: taylz876 Highly Voted 2Â years, 3Â months ago\nSelected Answer: C\nOption C is the most appropriate choice for capturing audit and data access logs from a Bigtable instance and sending them to your SIEM system.\n1) Enabling Data Read, Data Write, and Admin Read logs for the Bigtable instance ensures that you capture the relevant operations, including read and write operations, as well as administrative reads, in the audit logs.\n2) Creating a Pub/Sub topic as a Cloud Logging sink destination allows you to export the logs from Cloud Logging to Pub/Sub. This is a common approach for sending logs to external systems, including SIEMs.\n3) Adding your SIEM as a subscriber to the Pub/Sub topic ensures that the logs are forwarded to your SIEM system, allowing you to monitor and analyze them for security and compliance purposes.\nNB:A Cloud Logging sink destination is a configuration that specifies where logs collected by Google Cloud's Cloud Logging service should be sent or exported. It allows you to control the destination of logs generated by various Google Cloud services, such as Compute Engine, Cloud Storage, BigQuery, and more."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nhttps://cloud.google.com/bigtable/docs/audit-logging#permission-type"
      },
      {
        "index": 3,
        "text": "Anonymous: sinh 2Â years ago\nSelected Answer: C\nhttps://cloud.google.com/bigtable/docs/audit-logging"
      },
      {
        "index": 4,
        "text": "Anonymous: joao_01 2Â years, 4Â months ago\nSelected Answer: C\nIts C!"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC is the correct answer, as it helps you to read and write"
      },
      {
        "index": 6,
        "text": "Anonymous: scanner2 2Â years, 4Â months ago\nSelected Answer: C\nC is correct."
      },
      {
        "index": 7,
        "text": "Anonymous: 3arle 2Â years, 5Â months ago\nSelected Answer: C\nData Access audit logsâ€”except for BigQueryâ€”are disabled by default and you need to enable them"
      },
      {
        "index": 8,
        "text": "Anonymous: qannik 2Â years, 5Â months ago\nSelected Answer: B\nEnabling Admin Write logs for the Bigtable instance in Cloud Logging will capture administrative write actions on the Bigtable instance. This includes any configuration changes and metadata reads related to the Bigtable instance.\nCreating a Cloud Functions instance and configuring it to export logs from Cloud Logging to your SIEM allows you to take the captured logs and route them to your SIEM system in a format that your SIEM can understand. Cloud Functions can act as a serverless function to process and forward the logs to your SIEM using an appropriate method, such as sending them via an API or message queue."
      }
    ]
  },
  {
    "id": 165,
    "source": "examtopics",
    "question": "You want to set up a Google Kubernetes Engine cluster. Verifiable node identity and integrity are required for the cluster, and nodes cannot be accessed from the internet. You want to reduce the operational cost of managing your cluster, and you want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Deploy a private autopilot cluster.",
      "B": "Deploy a public autopilot cluster.",
      "C": "Deploy a standard public cluster and enable shielded nodes.",
      "D": "Deploy a standard private cluster and enable shielded nodes."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: scanner2 Highly Voted 2Â years, 4Â months ago\nSelected Answer: A\nIn a private cluster, nodes only have internal IP addresses, which means that nodes and Pods are isolated from the internet by default.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters\nShielded GKE Nodes provide strong, verifiable node identity and integrity to increase the security of Google Kubernetes Engine (GKE) nodes.\nNote: For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes"
      },
      {
        "index": 2,
        "text": "Anonymous: Timfdklfajlksdjlakf Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nscanner2 provided the correct answer."
      },
      {
        "index": 3,
        "text": "Anonymous: ashrafh 1Â year, 5Â months ago\nas per chatgpt\nOption A. Deploy a private autopilot cluster is a good choice because it combines:\nReduced Operational Costs: Google manages the infrastructure, scaling, and maintenance, minimizing your management overhead.\nEnhanced Security: Private Autopilot clusters use shielded nodes, ensuring verifiable node identity and integrity, and are not accessible from the internet.\nGoogle-Recommended Practices: Autopilot clusters follow best practices for performance and security with minimal configuration required from you."
      },
      {
        "index": 4,
        "text": "Anonymous: jithinlife 1Â year, 9Â months ago\nSelected Answer: D\nDeploying a standard private cluster and enabling shielded nodes would meet all the requirements. In a private cluster, nodes are not accessible from the internet by default, ensuring enhanced security. Enabling shielded nodes provides verifiable node identity and integrity, further strengthening the security measures. Additionally, following Google-recommended practices, such as using standard clusters instead of autopilot clusters, offers more control and helps reduce operational costs. BuenaCloudDE 1Â year, 6Â months ago\nShielded GKE Nodes feature is enabled by default. BuenaCloudDE 1Â year, 6Â months ago\nFor GKE Autopilot clusters."
      },
      {
        "index": 5,
        "text": "Anonymous: sukouto 1Â year, 10Â months ago\nSelected Answer: D\nReposting this subcomment because I believe most people are reading this incorrectly, and I want to contribute to the answers ratio:\nWhy is everyone so sure that \"operational cost\" refers to work-hours and not money? (i.e. \"operating costs\")\nFrom Wikipedia: Operating costs or operational costs, are the expenses which are related to the operation of a business, or to the operation of a device, component, piece of equipment or facility.\nThis question is asking to reduce the MONETARY cost. Standard costs less than Autopilot. Accordingly, the answer should be D. sukouto 1Â year, 10Â months ago\nFYI to all, the phrase \"operational cost\" is only found in two GCP documents (both blog articles, not official product documentation), and they use competing definitions... So this is a poorly worded question.\nThat said, since this was phrased as \"operational cost of *managing your cluster*\", I think I may have been incorrect. It seems perhaps this is indeed referring to the reduction of work-hours and manual effort needed to manage the cluster."
      },
      {
        "index": 6,
        "text": "Anonymous: sukouto 1Â year, 11Â months ago\nSince A and D both seem to provide the identity/integrity and internet inaccessibility, it seems the critical distinction is based on \"reduce the operational cost of managing your cluster\". \"Operational cost\" doesn't seem to be a commonly used term (from a quick google search), but \"operating costs\" seem to refer specifically to monetary expenses, not work-hours. Wouldn't a standard cluster be cheaper than autopilot? Thus the answer is D, not A?"
      },
      {
        "index": 7,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: D\nChatGPT says Option D,\nBy following this approach, you can meet your requirements for node security and access control while also benefitting from the operational cost savings associated with managed GKE clusters and Google's best practices for security. PiperMe 1Â year, 10Â months ago\nStop. Using. Chat GPT.\nD is viable for security, but with the standard GKE mode, you'd be responsible for managing the control plane and node-level operations, increasing operational complexity. \"You want to reduce the operational cost of managing your cluster\"\nOption A leverages the managed experience of Autopilot with the security of private nodes and shielded GKE for node identity/integrity. The answer is A. sukouto 1Â year, 10Â months ago\nWhy is everyone so sure that \"operational cost\" refers to work-hours and not money? (i.e. \"operating costs\")\nFrom Wikipedia: Operating costs or operational costs, are the expenses which are related to the operation of a business, or to the operation of a device, component, piece of equipment or facility.\nThis question is asking to reduce the MONETARY cost. Standard costs less than Autopilot. Accordingly, the answer should be D."
      },
      {
        "index": 8,
        "text": "Anonymous: MARINE777 2Â years ago\nSelected Answer: D\nAutopilot clusters are fully managed and do not have the option to restrict internet access.\nIn a private cluster, nodes are not accessible from the internet by default. Enabling shielded nodes provides verifiable node identity and integrity. PiperMe 1Â year, 10Â months ago\nThis is incorrect. By default, Autopilot clusters create nodes within a private VPC network. This inherently restricts internet access to the nodes themselves. The answer is A."
      },
      {
        "index": 9,
        "text": "Anonymous: ArtistS 2Â years, 2Â months ago\nA is correct. â€œreduce the operational cost of managing your clusterâ€, means you need to choose an autopilot cluster. Google will manage your cluster configuration. And about the â€œcannot be accessed from the internetâ€ you should use shielded nodes."
      },
      {
        "index": 10,
        "text": "Anonymous: rsvd 2Â years, 5Â months ago\nSelected Answer: A\nNote: For GKE Autopilot clusters, the Shielded GKE Nodes feature is enabled by default and cannot be overridden."
      }
    ]
  },
  {
    "id": 166,
    "source": "examtopics",
    "question": "Your company wants to migrate their on-premises workloads to Google Cloud. The current on-premises workloads consist of:\n\nâ€¢ A Flask web application\nâ€¢ A backend API\nâ€¢ A scheduled long-running background job for ETL and reporting\n\nYou need to keep operational costs low. You want to follow Google-recommended practices to migrate these workloads to serverless solutions on Google Cloud. What should you do?",
    "options": {
      "A": "Migrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Compute Engine.",
      "B": "Migrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Cloud Run.",
      "C": "Run the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Cloud Run.",
      "D": "Run the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Compute Engine."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: interesting_owl Highly Voted 1Â year, 6Â months ago\nSelected Answer: B\nit's asking for a serverless solution so A and D are automatically out due to the inclusion of Compute Engine (server-based solution). you would use App engine to run a web app, not cloud storage. That's why it's B"
      },
      {
        "index": 2,
        "text": "Anonymous: joao_01 Most Recent 1Â year, 10Â months ago\nSelected Answer: B\nIts B, it's serveless and low cost"
      },
      {
        "index": 3,
        "text": "Anonymous: scanner2 1Â year, 10Â months ago\nSelected Answer: B\nSince the question is asking about serverless solutions. Here, B is the correct answer.\nMigrate web application to the App Engine.\nMigrate backend API to Cloud Run.\nMigrate scheduled long-running job to Cloud Task that will run the background job using Cloud Run."
      },
      {
        "index": 4,
        "text": "Anonymous: 3arle 1Â year, 11Â months ago\nSelected Answer: B\nB is most reasonable"
      },
      {
        "index": 5,
        "text": "Anonymous: qannik 1Â year, 11Â months ago\nSelected Answer: B\nhttps://cloud.google.com/architecture/migration-to-gcp-deploying-your-workloads"
      },
      {
        "index": 6,
        "text": "Anonymous: gpais 1Â year, 11Â months ago\nSelected Answer: B\nB seems the best option"
      }
    ]
  },
  {
    "id": 167,
    "source": "examtopics",
    "question": "Your company is moving its continuous integration and delivery (CI/CD) pipeline to Compute Engine instances. The pipeline will manage the entire cloud infrastructure through code. How can you ensure that the pipeline has appropriate permissions while your system is following security best practices?",
    "options": {
      "A": "â€¢ Attach a single service account to the compute instances.",
      "B": "â€¢ Add a step for human approval to the CI/CD pipeline before the execution of the infrastructure provisioning.",
      "C": "â€¢ Attach a single service account to the compute instances.",
      "D": "â€¢ Create multiple service accounts, one for each pipeline with the appropriate minimal Identity and Access Management (IAM) permissions."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: guaose 2Â months, 1Â week ago\nSelected Answer: A\nPrincipio de privilegios mÃ­nimos\nUso de impersonaciÃ³n (actAs)\nSeguridad y trazabilidad"
      },
      {
        "index": 2,
        "text": "Anonymous: 41168b9 2Â months, 3Â weeks ago\nSelected Answer: A\nAttach a single service account to the compute instances.\nThis is the Compute Engine VM's identity. This service account should only have minimal, non-sensitive permissions, such as logging or monitoring access. This adheres to the Principle of Least Privilege for the base VM.\nAdd minimal rights to the service account.\nAs noted above, keep the VM's inherent identity highly restricted.\nAllow the service account to impersonate a Cloud Identity user with elevated permissions to create, update, or delete resources.\nThe VM's service account is granted the iam.serviceAccountUser role on a separate, privileged Service Account (often called the \"Provisioner\" or \"Deployer\" SA).\nWhen the pipeline needs to manage infrastructure, the VM service account impersonates the privileged account for a short time to execute the tasks."
      },
      {
        "index": 3,
        "text": "Anonymous: Ice_age 1Â year, 1Â month ago\nSome of these questions are just endurance tests to drain you before you finish reading the entire question and all of the answers."
      },
      {
        "index": 4,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nIt seems, you all just use chat gpt to get the answer. But did you even notice it says one they need to move only one pipeline? iooj 1Â year, 4Â months ago\nBy the way, chat gpt o1-preview says: that A is the answer\nPrinciple of Least Privilege: By assigning minimal rights to the service account, you limit access to only what's necessary for regular operations.\nImpersonation for Elevated Actions: Allowing the service account to impersonate a Cloud Identity user with elevated permissions ensures that higher-level permissions are used only when needed and are tightly controlled.\nSecurity Best Practices: This approach avoids the use of long-lived credentials or storing service account keys, reducing potential security risks."
      },
      {
        "index": 5,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: D\nOption D combines the principle of least privilege with granular permissions, secure credential management, and controlled access during pipeline execution."
      },
      {
        "index": 6,
        "text": "Anonymous: guru_ji 1Â year, 11Â months ago\nSelected Answer: D\nOptions A and C both involve attaching a single service account to the compute instances, which goes against the principle of least privilege and increases the risk if that single account is compromised. Option B introduces human approval into the CI/CD pipeline, which could slow down the deployment process and might not be feasible for fully automated deployments. Therefore, option D is the most suitable choice for ensuring both security and efficiency in the CI/CD pipeline setup."
      },
      {
        "index": 7,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: D\nPrinciple of Least Privilege: Creating separate service accounts for different aspects of your CI/CD pipeline allows you to adhere to the principle of least privilege. This means each service account is granted only the permissions necessary for its specific role in the pipeline.\nSecurity and Organization: Using multiple service accounts makes it easier to manage permissions, track activities, and audit usage for specific tasks or components of your CI/CD process.\nSecret Management: Storing the service account key files in a secret manager service (like Google Cloud Secret Manager) enhances security. This approach securely manages and accesses these keys, reducing the risk of unauthorized access or exposure.\nDynamic Access: Allowing the CI/CD pipeline to request the appropriate secrets during execution ensures that credentials are provided only when needed and aren't unnecessarily exposed or stored in less secure environments. Cynthia2023 2Â years ago\nA. Single Service Account with Impersonation: While using a single service account with minimal rights and impersonation can work, it introduces complexity and might not offer the same level of granularity and security as multiple service accounts. Impersonation also adds an additional layer that needs to be securely managed."
      },
      {
        "index": 8,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: D\nChatGPT says Option D,\nBy following this approach, you can ensure that your CI/CD pipeline has appropriate permissions while adhering to security best practices, including the principle of least privilege and secure management of credentials. 1826c27 11Â months, 1Â week ago\nwhat Is the point of telling us what chatgpt said? we all have access to chatgpt idiot"
      }
    ]
  },
  {
    "id": 168,
    "source": "examtopics",
    "question": "Your application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?",
    "options": {
      "A": "Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days.",
      "B": "Create a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days.",
      "C": "Create a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock.",
      "D": "Enable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gius3 2Â days, 4Â hours ago\nSelected Answer: B\nAnswer is B, as even the ARchive Class has a cost"
      },
      {
        "index": 2,
        "text": "Anonymous: Phat 9Â months, 4Â weeks ago\nSelected Answer: A\nIt seems we need to select ones with cheapest cost instead of deleting them."
      },
      {
        "index": 3,
        "text": "Anonymous: kapara 1Â year ago\nSelected Answer: A\nThe question is badly worded - because they didn't tell us what they intend to do with the files afterwards.\nI choose A anyway, because with lifecycle management you can delete files after 30 days, and using other services in this situation is really unnecessary.."
      },
      {
        "index": 4,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nIt is cheaper to delete the files as there is no requirement to keep them."
      },
      {
        "index": 5,
        "text": "Anonymous: louisaok 1Â year, 3Â months ago\nSelected Answer: A\nThis is the same logic as Microsoft:\nwhen you have 2 options: one needs to pay and the other is free, choose the one with fee.\nThat is the right answer."
      },
      {
        "index": 6,
        "text": "Anonymous: master9 1Â year, 4Â months ago\nSelected Answer: A\nCloud Storage lifecycle management, you can automatically transition objects between storage classes based on certain conditions, such as age. Since your application only requires access to files created in the last 30 days, you can set a lifecycle rule to move files that are older than 30 days to Archive Storage, which offers the lowest storage costs but is designed for infrequent access."
      },
      {
        "index": 7,
        "text": "Anonymous: klayhung 1Â year, 4Â months ago\nSelected Answer: A\nThis option utilizes Cloud Storage's built-in object lifecycle management feature, which can automatically transition files older than 30 days to Archive Storage, thereby saving storage costs without requiring manual management. In comparison, option B is feasible but more complex and does not align with best practices."
      },
      {
        "index": 8,
        "text": "Anonymous: caminosdk 1Â year, 4Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 9,
        "text": "Anonymous: rajeevpt 1Â year, 5Â months ago\nA\nA is Correct because it suggests changing the storage class to Archive Storage for objects with an age of over 30 days through a lifecycle rule on the storage bucket. This is a cost-effective solution because Google Cloud Storage offers different storage classes with varying costs. The \"Archive Storage\" class is designed for infrequently accessed data and comes at a lower cost compared to the standard storage class. Using a lifecycle rule to transition objects older than 30 days to the Archive Storage class helps save costs by utilizing a more cost-efficient storage class for older data."
      },
      {
        "index": 10,
        "text": "Anonymous: flummoxed_individual 1Â year, 6Â months ago\nSelected Answer: A\nAnother classic annoyingly vague question, but I would have to go with A because 'normally' you would keep files for longer than 30 days. If it is ok to delete, then B"
      }
    ]
  },
  {
    "id": 169,
    "source": "examtopics",
    "question": "Your manager asks you to deploy a workload to a Kubernetes cluster. You are not sure of the workload's resource requirements or how the requirements might vary depending on usage patterns, external dependencies, or other factors. You need a solution that makes cost-effective recommendations regarding CPU and memory requirements, and allows the workload to function consistently in any situation. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Configure the Horizontal Pod Autoscaler for availability, and configure the cluster autoscaler for suggestions.",
      "B": "Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.",
      "C": "Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Cluster autoscaler for suggestions.",
      "D": "Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Horizontal Pod Autoscaler for suggestions."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Urbanvzla Highly Voted 1Â year, 6Â months ago\nSelected Answer: B\nHorizontal Pod Autoscaler (HPA): It automatically scales the number of pods in a deployment, replica set, or stateful set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). This helps maintain availability by ensuring that your application has the necessary number of pods to handle the workload.\nVertical Pod Autoscaler (VPA): It automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications. This is particularly useful when you're unsure of the resource requirements. VPA makes recommendations for the appropriate CPU and memory settings based on usage patterns, which can be very effective for cost optimization.\nThis combination ensures that your workload is both horizontally scalable (to handle changes in demand) and vertically optimized (to use resources efficiently), following Google-recommended practices for Kubernetes workloads."
      },
      {
        "index": 2,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nI believe B is the best choice: HPA ensures availability by scaling the number of pods based on metrics (like CPU utilization). The VPA analyzes resource utilization and provides recommendations for CPU and memory requests and limits. This is key for right-sizing your pods for optimal cost efficiency."
      },
      {
        "index": 3,
        "text": "Anonymous: Lakshvenkat 1Â year, 6Â months ago\nSelected Answer: D\nD is the correct answer kuracpalac 1Â year, 4Â months ago\nVPA is not recommended as per Google requirements, so that answer must be wrong."
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: B\nHorizontal Pod Autoscaler (HPA): HPA automatically adjusts the number of pods in a deployment based on observed CPU utilization (or other select metrics). This is crucial for maintaining the availability of your workload, especially if the workload experiences varying levels of traffic or load. HPA ensures that there are enough pods to handle the load, scaling out (adding more pods) when demand is high and scaling in (removing pods) when demand is low.\nVertical Pod Autoscaler (VPA) Recommendations: VPA automatically adjusts the CPU and memory reservations for pods in a deployment. It can operate in a mode where it only provides recommendations (without automatically applying them), which is useful for understanding the resource needs of your workload. VPA recommendations can guide you in setting appropriate CPU and memory limits based on the observed usage of your workload. Cynthia2023 1Â year, 6Â months ago\nA. Cluster Autoscaler for Suggestions: While the Cluster Autoscaler is useful for scaling the number of nodes in the cluster, it doesnâ€™t provide recommendations on the CPU and memory requirements for individual pods.\nC. VPA for Availability, Cluster Autoscaler for Suggestions: VPA can automatically adjust pod sizes, but using it for ensuring availability might lead to frequent and potentially disruptive pod restarts. The Cluster Autoscaler is again more about node-level scaling rather than providing pod resource recommendations.\nD. VPA for Availability, HPA for Suggestions: This configuration isn't ideal as VPA's primary function isn't about maintaining high availability but rather about optimizing resource allocation. HPA, on the other hand, is specifically designed for scaling the number of pods based on load, which is directly related to availability."
      },
      {
        "index": 5,
        "text": "Anonymous: kaby1987 1Â year, 6Â months ago\nSelected Answer: B\nAns is B\nB. Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions.\nThis approach allows you to manage the number of pods based on the workload (HPA) and get optimal CPU and memory settings for each pod (VPA), which is in line with Google-recommended practices for managing Kubernetes workloads with uncertain resource requirements. This combination ensures that your workload can function consistently in varying situations by automatically adjusting both the quantity of pods and the resources each pod is allocated."
      },
      {
        "index": 6,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: D\nChatGPT says option D,\nBy configuring VPA for resource recommendations based on actual usage patterns and HPA for scaling pod instances based on demand, you can ensure that your workload is both cost-effective and capable of adapting to varying resource requirements, all while following Google-recommended practices for Kubernetes workloads. PiperMe 1Â year, 4Â months ago\nChat strikes again. Option B provides a more tailored and Google-recommended approach given the uncertainty about the workload's resource needs. It prioritizes establishing an efficient baseline with VPA before relying on HPA for scaling."
      }
    ]
  },
  {
    "id": 170,
    "source": "examtopics",
    "question": "You need to migrate invoice documents stored on-premises to Cloud Storage. The documents have the following storage requirements:\n\nâ€¢ Documents must be kept for five years.\nâ€¢ Up to five revisions of the same invoice document must be stored, to allow for corrections.\nâ€¢ Documents older than 365 days should be moved to lower cost storage tiers.\n\nYou want to follow Google-recommended practices to minimize your operational and development costs. What should you do?",
    "options": {
      "A": "Enable retention policies on the bucket, and use Cloud Scheduler to invoke a Cloud Function to move or delete your documents based on their metadata.",
      "B": "Enable retention policies on the bucket, use lifecycle rules to change the storage classes of the objects, set the number of versions, and delete old files.",
      "C": "Enable object versioning on the bucket, and use Cloud Scheduler to invoke a Cloud Functions instance to move or delete your documents based on their metadata.",
      "D": "Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Cynthia2023 Highly Voted 1Â year, 6Â months ago\nSelected Answer: D\n- Object Versioning: Enabling object versioning on the Cloud Storage bucket allows you to store up to five revisions of the same invoice document. This satisfies the requirement for keeping multiple versions of each document for corrections.\n- Lifecycle Conditions: Google Cloud Storage allows you to define lifecycle conditions for objects within a bucket. These conditions can automatically change the storage class of objects when they meet certain criteria, such as age. After 365 days, you can automatically move documents to lower-cost storage classes like Nearline, Coldline, or Archive, which reduces storage costs while still retaining the data.\n- Version Management and Deletion: The lifecycle rules can also be configured to manage the number of object versions retained and to delete old versions or objects, ensuring compliance with the five-year retention requirement. Cynthia2023 1Â year, 6Â months ago\nWhy B is not correct:\nLifecycle rules in Google Cloud Storage can be used to manage the deletion of old versions of objects. However, it's important to note that lifecycle rules alone do not set the number of versions to keep; they can only delete versions based on age or other criteria.\nLifecycle rules in Google Cloud Storage do not have a direct setting to limit the number of object versions (like keeping only the last five versions). Object versioning in Google Cloud Storage keeps all versions of an object until they are explicitly deleted (either manually or through lifecycle rules)."
      },
      {
        "index": 2,
        "text": "Anonymous: TanTran04 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nFollow D"
      },
      {
        "index": 3,
        "text": "Anonymous: sukouto 1Â year, 4Â months ago\nSelected Answer: B\nI believe the answer is actually B, and D won't cut it.\nD does not address the need \"Documents must be kept for five years.\" A retention policy is required, otherwise someone can just delete a document.\nSomeone else suggested that you can't set object versioning with life cycle rules, but that's not quite true. https://cloud.google.com/storage/docs/lifecycle\nYou can, but it does require object versioning to be enabled... So none of these answers are ideal, but I think the omission of setting a retention policy explicitly misses the first requirement stated. sukouto 1Â year, 4Â months ago\nUpon further reading, it seems Retention Policies and Object Versioning are mutually exclusive, meaning B cannot cover the second requirement. https://cloud.google.com/storage/docs/object-versioning\nIt's not explicitly stated, but it is implied that Object Versioning can prevent total deletion (i.e. deleting a live version of an object moves it to a non-current version).\nI guess the answer will have to be D"
      },
      {
        "index": 4,
        "text": "Anonymous: JB28 1Â year, 6Â months ago\nThe correct answer is **D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files**.\nHere's why:\n- **Object versioning** allows you to keep up to five revisions of the same invoice document.\n- **Lifecycle conditions** can be used to automatically change the storage class of objects older than 365 days to a lower-cost storage tier.\n- You can also set the number of versions to keep and automatically delete old files, which helps to manage storage costs effectively.\nThis approach aligns with Google-recommended practices and helps to minimize operational and development costs."
      },
      {
        "index": 5,
        "text": "Anonymous: Gocool28 1Â year, 6Â months ago\nObvious answer is D 1826c27 11Â months, 2Â weeks ago\nwhy not B mr Obvious? \"Documents must be kept for five years.\" - how does D cover this requirement?"
      },
      {
        "index": 6,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: D\nPer ChatGPT, Option D aligns with Google-recommended practices for managing objects in Cloud Storage, including versioning, lifecycle management, and cost optimization, making it the best choice for the given scenario. 1826c27 11Â months, 2Â weeks ago\nPer my grandma, Option A aligns with Google-recommended practices for managing objects in Cloud Storage, including versioning, lifecycle management, and cost optimization, making it the best choice for the given scenario."
      },
      {
        "index": 7,
        "text": "Anonymous: shiowbah 1Â year, 6Â months ago\nD. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files."
      }
    ]
  },
  {
    "id": 171,
    "source": "examtopics",
    "question": "You installed the Google Cloud CLI on your workstation and set the proxy configuration. However, you are worried that your proxy credentials will be recorded in the gcloud CLI logs. You want to prevent your proxy credential from being logged. What should you do?",
    "options": {
      "A": "Configure username and password by using gcloud config set proxy/username and gcloud config set proxy/password commands.",
      "B": "Encode username and password in sha256 encoding, and save in to a text file. Use filename as a value in the gcloud config set core/custom_ca_certs_file command.",
      "C": "Provide values for CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD in the gcloud CLI tool configuration file.",
      "D": "Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ChemaKSado Highly Voted 1Â year, 10Â months ago\nAnswer is D. See Google Docs: https://cloud.google.com/sdk/docs/proxy-settings\nAlternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables.\nexport CLOUDSDK_PROXY_USERNAME [USERNAME]\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]"
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nAlternatively, to avoid having the proxy credentials recorded in any logs (such as shell history or gcloud CLI logs) or in the gcloud CLI configuration file, you can set the properties using environment variables\nhttps://cloud.google.com/sdk/docs/proxy-settings denno22 1Â year, 3Â months ago\nexport CLOUDSDK_PROXY_USERNAME [USERNAME]\nexport CLOUDSDK_PROXY_PASSWORD [PASSWORD]"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: D\nOption D is the best answer."
      },
      {
        "index": 4,
        "text": "Anonymous: leoalvarezh 1Â year, 11Â months ago\nSelected Answer: D\nTo avoid shell CLI history, logs or conf. file. Google recommends to set it on env. variables related with no storing of those values. Option D."
      },
      {
        "index": 5,
        "text": "Anonymous: sinh 2Â years ago\nSelected Answer: D\nhttps://cloud.google.com/sdk/docs/proxy-settings"
      },
      {
        "index": 6,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: D\n- Using Environment Variables: By setting the proxy credentials as environment variables (CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD), you avoid having to enter them directly into the CLI tool where they might be logged. Environment variables are a common way to securely pass sensitive information like credentials.\n- No Logging of Credentials: The gcloud CLI typically does not log environment variables, so your credentials should be safe from being recorded in the CLI logs.\n- Ease of Use: Setting environment variables is straightforward and does not require modifying configuration files or encoding credentials. Cynthia2023 2Â years ago\nA. Using gcloud config set for Username and Password: This approach directly enters the credentials into the gcloud CLI configuration, which could potentially be logged or exposed in the configuration file.\nB. Encoding Credentials and Custom CA Certs File: This option suggests a method that isn't directly related to setting proxy credentials. The core/custom_ca_certs_file is used for specifying a custom CA (Certificate Authority) certificate file, not for proxy credentials.\nC. Using the Configuration File: Modifying the configuration file to include proxy credentials might expose them in plain text within the file, which could be a security risk. It's generally safer to use environment variables for this purpose."
      },
      {
        "index": 7,
        "text": "Anonymous: kaby1987 2Â years ago\nSelected Answer: D\nAns is D"
      },
      {
        "index": 8,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: C\nPer ChatGPT, Option C is the most appropriate choice for securely providing proxy credentials to the gcloud CLI tool without risking exposure in logs or other outputs. PiperMe 1Â year, 10Â months ago\nI think this test may have rocked you due to ChatGPT. Storing credentials in the config file increases the risk of exposure if the file is compromised. It's the same problem as A. The answer is D."
      },
      {
        "index": 9,
        "text": "Anonymous: shiowbah 2Â years ago\nD. Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool."
      }
    ]
  },
  {
    "id": 172,
    "source": "examtopics",
    "question": "Your company developed an application to deploy on Google Kubernetes Engine. Certain parts of the application are not fault-tolerant and are allowed to have downtime. Other parts of the application are critical and must always be available. You need to configure a Google Kubernetes Engine cluster while optimizing for cost. What should you do?",
    "options": {
      "A": "Create a cluster with a single node-pool by using standard VMs. Label he fault-tolerant Deployments as spot_true.",
      "B": "Create a cluster with a single node-pool by using Spot VMs. Label the critical Deployments as spot_false.",
      "C": "Create a cluster with both a Spot VM node pool and a node pool by using standard VMs. Deploy the critical deployments on the Spot VM node pool and the fault-tolerant deployments on the node pool by using standard VMs.",
      "D": "Create a cluster with both a Spot VM node pool and a nods pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: halifax 1Â year ago\nSelected Answer: D\nWeird - why does it say:\n\"Certain parts of the application are NOT fault-tolerant and are allowed to have downtime,\" I think the word \"NOT\" is a typo, if they are allowed to have downtime, they are fault-tolerant.\nD is correct but the word NOT is probably a typo."
      },
      {
        "index": 2,
        "text": "Anonymous: JoseCloudEng1994 1Â year ago\nSelected Answer: D\nPretty obvious. Its just a struggle to read the whole thing"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe 1Â year, 4Â months ago\nSelected Answer: D\nCorrect answer is D."
      },
      {
        "index": 4,
        "text": "Anonymous: blackBeard33 1Â year, 5Â months ago\nSelected Answer: D\nD is the right answer here. Spot Vms are fault-tolerant."
      },
      {
        "index": 5,
        "text": "Anonymous: JB28 1Â year, 6Â months ago\nThe correct answer is D. Create a cluster with both a Spot VM node pool and a node pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool.\nHereâ€™s why:\nSpot VMs are cost-effective but can be preempted at any time, making them suitable for fault-tolerant parts of the application that can afford downtime.\nStandard VMs, while more expensive, provide consistent availability and are ideal for critical parts of the application that must always be available.\nBy creating a cluster with both types of node pools, you can optimize for cost while ensuring the availability of critical application components."
      },
      {
        "index": 6,
        "text": "Anonymous: Cynthia2023 1Â year, 6Â months ago\nSelected Answer: D\nSpot VM Node Pool for Fault-Tolerant Parts: Spot VMs in GKE are cost-effective but can be preempted (terminated) by Google Cloud with little notice if their resources are needed elsewhere. They are suitable for workloads that can handle interruptions, like the fault-tolerant parts of your application.\nStandard VM Node Pool for Critical Parts: Standard VMs offer more reliability and are not subject to preemption like Spot VMs. Using a standard VM node pool for the critical parts of your application ensures they remain available and are not disrupted by potential preemptions."
      },
      {
        "index": 7,
        "text": "Anonymous: kaby1987 1Â year, 6Â months ago\nSelected Answer: D\nAns is D"
      },
      {
        "index": 8,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: C\nPer ChatGPT, Option C aligns with the requirements of optimizing cost, ensuring fault tolerance for certain parts of the application, and maintaining high availability for critical parts by using a combination of Spot VM node pools and standard VM node pools in the same GKE cluster. ashiqnazeem 1Â year, 5Â months ago\n@KelvinToo chatgpt answers are not always correct. dan12q 1Â year, 6Â months ago\nCritical deployments is not fault-tolerant. Spot VM can restart after 24 hours."
      },
      {
        "index": 9,
        "text": "Anonymous: shiowbah 1Â year, 6Â months ago\nD. Create a cluster with both a Spot VM node pool and a nods pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool."
      }
    ]
  },
  {
    "id": 173,
    "source": "examtopics",
    "question": "You need to deploy an application in Google Cloud using serverless technology. You want to test a new version of the application with a small percentage of production traffic. What should you do?",
    "options": {
      "A": "Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting.",
      "B": "Deploy the application to Google Kubernetes Engine. Use Anthos Service Mash for traffic splitting.",
      "C": "Deploy the application to Cloud Functions. Specify the version number in the functions name.",
      "D": "Deploy the application to App Engine. For each new version, create a new service."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: carpa_jo Highly Voted 1Â year, 11Â months ago\nSelected Answer: A\nI would go for A. Here is why:\nCore requirements:"
      },
      {
        "index": 1,
        "text": "Serverless technology"
      },
      {
        "index": 2,
        "text": "Gradual deployment\nA: Cloud Run is serverless and supports gradual deployments with the help of tags and gcloud run services update-traffic.\nB: GKE is not considered serverless. Anthos Service Mash does support gradual deployments.\nC: Cloud Functions is serverless but specifying the version number in the functions name doesn't help to achieve gradual deployments. Instead using revisions or tags in combination with gcloud run services update-traffic is required.\nD: App Engine is serverless. For gradual deployments, you should create a new version of your service and then use gcloud app services set-traffic to split your traffic. Don't create a new service for each new version."
      },
      {
        "index": 2,
        "text": "Anonymous: 364db8d Most Recent 4Â months ago\nSelected Answer: A\nAnthos Service Mashed Potatoes"
      },
      {
        "index": 3,
        "text": "Anonymous: yomi95 1Â year, 3Â months ago\nSelected Answer: A\nAnswer A\nD. also comes close with App Engine (since serverless and can split traffic in versions), but here is says create new service for each version where it's not needed in App engine."
      },
      {
        "index": 4,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer."
      },
      {
        "index": 5,
        "text": "Anonymous: JB28 2Â years ago\nThe correct answer is **A. Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting**.\nHere's why:\n- **Cloud Run** is a serverless platform that allows you to deploy and run your applications without worrying about infrastructure management. It supports deploying new versions of an application and gradually rolling out updates using traffic splitting. This makes it ideal for testing a new version of an application with a small percentage of production traffic.\n- The other options do not provide the same level of support for serverless deployment and traffic splitting for testing new versions of an application."
      },
      {
        "index": 6,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: A\nPer ChatGPT, Option A, deploying the application to Cloud Run and using gradual rollouts for traffic splitting, is the best choice for testing a new version of the application with a small percentage of production traffic while leveraging serverless technology in Google Cloud."
      },
      {
        "index": 7,
        "text": "Anonymous: shiowbah 2Â years ago\nA. Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting."
      }
    ]
  },
  {
    "id": 174,
    "source": "examtopics",
    "question": "Your company's security vulnerability management policy wants a member of the security team to have visibility into vulnerabilities and other OS metadata for a specific Compute Engine instance. This Compute Engine instance hosts a critical application in your Google Cloud project. You need to implement your company's security vulnerability management policy. What should you do?",
    "options": {
      "A": "â€¢ Ensure that the Ops Agent is installed on the Compute Engine instance.",
      "B": "â€¢ Ensure that the Ops Agent is installed on the Compute Engine instance.",
      "C": "â€¢ Ensure that the OS Config agent is installed on the Compute Engine instance.",
      "D": "â€¢ Ensure that the OS Config agent is installed on the Compute Engine instance."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Cynthia2023 Highly Voted 1Â year, 6Â months ago\nSelected Answer: C\nOps Agent: The Ops Agent is primarily used for collecting system and application metrics, as well as logs in Google Cloud. It is adept at monitoring the performance and health of applications and virtual machines but does not specialize in vulnerability assessment or OS-level inventory management.\nOS Config Agent: This agent is specifically designed for OS configuration, inventory management, and vulnerability reporting in Google Cloud. It can gather and report on system-level inventory information (like installed packages) and OS vulnerabilities. Cynthia2023 1Â year, 6Â months ago\nVisibility into Vulnerabilities and Other OS Metadata:\nTo access vulnerability data specifically, the roles/osconfig.vulnerabilityReportViewer role is more appropriate. This role is designed to provide access to vulnerability reports generated by the OS Config agent.\nTo access general OS metadata, the osconfig.inventoryViewer role is suitable, as it allows the user to view inventory information collected by the OS Config agent. Cynthia2023 1Â year, 6Â months ago\nIn the context of the provided options, none of them specifically combines both the osconfig.inventoryViewer and roles/osconfig.vulnerabilityReportViewer roles with the OS Config agent. However, Option C (ensuring the OS Config agent is installed and providing the roles/osconfig.vulnerabilityReportViewer permission) comes closest to fulfilling the requirement, particularly for the critical aspect of vulnerability visibility."
      },
      {
        "index": 2,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nC is correct here. It leverages the OS Config agent and a well-defined IAM role."
      },
      {
        "index": 3,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: C\nPer ChatGPT, Option C aligns with the requirement of providing visibility into vulnerabilities and other OS metadata for the specific Compute Engine instance while following the principle of least privilege by granting only the necessary permissions to the security team member."
      },
      {
        "index": 4,
        "text": "Anonymous: shiowbah 1Â year, 6Â months ago\nC. â€¢ Ensure that the OS Config agent is installed on the Compute Engine instance.\nâ€¢ Provide the security team member roles/osconfig.vulnerabilityReportViewer permission."
      }
    ]
  },
  {
    "id": 175,
    "source": "examtopics",
    "question": "You want to enable your development team to deploy new features to an existing Cloud Run service in production. To minimize the risk associated with a new revision, you want to reduce the number of customers who might be affected by an outage without introducing any development or operational costs to your customers. You want to follow Google-recommended practices for managing revisions to a service. What should you do?",
    "options": {
      "A": "Ask your customers to retry access to your service with exponential backoff to mitigate any potential problems after the new revision is deployed.",
      "B": "Gradually roll out the new revision and split customer traffic between the revisions to allow rollback in case a problem occurs.",
      "C": "Send all customer traffic to the new revision, and roll back to a previous revision if you witness any problems in production.",
      "D": "Deploy your application to a second Cloud Run service, and ask your customers to use the second Cloud Run service."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: shiowbah Highly Voted 1Â year, 6Â months ago\nB. Gradually roll out the new revision and split customer traffic between the revisions to allow rollback in case a problem occurs."
      },
      {
        "index": 2,
        "text": "Anonymous: carpa_jo Highly Voted 1Â year, 5Â months ago\nSelected Answer: B\nI would go with B. Here is why:\nRequirements:"
      },
      {
        "index": 1,
        "text": "Reduce the number of customers who might be affected by an outage (caused by a new version release)"
      },
      {
        "index": 2,
        "text": "Don't introduce any development or operational costs to the customers.\n(3. Follow Google-recommended practices)\nA: Might require the customer to setup exponential backoff on their end -> Requirement 2 not met.\nB: Fulfills all requirements.\nC: Does not meet requirement 1, as switching 100% traffic at once to the new version would affect all of the customers, if there would be any issue in this version.\nD: Does not meet requirements 1, 2 and 3, as an issue would affect all customers (once they have switched to the new service), switching to the second cloud run service causes dev/ops costs on the customer side and it doesn't follow Google-recommended practices, as a new version of an existing service should not be released as a new service."
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nB for the win! Deploy new features = split traffic. Every time."
      },
      {
        "index": 4,
        "text": "Anonymous: KelvinToo 1Â year, 6Â months ago\nSelected Answer: B\nPer ChatGPT, Option B aligns with the recommended practice of gradually rolling out new revisions, allowing for controlled monitoring and risk mitigation, without introducing any development or operational costs to customers."
      }
    ]
  },
  {
    "id": 176,
    "source": "examtopics",
    "question": "You have deployed an application on a Compute Engine instance. An external consultant needs to access the Linux-based instance. The consultant is connected to your corporate network through a VPN connection, but the consultant has no Google account. What should you do?",
    "options": {
      "A": "Instruct the external consultant to use the gcloud compute ssh command line tool by using Identity-Aware Proxy to access the instance.",
      "B": "Instruct the external consultant to use the gcloud compute ssh command line tool by using the public IP address of the instance to access it.",
      "C": "Instruct the external consultant to generate an SSH key pair, and request the public key from the consultant. Add the public key to the instance yourself, and have the consultant access the instance through SSH with their private key.",
      "D": "Instruct the external consultant to generate an SSH key pair, and request the private key from the consultant. Add the private key to the instance yourself, and have the consultant access the instance through SSH with their public key."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Cynthia2023 Highly Voted 2Â years ago\nSelected Answer: C\nA. Using Identity-Aware Proxy (IAP): While IAP is a secure method of accessing Compute Engine instances, it typically requires a Google account for authentication, which the consultant does not have."
      },
      {
        "index": 2,
        "text": "Anonymous: JB28 Highly Voted 2Â years ago\nThe correct answer is **C**.\nTo allow an external consultant to access a Linux-based Compute Engine instance, you should:\n- Instruct the external consultant to generate an **SSH key pair**. This will result in a public key and a private key.\n- Request the **public key** from the consultant. The public key can be shared without compromising security.\n- Add the public key to the instance yourself. This will allow the consultant to authenticate with the Compute Engine instance.\n- Have the consultant access the instance through SSH with their **private key**. The private key should be kept secret and not shared.\nThe other options (A, B, and D) are not correct because they either require the consultant to have a Google account, expose the instance to the public internet, or involve sharing the private key, which is a security risk."
      },
      {
        "index": 3,
        "text": "Anonymous: JoseCloudEng1994 Most Recent 1Â year ago\nSelected Answer: A\nI would say A.\nhttps://cloud.google.com/iap/docs/external-identities\nIAP controls access to your applications and resources. It leverages user identity and the context of a request to determine if a user should be allowed access. IAP is a building block toward BeyondCorp, an enterprise security model that enables employees to work from untrusted networks without using a VPN.\nBy default, IAP uses Google identities and IAM. By leveraging Identity Platform instead, you can authenticate users with a wide range of external identity providers, such as:\nEmail/password\nOAuth (Google, Facebook, Twitter, GitHub, Microsoft, etc.)\nSAML\nOIDC\nPhone number\nCustom\nAnonymous\nThis is useful if your application is already using an external authentication system, and migrating your users to Google accounts is impractical."
      },
      {
        "index": 4,
        "text": "Anonymous: sahuprashant123 1Â year, 1Â month ago\nSelected Answer: C\nThe most secure and recommended method for providing SSH access to a consultant who does not have a Google account is to have them generate an SSH key pair, provide you with the public key, and then add it to the instance so they can authenticate using their private key."
      },
      {
        "index": 5,
        "text": "Anonymous: sahuprashant123 1Â year, 1Â month ago\nSelected Answer: C\nIAP can be used for secure SSH access but is only feasible if the consultant has or is provided with a Google account. If providing a Google account is not an option, the best alternative is to use the existing VPN connection with SSH key-based access."
      },
      {
        "index": 6,
        "text": "Anonymous: NinjaCloud 1Â year, 1Â month ago\nSelected Answer: A\nOption A is a valid answer. IAP allows access using third-party identity providers (like SAML, OAuth 2.0), so the consultant doesn't need to have a Google account."
      },
      {
        "index": 7,
        "text": "Anonymous: FormacionCloud314 1Â year, 2Â months ago\nThe correct answer is C, why is option A discarded, being complementary with previous answers, see question number 152... It says it very clear, the auditor is connected through the VPN network, if we observe the documentation based on IAP, it is recommended for uses outside the VPN, (...IAP is a building block toward BeyondCorp, an enterprise security model that enables employees to work from untrusted networks without using a VPN....). My answer will be C\nhttps://cloud.google.com/iap/docs/external-identities"
      },
      {
        "index": 8,
        "text": "Anonymous: Chetantest07 1Â year, 3Â months ago\nSelected Answer: A\nA. IAP can work with no google account.https://cloud.google.com/iap/docs/external-identities"
      },
      {
        "index": 9,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nhttps://cloud.google.com/iap/docs/external-identities."
      },
      {
        "index": 10,
        "text": "Anonymous: C0D3LK 1Â year, 4Â months ago\nSelected Answer: C\nAlthough its confusing and contradicting with Question 152, I would proceed to choose C for this. Question states no google account and the consultant is connected through VPN to the corporate network also its \"an instant\", not many.."
      }
    ]
  },
  {
    "id": 177,
    "source": "examtopics",
    "question": "After a recent security incident, your startup company wants better insight into what is happening in the Google Cloud environment. You need to monitor unexpected firewall changes and instance creation. Your company prefers simple solutions. What should you do?",
    "options": {
      "A": "Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Cloud Storage. Use BigQuery to periodically analyze log events in the storage bucket.",
      "B": "Use Cloud Logging filters to create log-based metrics for firewall and instance actions. Monitor the changes and set up reasonable alerts.",
      "C": "Install Kibana on a compute instance. Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Pub/Sub. Target the Pub/Sub topic to push messages to the Kibana instance. Analyze the logs on Kibana in real time.",
      "D": "Turn on Google Cloud firewall rules logging, and set up alerts for any insert, update, or delete events."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Stargazer11 Highly Voted 1Â year, 11Â months ago\nSelected Answer: B\nlog sink is advanced and it is used for routing logs to specific destinations.\nso answer B"
      },
      {
        "index": 2,
        "text": "Anonymous: BuenaCloudDE Most Recent 1Â year, 6Â months ago\nSelected Answer: B\nI think that key-words is \"After a recent security incident\", you need be notified if something happening with so important thing like secure."
      },
      {
        "index": 3,
        "text": "Anonymous: blackBeard33 1Â year, 11Â months ago\nSelected Answer: B\nB is a simple solution."
      },
      {
        "index": 4,
        "text": "Anonymous: interesting_owl 2Â years ago\nSelected Answer: A\nthis is simple."
      },
      {
        "index": 5,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: B\nPer ChatGPT, Option B provides a simple and effective solution using native Google Cloud services (Cloud Logging and log-based metrics) to monitor unexpected firewall changes and instance creation, while also allowing for the setup of reasonable alerts to ensure timely response to any security incidents."
      }
    ]
  },
  {
    "id": 178,
    "source": "examtopics",
    "question": "You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in the crm-databases project. You want to follow Google-recommended practices to grant access to the service account in the web-applications project. What should you do?",
    "options": {
      "A": "Grant \"project owner\" for web-applications appropriate roles to crm-databases.",
      "B": "Grant \"project owner\" role to crm-databases and the web-applications project.",
      "C": "Grant \"project owner\" role to crm-databases and roles/bigquery.dataViewer role to web-applications.",
      "D": "Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gocool28 Highly Voted 2Â years ago\nSelected Answer: D\nD is the least privilege and Google's recommended practices."
      },
      {
        "index": 2,
        "text": "Anonymous: 85c887f Most Recent 9Â months, 3Â weeks ago\nSelected Answer: C\nC option is not the best in part \"project owner\", but at least Project Owner will enable possibility to grant roles on this project. Option D is more confusion to me. Why in option D we \"Grant roles/bigquery.dataViewer role to crm-databases\"? Should not it be granted to the service account of the web-applications project as it supposed to need to access to datasets on crm-databases? AdelElagawany 3Â months, 1Â week ago\nThe service account which will be assigned the roles:\n- The SA is created in the project \"web-applications\", therefore the SA should be granted the required access to do its job in that project + BigQuery Job User in the same project \"web-applications\" to be able to run BigQuery Jobs (Able to query) from there.\n- In the \"crm-databases\", that SA should be able to access the BigQuery database so it must have \"Bigquery Data Viewer\" role there."
      },
      {
        "index": 3,
        "text": "Anonymous: yomi95 1Â year, 3Â months ago\nSelected Answer: D\nThe question does not describe any project requiring \"owner\" role access, hence granting that role to any of the project would violate least privilege.\nCan argue that crm-databases should have full access hence need owner role, but question does not mention specifically, and we only assume that."
      },
      {
        "index": 4,
        "text": "Anonymous: d52e44d 1Â year, 9Â months ago\nSelected Answer: A\nI had my exam today and select A. I did only because of these sentence \"service accounts for an application that spans multiple projects .\" not 100% sure if it's correct but service account for web apps needs permissions to span projects. Maybe I got it wrong but A makes sense.\nIt's tricky cause you don't know if web-apps will also do some updates on BigQuery or not."
      },
      {
        "index": 5,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: D\nD is the best answer and, for me, it was a process of elimination. The Project Owner role grants far-reaching permissions beyond what's needed for reading BQ datasets, violating the principle of least privilege."
      },
      {
        "index": 6,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: A\nInterpreting 'Project Owner' as the responsible entity, and not as the 'Project Owner' IAM role in Google Cloud: In this case, the instruction directs the person or entity managing the 'web-applications' project to grant appropriate roles for accessing the 'crm-databases' project. If this interpretation aligns with the intent of Option A, then it would indeed be a correct approach. Otherwise, none of the provided options would be correct. RKS_2021 1Â year, 4Â months ago\nWe need to assign roles to the service account. It should have read access on the crm project.\nD is correct. LautaroBarone 1Â year, 11Â months ago\nYou're managing the service accounts, why would you grant any role to 'web-applications' project owner? The most appropiate should be D, because you are granting a wrong role to the service accounts in 'crm-databases' project, but then the option says that appropiate roles will be granted to service accounts in 'web-applications' project."
      },
      {
        "index": 7,
        "text": "Anonymous: dan12q 2Â years ago\nIt is 116 question. The answer is D."
      },
      {
        "index": 8,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: D\nPer ChatGPT, Option D aligns with the principle of least privilege, provides separation of concerns between projects, and allows for granular access control, making it the best choice for granting access to the service account in the web-applications project to access BigQuery datasets in the crm-databases project while following Google-recommended practices. Cynthia2023 2Â years ago\nwhy give the role to the project crm-databases, it makes no sense."
      },
      {
        "index": 9,
        "text": "Anonymous: shiowbah 2Â years ago\nD. Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications."
      }
    ]
  },
  {
    "id": 179,
    "source": "examtopics",
    "question": "Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnetwork with range 172.16.20.128/25. There are no private IP addresses available in the subnetwork. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?",
    "options": {
      "A": "Modify the existing subnet range to 172.16.20.0/24.",
      "B": "Create a new Secondary IP Range in the VPC and configure the VMs to use that range.",
      "C": "Create a new VPC network for the VMs. Enable VPC Peering between the VMs'VPC network and the Dataproc cluster VPC network.",
      "D": "Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: apb98 Highly Voted 2Â years ago\nSelected Answer: A\nA. Same as question 129. Option A involves modifying the subnet range of the existing VPC network to increase the number of available IP addresses. By changing the subnet range to 172.16.20.0/24, you will have a larger IP address range to allocate to new VMs, allowing them to communicate with the Dataproc cluster.\nTo expand the IP range of a Compute Engine subnetwork, you can use:\ngcloud compute networks subnets expand-ip-range NAME Arjun727 1Â year, 11Â months ago\nModify is not equals to Expanding ashiqnazeem 1Â year, 11Â months ago\nnot the same question.\nQuestion #: 259 : \"There are no private IP addresses available in the subnetwork\"\nQuestion #: 129 : \"There are no private IP addresses available in the VPC network.\""
      },
      {
        "index": 2,
        "text": "Anonymous: srjulio1987 Most Recent 4Â months, 1Â week ago\nSelected Answer: B\nYou cannot simply change/expand the primary IP range of an existing subnet to another block."
      },
      {
        "index": 3,
        "text": "Anonymous: Jordarlu 9Â months, 3Â weeks ago\nSelected Answer: A\npls refer to https://cloud.google.com/blog/products/gcp/subnetwork-expansion-adds-even-more-flexibility-to-your-google-cloud-platform-private-networks"
      },
      {
        "index": 4,
        "text": "Anonymous: longph8 10Â months ago\nSelected Answer: B\nalthough â€œmodifyingâ€ (expanding) the primary range might be an option in some cases, the recommended solution in this scenario is Option B, as it is simpler, safer, and minimizes any potential disruptions to your existing resources."
      },
      {
        "index": 5,
        "text": "Anonymous: Esteban08 10Â months, 3Â weeks ago\nSelected Answer: B\nB. Create a new Secondary IP Range in the VPC and configure the VMs to use that range."
      },
      {
        "index": 6,
        "text": "Anonymous: 1826c27 11Â months, 2Â weeks ago\nSelected Answer: C\nA is incorrect because: \"You can expand the primary IPv4 range of an existing subnet by modifying its subnet mask, setting the prefix length to a smaller number\". PREFIX!"
      },
      {
        "index": 7,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\ngcloud compute networks subnets expand-ip-range - expand the IP range of a Compute Engine subnetwork"
      },
      {
        "index": 8,
        "text": "Anonymous: omunoz 1Â year, 8Â months ago\nThe question state \"using the minimum number of steps\" , then it should be A."
      },
      {
        "index": 9,
        "text": "Anonymous: kuracpalac 1Â year, 10Â months ago\nI would say A is the answer, but I have no idea what the Q means when specifying \"You want to add new VMs to communicate with your cluster using the minimum number of steps.\"\nDoes it mean that you want to add VMs and use the same subnet or add new VMs and use another subnet and then want those VMs communicating with the VMs in the other subnet?"
      },
      {
        "index": 10,
        "text": "Anonymous: STEVE_PEGLEG 1Â year, 11Â months ago\nSelected Answer: C\nThe reason A isn't correct is because you can only expand a subnet by \"setting the prefix length to a smaller number\"\nSee: https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet\nThe reason B isn't correct is because you can only use a secondary (aka 'alias') IP address when there is a primary already in place. In this scenario this isn't possible to do for the new VMs because there are no primary IP addresses available.\nTherefore C seems like a feasible approach, with fewer steps than D (even if D is possible, which I don't know)."
      }
    ]
  },
  {
    "id": 180,
    "source": "examtopics",
    "question": "You are building a backend service for an ecommerce platform that will persist transaction data from mobile and web clients. After the platform is launched, you expect a large volume of global transactions. Your business team wants to run SQL queries to analyze the data. You need to build a highly available and scalable data store for the platform. What should you do?",
    "options": {
      "A": "Create a multi-region Cloud Spanner instance with an optimized schema.",
      "B": "Create a multi-region Firestore database with aggregation query enabled.",
      "C": "Create a multi-region Cloud SQL for PostgreSQL database with optimized indexes.",
      "D": "Create a multi-region BigQuery dataset with optimized tables."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: apb98 Highly Voted 2Â years ago\nSelected Answer: A\nA. Key is â€œlarge volume of global transactionsâ€, so Cloud Spanner would be a good choice."
      },
      {
        "index": 2,
        "text": "Anonymous: Timfdklfajlksdjlakf Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nPiperMe perfectly summed it up. Rember: Global + SQL = Cloud Spanner"
      },
      {
        "index": 3,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: A\nA. Global + SQL = Cloud Spanner"
      },
      {
        "index": 4,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: A\nPer ChatGPT, Option A, creating a multi-region Cloud Spanner instance with an optimized schema, is the best choice for building a highly available and scalable data store that can efficiently handle global transactions and support SQL queries for analysis. 364db8d 4Â months ago\nYou are so smart"
      },
      {
        "index": 5,
        "text": "Anonymous: shiowbah 2Â years ago\nA. Create a multi-region Cloud Spanner instance with an optimized schema."
      }
    ]
  },
  {
    "id": 181,
    "source": "examtopics",
    "question": "You are in charge of provisioning access for all Google Cloud users in your organization. Your company recently acquired a startup company that has their own Google Cloud organization. You need to ensure that your Site Reliability Engineers (SREs) have the same project permissions in the startup company's organization as in your own organization. What should you do?",
    "options": {
      "A": "In the Google Cloud console for your organization, select Create role from selection, and choose destination as the startup company's organization.",
      "B": "In the Google Cloud console for the startup company, select Create role from selection and choose source as the startup company's Google Cloud organization.",
      "C": "Use the gcloud iam roles copy command, and provide the Organization ID of the startup company's Google Cloud Organization as the destination.",
      "D": "Use the gcloud iam roles copy command, and provide the project IDs of all projects in the startup company's organization as the destination."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: halifax 1Â year ago\nIf the correct answer is C ? How do you do it on the 'gcloud cli' command? I only know how to do it using project ID - can you do it using org ID?\ngcloud iam roles copy \\\n--source=\"roles/<source_role_id>\" \\\n--destination=<destination_role_name> \\\n--dest-project=<destination_project_id>\nI think, the 'gcloud cli ' command only copies roles from one organization to another within Google Cloud using project ID, not org ID!"
      },
      {
        "index": 2,
        "text": "Anonymous: yomi95 1Â year, 3Â months ago\nI'm confused between C and D, both use \"gcloud iam roles copy\" which is required for this scenario, but most voted answer is C while I'm unsure why giving project level permission is incorrect. According to lease privilege SRE should be okay to get permission project by project which is a hassle than C but it is not incorrect and can be done.\nAnyone pls correct me or give a link to an official document. yomi95 1Â year, 3Â months ago\nAccording to Gemini (Couldn't find official source) below is correct which means D is also possible answer. in the question is does not say fewest steps or best practice method.\ngcloud iam roles copy --source-organization=organizations/123456789 --destination-projects=projects/project1,projects/project2,projects/project3 --role-ids=roles/compute.admin,roles/storage.objectViewer denno22 1Â year, 3Â months ago\nWe could have a large number of projects, so copying by project may not be practical. Therefore copying by organisation is fewest steps.\nAlso just copying to all projects gives the same result as copying by organisation, but involves more steps."
      },
      {
        "index": 3,
        "text": "Anonymous: Cristianssenn 1Â year, 8Â months ago\nThe keys in question are SRE and project permissions. Choosing C will copy all your organization IAM permissions and not the only ones defined for SRE at project level.\nSo, the correct answer is D. iooj 1Â year, 4Â months ago\n--dest-project=DEST_PROJECT - the project of the destination role.\nWe cannot specify all projects. So, I think that's the trick here."
      },
      {
        "index": 4,
        "text": "Anonymous: Matheus_1346 1Â year, 8Â months ago\nSelected Answer: C\nThe correct one is C"
      },
      {
        "index": 5,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: C\nC. The gcloud iam roles copy command is designed specifically for copying IAM roles between different resources, including organizations."
      },
      {
        "index": 6,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: C\nPer ChatGPT, Option C is the correct choice for achieving consistency in project permissions across both organizations by leveraging the gcloud iam roles copy command and providing the Organization ID of the startup company's Google Cloud Organization as the destination."
      },
      {
        "index": 7,
        "text": "Anonymous: shiowbah 2Â years ago\nC. Use the gcloud iam roles copy command, and provide the Organization ID of the startup company's Google Cloud Organization as the destination."
      }
    ]
  },
  {
    "id": 182,
    "source": "examtopics",
    "question": "You need to extract text from audio files by using the Speech-to-Text API. The audio files are pushed to a Cloud Storage bucket. You need to implement a fully managed, serverless compute solution that requires authentication and aligns with Google-recommended practices. You want to automate the call to the API by submitting each file to the API as the audio file arrives in the bucket. What should you do?",
    "options": {
      "A": "Create an App Engine standard environment triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-TextAPI.",
      "B": "Run a Kubernetes job to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file.",
      "C": "Run a Python script by using a Linux cron job in Compute Engine to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file.",
      "D": "Create a Cloud Function triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-Text API."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: LaCubanita 1Â year, 3Â months ago\nWhy not App Engine? since it's also serverless."
      },
      {
        "index": 2,
        "text": "Anonymous: yomi95 1Â year, 3Â months ago\nSelected Answer: D\nReasons-\nServerless: Cloud Functions provide a serverless environment, eliminating the need for managing infrastructure.\nTriggered by Cloud Storage events: The Cloud Function can be triggered automatically as new audio files are added to the bucket, automating the process of submitting them to the API.\nAuthentication: Cloud Functions can use service accounts to authenticate with the Speech-to-Text API, ensuring secure access.\nFully managed: Cloud Functions handle the execution environment, scaling, and other operational aspects, reducing your management overhead.\nGoogle-recommended practices: Using Cloud Functions and Cloud Storage aligns with Google's recommended practices for building serverless applications."
      },
      {
        "index": 3,
        "text": "Anonymous: pzacariasf7 1Â year, 10Â months ago\nSelected Answer: D\nD, as PiperMe said."
      },
      {
        "index": 4,
        "text": "Anonymous: PiperMe 1Â year, 10Â months ago\nSelected Answer: D\n- Cloud Functions is serverless\n- It can be directly triggered by Cloud Storage events\n- Can integrate with the Speech-to-Text API"
      },
      {
        "index": 5,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: D\nPer ChatGPT, Option D provides a serverless, scalable, and fully managed solution that aligns with Google-recommended practices for automating the extraction of text from audio files using the Speech-to-Text API in response to Cloud Storage bucket events."
      },
      {
        "index": 6,
        "text": "Anonymous: shiowbah 2Â years ago\nD. Create a Cloud Function triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-Text API."
      }
    ]
  },
  {
    "id": 183,
    "source": "examtopics",
    "question": "Your customer wants you to create a secure website with autoscaling based on the compute instance CPU load. You want to enhance performance by storing static content in Cloud Storage. Which resources are needed to distribute the user traffic?",
    "options": {
      "A": "An external HTTP(S) load balancer with a managed SSL certificate to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend.",
      "B": "An external network load balancer pointing to the backend instances to distribute the load evenly. The web servers will forward the request to the Cloud Storage as needed.",
      "C": "An internal HTTP(S) load balancer together with Identity-Aware Proxy to allow only HTTPS traffic.",
      "D": "An external HTTP(S) load balancer to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend. Install the HTTPS certificates on the instance."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Cynthia2023 Highly Voted 2Â years ago\nSelected Answer: A\nWhy D is not correct:\nInstalling HTTPS Certificates on the Instance: While this approach can work, it's generally more efficient and secure to use a managed SSL certificate with the load balancer itself, rather than installing and managing certificates on each individual instance. Managing SSL certificates on the load balancer simplifies certificate management, especially in an autoscaling environment where new instances are frequently created and removed. yomi95 1Â year, 3Â months ago\nOption A: preferred.\nUses an external HTTP(S) load balancer with a managed SSL certificate. This means that the SSL certificate is managed by Google Cloud, and you don't need to install or renew it on your compute instances. This simplifies the management of your website and ensures that it is always accessible over HTTPS.\nOption D:\nUses an external HTTP(S) load balancer but requires you to install the HTTPS certificates on the instance. This means that you are responsible for managing and renewing the certificates, which can be more complex and time-consuming. kuracpalac 1Â year, 10Â months ago\nI'd say because the answer contains \"Install the HTTPS certificates on the instance.\", which is not a good way to go about things. You;d want the managed SSL cert (ans A) so that you don't have to worry about it. So A makes more sense."
      },
      {
        "index": 2,
        "text": "Anonymous: TanTran04 Most Recent 1Â year, 10Â months ago\nSelected Answer: A\nChoose option A"
      },
      {
        "index": 3,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: A\nChatGPT says the answer is A. KelvinToo 2Â years ago\nOption A aligns with the requirements of security, autoscaling, and performance optimization by leveraging Cloud Storage for static content while efficiently distributing user traffic."
      }
    ]
  },
  {
    "id": 184,
    "source": "examtopics",
    "question": "The core business of your company is to rent out construction equipment at large scale. All the equipment that is being rented out has been equipped with multiple sensors that send event information every few seconds. These signals can vary from engine status, distance traveled, fuel level, and more. Customers are billed based on the consumption monitored by these sensors. You expect high throughput â€“ up to thousands of events per hour per device â€“ and need to retrieve consistent data based on the time of the event. Storing and retrieving individual signals should be atomic. What should you do?",
    "options": {
      "A": "Create files in Cloud Storage as data comes in.",
      "B": "Create a file in Filestore per device, and append new data to that file.",
      "C": "Ingest the data into Cloud SQL. Use multiple read replicas to match the throughput.",
      "D": "Ingest the data into Bigtable. Create a row key based on the event timestamp."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: D\nhttps://cloud.google.com/bigtable/docs/overview#what-its-good-for"
      },
      {
        "index": 2,
        "text": "Anonymous: TanTran04 1Â year, 10Â months ago\nSelected Answer: D\nI saw this Q from somewhere before :3"
      },
      {
        "index": 3,
        "text": "Anonymous: sinh 2Â years ago\nThis is the same question as No. 139."
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: D\nit asks to retrieve consistent data based on the time of the event. D is the only option matching."
      },
      {
        "index": 5,
        "text": "Anonymous: apb98 2Â years ago\nSelected Answer: D\nD. Explanation:\n- Bigtable is a highly scalable, NoSQL database designed for high throughput and low-latency applications, making it suitable for scenarios with high ingest rates and rapid data retrieval.\n- Creating a row key based on the event timestamp would facilitate efficient retrieval of time-based data, ensuring consistency and atomicity for individual signals.\n- Bigtable's design allows for fast access to data using row keys, providing optimal performance when retrieving specific signals or events based on timestamps.\n- It also offers the scalability needed for handling thousands of events per hour per device. apb98 2Â years ago\nOptions A, B, and C might not efficiently handle the high throughput and atomic retrieval requirements:\n- Cloud Storage (Option A) might not offer the necessary atomicity for individual signal retrieval.\n- Filestore (Option B) could struggle with scaling and might not provide the atomic access needed for individual signal retrieval.\n- Cloud SQL (Option C) could face challenges in scaling to handle the high throughput effectively and might not match the required performance and atomicity for individual signal retrieval compared to Bigtable."
      },
      {
        "index": 6,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: D\nChatGPT says the answer is D."
      },
      {
        "index": 7,
        "text": "Anonymous: shiowbah 2Â years ago\nD. Ingest the data into Bigtable. Create a row key based on the event timestamp."
      }
    ]
  },
  {
    "id": 185,
    "source": "examtopics",
    "question": "You just installed the Google Cloud CLI on your new corporate laptop. You need to list the existing instances of your company on Google Cloud. What must you do before you run the gcloud compute instances list command? (Choose two.)",
    "options": {
      "A": "Run gcloud auth login, enter your login credentials in the dialog window, and paste the received login token to gcloud CLI.",
      "B": "Create a Google Cloud service account, and download the service account key. Place the key file in a folder on your machine where gcloud CLI can find it.",
      "C": "Download your Cloud Identity user account key. Place the key file in a folder on your machine where gcloud CLI can find it.",
      "D": "Run gcloud config set compute/zone $my_zone to set the default zone for gcloud CLI."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: apb98 Highly Voted 2Â years ago\nSelected Answer: A\nAE (I couldn't select both).\nIn addition to authenticating your account (Option A) to ensure the necessary permissions, setting the default project using `gcloud config set project [Your_Project_ID]` is crucial. It guarantees that subsequent commands are executed within the intended project's context. Both steps are essential before listing instances in Google Cloud Platform: proper authentication for permissions and setting the default project to ensure the commands run in the correct project context. apb98 2Â years ago\nD is not correct, as itâ€™s optional to set the zone."
      },
      {
        "index": 2,
        "text": "Anonymous: TanTran04 Highly Voted 1Â year, 10Â months ago\nSelected Answer: A\nI go with option A & E for basic actions"
      },
      {
        "index": 3,
        "text": "Anonymous: louisaok Most Recent 1Â year, 3Â months ago\nSelected Answer: E\nA & E is the right one"
      },
      {
        "index": 4,
        "text": "Anonymous: RKS_2021 1Â year, 4Â months ago\nSelected Answer: A\nAE correct options"
      },
      {
        "index": 5,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: E\nIt's A + E. Also thanks you so much for everyone else verifying. It's a huge help"
      },
      {
        "index": 6,
        "text": "Anonymous: leoalvarezh 1Â year, 11Â months ago\nSelected Answer: E\nIn my honest opinion A & E 1826c27 11Â months, 2Â weeks ago\nthank god you are so honest. it changes everything"
      },
      {
        "index": 7,
        "text": "Anonymous: Bagibo 2Â years ago\nIt can be E. So its A"
      },
      {
        "index": 8,
        "text": "Anonymous: SrinivasJasti 2Â years ago\nAE, auth and set project"
      },
      {
        "index": 9,
        "text": "Anonymous: KelvinToo 2Â years ago\nChatGPT says the answer is A and E. KelvinToo 2Â years ago\nVoting comment will only allow me to select one answer not both. 1826c27 11Â months, 2Â weeks ago\nmy grandma says it's BC RLIII 1Â year, 2Â months ago\nDE"
      },
      {
        "index": 10,
        "text": "Anonymous: shiowbah 2Â years ago\nA and E"
      }
    ]
  },
  {
    "id": 186,
    "source": "examtopics",
    "question": "You are planning to migrate your on-premises data to Google Cloud. The data includes:\n\nâ€¢ 200 TB of video files in SAN storage\nâ€¢ Data warehouse data stored on Amazon Redshift\nâ€¢ 20 GB of PNG files stored on an S3 bucket\n\nYou need to load the video files into a Cloud Storage bucket, transfer the data warehouse data into BigQuery, and load the PNG files into a second Cloud Storage bucket. You want to follow Google-recommended practices and avoid writing any code for the migration. What should you do?",
    "options": {
      "A": "Use gcloud storage for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files.",
      "B": "Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files.",
      "C": "Use Storage Transfer Service for the video files, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files.",
      "D": "Use Cloud Data Fusion for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Cynthia2023 Highly Voted 2Â years ago\nSelected Answer: B\nâ€¢ Transfer Appliance for Video Files: Given the large size of the video files (200 TB) in SAN storage, using Google's Transfer Appliance is a practical solution. Transfer Appliance is a hardware solution provided by Google for transferring large amounts of data. It is well-suited for scenarios where uploading data over the internet is too slow or not feasible.\nâ€¢ BigQuery Data Transfer Service for Data Warehouse Data: To migrate data from Amazon Redshift to BigQuery, the BigQuery Data Transfer Service is the appropriate tool. It automates the migration of data from several sources, including Amazon Redshift, into BigQuery.\nâ€¢ Storage Transfer Service for PNG Files: The Storage Transfer Service is ideal for moving data from Amazon S3 to Google Cloud Storage. It simplifies the process of importing your PNG files from S3 to a Cloud Storage bucket."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: B\nWe need the Transfer Appliance for the video."
      },
      {
        "index": 3,
        "text": "Anonymous: ac3baff 1Â year, 7Â months ago\nI think the answer is B. The controversy is whether the answer is B or C, i.e. whether one should use the transfer appliance or storage transfer service. From my knowledge, storage transfer service is not applicable for SAN, so the answer should be B."
      },
      {
        "index": 4,
        "text": "Anonymous: Stargazer11 1Â year, 11Â months ago\nSelected Answer: B\nIf it is on-prem to cloud, transfer appliance.\nStorage transfer service is for transferring data between buckets. sanjay1606 1Â year, 11Â months ago\nHi bro, I need to talk with you about GCP Associate Cloud Cngineer"
      },
      {
        "index": 5,
        "text": "Anonymous: apb98 2Â years ago\nSelected Answer: C\nC. Explanation:\n- Storage Transfer Service: It provides a straightforward way to transfer large amounts of data from an on-premises data source or cloud storage provider to Cloud Storage without writing code. This service can efficiently handle the migration of video files and PNG files.\n- BigQuery Data Transfer Service: This service allows the transfer of data from various sources, including Amazon Redshift, directly into BigQuery. It simplifies and automates the migration of data warehouse data without coding requirements. apb98 2Â years ago\nWhile options A and D involve using Dataflow and Cloud Data Fusion, respectively, these services may require additional configuration, development, or transformation logic, which is contrary to the requirement of avoiding code for migration.\nOption B suggests using Transfer Appliance for the videos, which might be applicable for large-scale physical data transfers, but it's not the most suitable option for this scenario involving Cloud migration without coding. blackBeard33 1Â year, 11Â months ago\nBut it is said that the data of the videos is on a SAN storage, is it san storage supported by google cloud storage transfer service? I cant see it here on this documentation: https://cloud.google.com/storage-transfer/docs/sources-and-sinks\nIt is not public accessible and I dont think it is a file system either. So option B should be the most suitable here. Dont you think ?"
      },
      {
        "index": 6,
        "text": "Anonymous: Bagibo 2Â years ago\nSelected Answer: B\nC makes sense. Use transfer appliance for vid"
      },
      {
        "index": 7,
        "text": "Anonymous: SrinivasJasti 2Â years ago\nC makes more sense"
      },
      {
        "index": 8,
        "text": "Anonymous: kaby1987 2Â years ago\nSelected Answer: B\nB. Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files: Transfer Appliance is designed for moving large amounts of data (like 200 TB of videos) into Google Cloud Storage. The BigQuery Data Transfer Service automates data movement from several sources, including Amazon Redshift, into BigQuery. Storage Transfer Service is appropriate for moving data from Amazon S3 to Google Cloud Storage."
      },
      {
        "index": 9,
        "text": "Anonymous: KelvinToo 2Â years ago\nSelected Answer: C\nChatGPT says the answer is C."
      },
      {
        "index": 10,
        "text": "Anonymous: shiowbah 2Â years ago\nB. Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files."
      }
    ]
  },
  {
    "id": 187,
    "source": "examtopics",
    "question": "You want to deploy a new containerized application into Google Cloud by using a Kubernetes manifest. You want to have full control over the Kubernetes deployment, and at the same time, you want to minimize configuring infrastructure. What should you do?",
    "options": {
      "A": "Deploy the application on GKE Autopilot.",
      "B": "Deploy the application on Cloud Run.",
      "C": "Deploy the application on GKE Standard.",
      "D": "Deploy the application on Cloud Functions."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Timfdklfajlksdjlakf Highly Voted 1Â year, 4Â months ago\nSelected Answer: A\nDespite the managed nature of the infrastructure of a GKE Autopilot Cluster, you still have full control over your Kubernetes workloads, configurations, and deployments. This allows you to use Kubernetes manifests and customize your deployment as needed."
      },
      {
        "index": 2,
        "text": "Anonymous: aditi86 Most Recent 8Â months ago\nSelected Answer: C\nA is incorect,Autopilot mode doesn't give you full control, as some configurations (like node management) are abstracted away."
      },
      {
        "index": 3,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nSelected Answer: A\nAutopilot mode is designed for those who prefer a hands-off approach. Itâ€™s best suited for businesses that want to leverage the power of Kubernetes without diving deep into its intricacies. It offers a simplified, managed experience with Google taking care of the operational overhead.\nOn the other hand, Standard mode is for those who want granular control over their Kubernetes environment. Itâ€™s ideal for businesses with specific requirements and those who have the expertise to manage and optimize their Kubernetes clusters."
      },
      {
        "index": 4,
        "text": "Anonymous: caminosdk 1Â year, 4Â months ago\nSelected Answer: C\nC is the answer iooj 1Â year, 4Â months ago\nbut not to this question"
      },
      {
        "index": 5,
        "text": "Anonymous: jhumpamp 1Â year, 5Â months ago\nSelected Answer: C\nKey requirements \"full control\" and \"minimize configuring infrastructure\" - GKE Standard Supports both"
      },
      {
        "index": 6,
        "text": "Anonymous: ashrafh 1Â year, 5Â months ago\nC is the answe"
      },
      {
        "index": 7,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: A\nMinimise effort. And GKE is a managed k8s service for deploying container-ised apps using k8s"
      },
      {
        "index": 8,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: A\nA- minimize effort"
      }
    ]
  },
  {
    "id": 188,
    "source": "examtopics",
    "question": "Your team is building a website that handles votes from a large user population. The incoming votes will arrive at various rates. You want to optimize the storage and processing of the votes. What should you do?",
    "options": {
      "A": "Save the incoming votes to Firestore. Use Cloud Scheduler to trigger a Cloud Functions instance to periodically process the votes.",
      "B": "Use a dedicated instance to process the incoming votes. Send the votes directly to this instance.",
      "C": "Save the incoming votes to a JSON file on Cloud Storage. Process the votes in a batch at the end of the day.",
      "D": "Save the incoming votes to Pub/Sub. Use the Pub/Sub topic to trigger a Cloud Functions instance to process the votes."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: BuenaCloudDE Highly Voted 1Â year, 6Â months ago\nSelected Answer: D\nPub/Sub: Google Cloud Pub/Sub is a messaging service which directly uses for this purpose.\nCloud Functions: By triggering a Cloud Function with a Pub/Sub topic, you can process the votes as they arrive, ensuring low-latency handling and efficient scaling based on demand. This approach provides real-time processing and can handle bursts of traffic effectively."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nUse Pub/Sub."
      },
      {
        "index": 3,
        "text": "Anonymous: Timfdklfajlksdjlakf 1Â year, 4Â months ago\nSelected Answer: D\nD. is the correct answer"
      },
      {
        "index": 4,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: D\nPub/Sub (a messaging service) triggering a Cloud Function (the glue between services that are otherwise independent) to trigger something else to process the votes"
      }
    ]
  },
  {
    "id": 189,
    "source": "examtopics",
    "question": "You are deploying an application on Google Cloud that requires a relational database for storage. To satisfy your companyâ€™s security policies, your application must connect to your database through an encrypted and authenticated connection that requires minimal management and integrates with Identity and Access Management (IAM). What should you do?",
    "options": {
      "A": "Deploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure a database user and password.",
      "B": "Deploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure IAM database authentication.",
      "C": "Deploy a Cloud SQL database and configure IAM database authentication. Access the database through the Cloud SQL Auth Proxy.",
      "D": "Deploy a Cloud SQL database and configure a database user and password. Access the database through the Cloud SQL Auth Proxy."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: BuenaCloudDE Highly Voted 1Â year, 6Â months ago\nSelected Answer: C\nCloud SQL Auth Proxy: This proxy ensures secure connections to your Cloud SQL database by automatically handling encryption (SSL/TLS) and IAM-based authentication. It simplifies the management of secure connections without needing to manage SSL/TLS certificates manually.\nIAM Database Authentication: This allows you to use IAM credentials to authenticate to the database, providing a unified and secure authentication mechanism that integrates seamlessly with Google Cloud IAM. BuenaCloudDE 1Â year, 6Â months ago\nA,B: You must managing SSL certification and database credentials.\nD: Relies on database-specific credentials rather than IAM, which doesn't fully leverage the benefits of IAM integration."
      },
      {
        "index": 2,
        "text": "Anonymous: ashrafh Most Recent 1Â year, 5Â months ago\nAs per Google Gemini answwer is B. NiveusSol 1Â year, 3Â months ago\nGoogle Gemini is not relaible"
      },
      {
        "index": 3,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: C\nInitially chose B but then read BuenaCloudDE's comment and agreed that managing SSL certs is more complicated than using Cloud SQL Auth Proxy"
      }
    ]
  },
  {
    "id": 190,
    "source": "examtopics",
    "question": "You have two Google Cloud projects: project-a with VPC vpc-a (10.0.0.0/16) and project-b with VPC vpc-b (10.8.0.0/16). Your frontend application resides in vpc-a and the backend API services are deployed in vpc-b. You need to efficiently and cost-effectively enable communication between these Google Cloud projects. You also want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Create an OpenVPN connection between vpc-a and vpc-b.",
      "B": "Create VPC Network Peering between vpc-a and vpc-b.",
      "C": "Configure a Cloud Router in vpc-a and another Cloud Router in vpc-b.",
      "D": "Configure a Cloud Interconnect connection between vpc-a and vpc-b."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nGoogle Cloud VPC Network Peering connects two Virtual Private Cloud (VPC) networks so that resources in each network can communicate with each other. Peered VPC networks can be in the same project, different projects of the same organization, or different projects of different organizations.\nhttps://cloud.google.com/vpc/docs/vpc-peering"
      },
      {
        "index": 2,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: B\nVPC Network Peering is the most efficient and cost-effective compared to the other options"
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: B\nVPC Network Peering: This allows private and secure communication between VPCs in different Google Cloud projects without using public IP addresses or VPN connections. It is cost-effective because it only incurs network egress charges within the same region and provides high-bandwidth, low-latency connectivity."
      },
      {
        "index": 4,
        "text": "Anonymous: RuchiMishra 1Â year, 6Â months ago\nSelected Answer: B\nWhy other options are not as suitable:\nA. OpenVPN connection: OpenVPN requires setting up and managing a VPN gateway, adding complexity and potential overhead.\nC. Cloud Routers: While Cloud Routers are powerful tools for managing dynamic routing, they are unnecessary for simple communication between two VPCs.\nD. Cloud Interconnect: Cloud Interconnect is a high-speed, dedicated connection for hybrid cloud environments. It's overkill for connecting two VPCs within GCP and would be much more expensive than VPC Network Peering."
      }
    ]
  },
  {
    "id": 191,
    "source": "examtopics",
    "question": "Your company is running a critical workload on a single Compute Engine VM instance. Your company's disaster recovery policies require you to back up the entire instanceâ€™s disk data every day. The backups must be retained for 7 days. You must configure a backup solution that complies with your companyâ€™s security policies and requires minimal setup and configuration. What should you do?",
    "options": {
      "A": "Configure the instance to use persistent disk asynchronous replication.",
      "B": "Configure daily scheduled persistent disk snapshots with a retention period of 7 days.",
      "C": "Configure Cloud Scheduler to trigger a Cloud Function each day that creates a new machine image and deletes machine images that are older than 7 days.",
      "D": "Configure a bash script using gsutil to run daily through a cron job. Copy the diskâ€™s files to a Cloud Storage bucket with archive storage class and an object lifecycle rule to delete the objects after 7 days."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Ciupaz 1Â year, 2Â months ago\nSelected Answer: B\nThis is a native GCP feature that requires minimal configuration and is fully managed."
      },
      {
        "index": 2,
        "text": "Anonymous: 33d6a28 1Â year, 2Â months ago\nSelected Answer: B\nThe key word is 'disk'"
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: B\nB is the answer."
      },
      {
        "index": 4,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: B\nYou can create snapshots from persistent disks. B is the simplest option"
      },
      {
        "index": 5,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: B\nB is answer."
      },
      {
        "index": 6,
        "text": "Anonymous: RuchiMishra 1Â year, 6Â months ago\nSelected Answer: B\nCompute Engine snapshots provide a fast and efficient way to back up the entire disk of a VM instance, including the operating system, applications, and data. They are incremental backups, meaning they only store the changes made since the last snapshot, which helps save storage costs."
      }
    ]
  },
  {
    "id": 192,
    "source": "examtopics",
    "question": "Your company requires that Google Cloud products are created with a specific configuration to comply with your companyâ€™s security policies. You need to implement a mechanism that will allow software engineers at your company to deploy and update Google Cloud products in a preconfigured and approved manner. What should you do?",
    "options": {
      "A": "Create Java packages that utilize the Google Cloud Client Libraries for Java to configure Google Cloud products. Store and share the packages in a source code repository.",
      "B": "Create bash scripts that utilize the Google Cloud CLI to configure Google Cloud products. Store and share the bash scripts in a source code repository.",
      "C": "Use the Google Cloud APIs by using curl to configure Google Cloud products. Store and share the curl commands in a source code repository.",
      "D": "Create Terraform modules that utilize the Google Cloud Terraform Provider to configure Google Cloud products. Store and share the modules in a source code repository."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: D\nHere's why this is the most suitable solution:\nInfrastructure as Code (IaC): Terraform is an IaC tool that allows you to define and provision infrastructure resources using declarative configuration files. This approach ensures consistency in resource configuration across different deployments, making it easier to enforce security policies and compliance requirements.\nModularity: Terraform modules promote reusability and maintainability. You can create modules for specific GCP products or configurations and share them within your organization. This reduces duplication of effort and ensures that all deployments adhere to the same standards."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nTerraform is the most commonly used tool to provision and automate Google Cloud infrastructure.\nhttps://cloud.google.com/docs/terraform/terraform-overview"
      }
    ]
  },
  {
    "id": 193,
    "source": "examtopics",
    "question": "You are a Google Cloud organization administrator. You need to configure organization policies and log sinks on Google Cloud projects that cannot be removed by project users to comply with your company's security policies. The security policies are different for each company department. Each company department has a user with the Project Owner role assigned to their projects. What should you do?",
    "options": {
      "A": "Use a standard naming convention for projects that includes the department name. Configure organization policies on the organization and log sinks on the projects.",
      "B": "Use a standard naming convention for projects that includes the department name. Configure both organization policies and log sinks on the projects.",
      "C": "Organize projects under folders for each department. Configure both organization policies and log sinks on the folders.",
      "D": "Organize projects under folders for each department. Configure organization policies on the organization and log sinks on the folders."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: C\nAs per question, the security policies are different for each company department. Hence, organizing each department in folders, Organizational Policies on Folders, and Log Sinks on Folders will work. Project owners cannot modify or remove organization policies applied at the folder or organization level."
      },
      {
        "index": 2,
        "text": "Anonymous: helloitsme123 Most Recent 1Â month ago\nSelected Answer: D\nOrganizing at folder level will address the need to separate different departments and log sinking. If organizational policies are needed, you should apply at organization level which will be inherited by folders and projects."
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: C\nRuchiMishra is right."
      }
    ]
  },
  {
    "id": 194,
    "source": "examtopics",
    "question": "You are deploying a web application using Compute Engine. You created a managed instance group (MIG) to host the application. You want to follow Google-recommended practices to implement a secure and highly available solution. What should you do?",
    "options": {
      "A": "Use SSL proxy load balancing for the MIG and an A record in your DNS private zone with the load balancer's IP address.",
      "B": "Use SSL proxy load balancing for the MIG and a CNAME record in your DNS public zone with the load balancerâ€™s IP address.",
      "C": "Use HTTP(S) load balancing for the MIG and a CNAME record in your DNS private zone with the load balancerâ€™s IP address.",
      "D": "Use HTTP(S) load balancing for the MIG and an A record in your DNS public zone with the load balancerâ€™s IP address."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kairosfc 1Â year, 5Â months ago\nSelected Answer: D\nD IS CORRECT"
      },
      {
        "index": 2,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: D\nD is correct - web application would use HTTP/S and you would need an A record which is public to access it"
      },
      {
        "index": 3,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: D\nD is correct!"
      },
      {
        "index": 4,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nCompliance question was already two times before. And I stay with HTTP(S) load balancer because it recommend practices in Associate Cloud Engineer Path.\nHTTP(S) Load Balancing: This is a globally distributed, managed service for HTTP and HTTPS traffic that provides high availability, automatic scaling, and support for SSL termination. It ensures your web application is secure and can handle varying traffic loads efficiently.\nA Record in DNS Public Zone: An A record maps your domain name to the IP address of the load balancer, making your application accessible to users over the internet. Using a public DNS zone ensures that your application is reachable globally. BuenaCloudDE 1Â year, 6Â months ago\nSSL Proxy Load Balancing with DNS Private Zone and A record: SSL Proxy Load Balancing is suitable for non-HTTP(S) traffic and not recommended for web applications serving HTTP/HTTPS content. Using a private DNS zone would restrict access to internal networks, not the internet."
      }
    ]
  },
  {
    "id": 195,
    "source": "examtopics",
    "question": "You have several hundred microservice applications running in a Google Kubernetes Engine (GKE) cluster. Each microservice is a deployment with resource limits configured for each container in the deployment. You've observed that the resource limits for memory and CPU are not appropriately set for many of the microservices. You want to ensure that each microservice has right sized limits for memory and CPU. What should you do?",
    "options": {
      "A": "Configure a Vertical Pod Autoscaler for each microservice.",
      "B": "Modify the cluster's node pool machine type and choose a machine type with more memory and CPU.",
      "C": "Configure a Horizontal Pod Autoscaler for each microservice.",
      "D": "Configure GKE cluster autoscaling."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nHere's why a Vertical Pod Autoscaler (VPA) is the most suitable solution for this scenario:\nRight-Sizing Resources: VPA is designed to automatically adjust the resource requests and limits (CPU and memory) for pods based on their actual usage. This ensures that pods have enough resources to run efficiently without being over-provisioned, which can lead to wasted resources and higher costs.\nAutomated Optimization: VPA continuously monitors the resource usage of your pods and recommends optimal settings. You can choose to apply these recommendations automatically or manually, giving you flexibility and control over the process.\nMicroservice-Specific Tuning: By configuring a VPA for each microservice, you can fine-tune the resource allocation for each individual service based on its specific needs and usage patterns. This is more efficient than making blanket changes to the entire cluster or node pool."
      },
      {
        "index": 2,
        "text": "Anonymous: RLIII Most Recent 1Â year, 2Â months ago\nIt is A\nOption A: Configure a Vertical Pod Autoscaler for each microservice is the best solution because the VPA automatically adjusts the resource requests and limits for each microservice, ensuring that memory and CPU resources are correctly sized according to their actual usage patterns. This will address your issue of inappropriate resource limits for many of the microservices in your GKE cluster."
      },
      {
        "index": 3,
        "text": "Anonymous: jhumpamp 1Â year, 5Â months ago\nSelected Answer: C\nAs mentioned \"resource limits configured for each container in the deployment\", so can not vertically increase resources/memory\nCluster scale out is not relevant here.\nWith Horizontal scaling, adding more pods ultimately makes sure each service have \"right sized memory and CPU\""
      },
      {
        "index": 4,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: A\nVertical is for more specific resources of the individual pods. Horizontal is for creating more copies of the instances (adding more pods)."
      },
      {
        "index": 5,
        "text": "Anonymous: user636 1Â year, 6Â months ago\nSelected Answer: A\nA seems better"
      }
    ]
  },
  {
    "id": 196,
    "source": "examtopics",
    "question": "Your company uses BigQuery to store and analyze data. Upon submitting your query in BigQuery, the query fails with a quotaExceeded error. You need to diagnose the issue causing the error. What should you do? (Choose two.)",
    "options": {
      "A": "Use BigQuery BI Engine to analyze the issue.",
      "B": "Use the INFORMATION_SCHEMA views to analyze the underlying issue.",
      "C": "Configure Cloud Trace to analyze the issue.",
      "D": "Search errors in Cloud Audit Logs to analyze the issue."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: BD\nB & D.\nHere's why these two methods are crucial for diagnosing quotaExceeded errors in BigQuery:\nB. INFORMATION_SCHEMA Views: BigQuery's INFORMATION_SCHEMA provides metadata about datasets, tables, and jobs. Relevant views like JOBS_BY_PROJECT and JOBS_BY_USER can help you analyze recent queries, their resource consumption (bytes processed, slots used), and any errors encountered. This can reveal which queries are exceeding quotas and what type of quota (e.g., query size, daily limit) is being exceeded.\nD. Cloud Audit Logs: Audit logs record all API calls and administrative actions within your GCP projects. By searching for quotaExceeded errors in the audit logs, you can see the exact error messages, timestamps, and potentially the queries that triggered the error. This helps pinpoint the specific resources and actions causing the issue."
      },
      {
        "index": 2,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: BD\nhttps://cloud.google.com/bigquery/docs/troubleshoot-quotas#diagnosis"
      },
      {
        "index": 3,
        "text": "Anonymous: Chetantest07 1Â year, 3Â months ago\nSelected Answer: BD\nhttps://cloud.google.com/bigquery/docs/troubleshoot-quotas"
      },
      {
        "index": 4,
        "text": "Anonymous: user636 1Â year, 6Â months ago\nSelected Answer: BD\nBD is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: BD\na. Query INFORMATION_SCHEMA to identify requests that exceed quotas.\nb. See the Cloud Audit Logs for more details on 'quotaExceeded' errors and the context in which they occur."
      }
    ]
  },
  {
    "id": 197,
    "source": "examtopics",
    "question": "Your team has developed a stateless application which requires it to be run directly on virtual machines. The application is expected to receive a fluctuating amount of traffic and needs to scale automatically. You need to deploy the application. What should you do?",
    "options": {
      "A": "Deploy the application on a managed instance group and configure autoscaling.",
      "B": "Deploy the application on a Kubernetes Engine cluster and configure node pool autoscaling.",
      "C": "Deploy the application on Cloud Functions and configure the maximum number instances.",
      "D": "Deploy the application on Cloud Run and configure autoscaling."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nHere's why A is the most suitable solution:\nManaged Instance Groups (MIGs): MIGs are designed to manage groups of identical VMs, making them ideal for running stateless applications. They provide features like auto-scaling, auto-healing, and load balancing, which are crucial for handling fluctuating traffic.\nAutoscaling: You can configure autoscaling policies to automatically add or remove VM instances based on metrics like CPU utilization, HTTP load balancing traffic, or Stackdriver Monitoring metrics. This ensures that your application can scale up to handle peak traffic and scale down during periods of low demand."
      },
      {
        "index": 2,
        "text": "Anonymous: JoseCloudEng1994 Most Recent 1Â year ago\nSelected Answer: A\nKey word \"virtual instances\". Otherwise I would have picked Cloud Run"
      },
      {
        "index": 3,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: A\nIf the application did not have to be run on VMs I would have chosen D as Cloud Run would be easier and can scale to zero which reduces idle-time cost. But the app does, so MIGs is the choice."
      },
      {
        "index": 4,
        "text": "Anonymous: user636 1Â year, 6Â months ago\nSelected Answer: A\nMIG supports stateless applications."
      },
      {
        "index": 5,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: A\nVote for A"
      }
    ]
  },
  {
    "id": 198,
    "source": "examtopics",
    "question": "Your web application is hosted on Cloud Run and needs to query a Cloud SQL database. Every morning during a traffic spike, you notice API quota errors in Cloud SQL logs. The project has already reached the maximum API quota. You want to make a configuration change to mitigate the issue. What should you do?",
    "options": {
      "A": "Modify the minimum number of Cloud Run instances.",
      "B": "Use traffic splitting.",
      "C": "Modify the maximum number of Cloud Run instances.",
      "D": "Set a minimum concurrent requests environment variable for the application."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: RuchiMishra Highly Voted 1Â year, 6Â months ago\nSelected Answer: A\nHere's why A is the most effective solution to mitigate API quota errors during traffic spikes:\nCold Starts and API Calls: Cloud Run services scale to zero when not in use. When a new request arrives, a new instance is spun up, leading to a cold start. During this cold start, multiple API calls might be made to initialize the application and connect to the Cloud SQL database. If there's a sudden spike in traffic, a large number of cold starts can occur simultaneously, exceeding the Cloud SQL API quota.\nMinimum Instances: By setting a minimum number of Cloud Run instances, you can ensure that a few instances are always running, even during periods of low traffic. This eliminates cold starts during traffic spikes and reduces the number of concurrent API calls made to Cloud SQL, helping you stay within the quota limits."
      },
      {
        "index": 2,
        "text": "Anonymous: flummoxed_individual Highly Voted 1Â year, 5Â months ago\nSelected Answer: A\nThere has been a previous question relating to this issue which is caused by Cold Starts (RuchiMishra explains this). Solving the issue would be by configuring a minimum number of instances always running"
      },
      {
        "index": 3,
        "text": "Anonymous: Lutech Most Recent 9Â months, 4Â weeks ago\nSelected Answer: C\nSetting a minimum number of Cloud Run instances ensures that there are always a few instances running, even when there is no traffic.\nWhile this would prevent cold starts, it doesn't directly address the traffic spike issue, where the number of active instances needs to scale to accommodate the load.\nDuring a traffic spike, if the maximum number of Cloud Run instances is not capped, Cloud Run will still spin up new instances to handle the increased load. This can cause more database connections than Cloud SQL can handle, leading to API quota errors."
      },
      {
        "index": 4,
        "text": "Anonymous: Esteban08 10Â months, 3Â weeks ago\nSelected Answer: D\nCloud Run allows you to configure a concurrency setting (using the --concurrency flag when deploying the service). In this context, â€œset a minimum concurrent requests environment variableâ€ refers to configuring your service so that each instance handles a higher number of concurrent requests. Therefore, don't reach the API quota errors due too many cloud run instances created and doing requests at the same time."
      },
      {
        "index": 5,
        "text": "Anonymous: jlocke 11Â months, 1Â week ago\nA minimum number of running instances helps reduce cold start delays, but it does not prevent new instances from scaling up rapidly during a traffic spike.\nThe API quota issue occurs when too many Cloud Run instances spawn too quickly, each establishing new connections to Cloud SQL."
      },
      {
        "index": 6,
        "text": "Anonymous: meh_33 1Â year, 5Â months ago\nSelected Answer: A\nA Make sense"
      },
      {
        "index": 7,
        "text": "Anonymous: user636 1Â year, 6Â months ago\nSelected Answer: A\nAs explained by RuchiMishra, we need to keep a minimum number of instances always running. 1826c27 11Â months, 2Â weeks ago\nAs explained by user636, we need read explanation of RuchiMishra to keep a minimum number of instances always running."
      }
    ]
  },
  {
    "id": 199,
    "source": "examtopics",
    "question": "You need to deploy a single stateless web application with a web interface and multiple endpoints. For security reasons, the web application must be reachable from an internal IP address from your company's private VPC and on-premises network. You also need to update the web application multiple times per day with minimal effort and want to manage a minimal amount of cloud infrastructure. What should you do?",
    "options": {
      "A": "Deploy the web application on Google Kubernetes Engine standard edition with an internal ingress.",
      "B": "Deploy the web application on Cloud Run with Private Google Access configured.",
      "C": "Deploy the web application on Cloud Run with Private Service Connect configured.",
      "D": "Deploy the web application to GKE Autopilot with Private Google Access configured."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Timfdklfajlksdjlakf Highly Voted 1Â year, 4Â months ago\nSelected Answer: C\nOption B: Private Google Access allows internal Google Cloud resource access but does not make Cloud Run services accessible from on-premises networks.\nOption C: Private Service Connect creates private endpoints that are accessible from your VPC, and with the proper network configuration (e.g., VPN or Interconnect), allows access from on-premises networks."
      },
      {
        "index": 2,
        "text": "Anonymous: bad5fad Highly Voted 1Â year, 4Â months ago\nSelected Answer: C\nWe need to connect to the on-premise network. Private google connect can enable that"
      },
      {
        "index": 3,
        "text": "Anonymous: rohitgeeked Most Recent 1Â year ago\nThe correct answer is C.\nHere's the documentation:\nhttps://cloud.google.com/run/docs/securing/private-networking#from-on-prem"
      },
      {
        "index": 4,
        "text": "Anonymous: Davyies 1Â year, 5Â months ago\nSelected Answer: C\nWe need to connect to the on-premise network. Private google access does not enable this."
      },
      {
        "index": 5,
        "text": "Anonymous: flummoxed_individual 1Â year, 5Â months ago\nSelected Answer: B\nThe sentence \"you also need to update the web application multiple times per day with minimal effort and want to manage a minimal amount of cloud infrastructure\" makes Cloud Run favourable over GKE."
      },
      {
        "index": 6,
        "text": "Anonymous: BuenaCloudDE 1Â year, 6Â months ago\nSelected Answer: B\nThe most important thing that told me to choose B is the easier upgrade that Cloud Run provides."
      },
      {
        "index": 7,
        "text": "Anonymous: RuchiMishra 1Â year, 6Â months ago\nSelected Answer: B\nHere's why B is the most suitable for the given requirements:\nCloud Run: Cloud Run is a fully managed serverless platform for containerized applications. It eliminates the need to manage infrastructure, making it easy to deploy and update your web application multiple times a day with minimal effort. It also scales automatically based on traffic.\nPrivate Google Access (PGA): PGA allows resources in a private VPC network (without public IP addresses) to access Google APIs and services, including Cloud Run. This enables you to keep your web application private while still making it accessible from your internal network and on-premises environment. Timfdklfajlksdjlakf 1Â year, 4Â months ago\nIt doesn't create private endpoints"
      }
    ]
  },
  {
    "id": 200,
    "source": "examtopics",
    "question": "You use Cloud Logging to capture application logs. You now need to use SQL to analyze the application logs in Cloud Logging, and you want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Develop SQL queries by using Gemini for Google Cloud.",
      "B": "Enable Log Analytics for the log bucket and create a linked dataset in BigQuery.",
      "C": "Create a schema for the storage bucket and run SQL queries for the data in the bucket.",
      "D": "Export logs to a storage bucket and create an external view in BigQuery."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Phat 10Â months, 1Â week ago\nSelected Answer: B\nIt's B"
      },
      {
        "index": 2,
        "text": "Anonymous: peddyua 11Â months, 3Â weeks ago\nSelected Answer: B\nRecommended practice: Google recommends enabling Log Analytics for log buckets, which seamlessly integrates Cloud Logging with BigQuery.\nEfficiency: It provides a native, real-time integration for querying logs using SQL without the need for additional exports or complex configurations.\nAutomation: Log Analytics automatically creates a linked dataset in BigQuery, simplifying setup and maintenance."
      },
      {
        "index": 3,
        "text": "Anonymous: 26b39bb 1Â year, 1Â month ago\nSelected Answer: B\nB is the correct answer"
      }
    ]
  },
  {
    "id": 201,
    "source": "examtopics",
    "question": "You need to deploy a third-party software application onto a single Compute Engine VM instance. The application requires the highest speed read and write disk access for the internal database. You need to ensure the instance will recover on failure. What should you do?",
    "options": {
      "A": "Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateful managed instance group.",
      "B": "Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateless managed instance group.",
      "C": "Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateful managed instance group.",
      "D": "Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateless managed instance group."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: samyakaparna 1Â year, 1Â month ago\nSelected Answer: C\nhttps://cloud.google.com/compute/docs/disks/hyperdisks\nHyperdisk volumes feature substantially better performance than Persistent Disk. With Hyperdisk, you get dedicated IOPS and throughput with each volume, as compared to Persistent Disk where performance is shared between volumes of the same type."
      },
      {
        "index": 2,
        "text": "Anonymous: 26b39bb 1Â year, 1Â month ago\nSelected Answer: C\nFor performance-critical applications, use Hyperdisk Extreme if Extreme Persistent Disk isn't supported or doesn't provide enough performance. Hyperdisk Extreme disks feature higher maximum IOPS and throughput along with low sub-millisecond latencies, and offer high performance for the most demanding workloads, such as high performance databases.\nhttps://cloud.google.com/compute/docs/disks/hyperdisks#:~:text=Hyperdisk%20Extreme%20disks%20feature%20higher,such%20as%20high%20performance%20databases."
      },
      {
        "index": 3,
        "text": "Anonymous: Moin23 1Â year, 1Â month ago\nSelected Answer: C\nHyperdisk Extreme disks feature higher maximum IOPS and throughput along with low sub-millisecond latencies, and offer high performance for the most demanding workloads, such as high performance databases."
      },
      {
        "index": 4,
        "text": "Anonymous: Ciupaz 1Â year, 1Â month ago\nSelected Answer: A\nHyperdisk Extreme does not exists in GCP. halifax 1Â year ago\nWhat is Hyperdisk Extreme?\nHyperdisk Extreme is a high-performance block storage service that's part of Google Cloud's Compute Engine platform. It's designed for workloads that require high performance, such as real-time analytics and machine learning model training."
      }
    ]
  },
  {
    "id": 202,
    "source": "examtopics",
    "question": "You have a VM instance running in a VPC with single-stack subnets. You need to ensure that the VM instance has a fixed IP address so that other services hosted in the same VPC can communicate with the VM. You want to follow Google-recommended practices while minimizing cost. What should you do?",
    "options": {
      "A": "Promote the existing IP address of the VM to become a static external IP address.",
      "B": "Promote the existing IP address of the VM to become a static internal IP address.",
      "C": "Reserve a new static external IPv6 address and assign the new IP address to the VM.",
      "D": "Reserve a new static external IP address and assign the new IP address to the VM."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sahuprashant123 1Â year, 1Â month ago\nSelected Answer: B\nFixed IP Address for Communication Within the Same VPC:\nSince the services need to communicate within the same VPC and you want a fixed IP address, the best practice is to use a static internal IP address. This ensures that the VM will always have the same internal IP, which can be used for communication between services within the same VPC.\nTo minimize cost while ensuring that the VM instance has a fixed IP for internal communication within the VPC, promote the existing internal IP address to be static. This follows Google-recommended practices and ensures cost-effectiveness."
      },
      {
        "index": 2,
        "text": "Anonymous: 26b39bb 1Â year, 1Â month ago\nSelected Answer: B\nA. Promote the existing IP address of the VM to become a static external IP address.-> Incorrect you do not need an external IP because all the devices are within the same natework.\nB. Promote the existing IP address of the VM to become a static internal IP address.-> Correct Internal Static IP allows communication with all the devices within the same VPC while maintaining the same IP.\nC. Reserve a new static external IPv6 address and assign the new IP address to the VM.-> Incorrect. You do not need to define IPv6 IP\nD. Reserve a new static external IP address and assign the new IP address to the VM -> Incorrect you do not need an external IP"
      },
      {
        "index": 3,
        "text": "Anonymous: Ciupaz 1Â year, 1Â month ago\nSelected Answer: B\nB is the correct answer."
      }
    ]
  },
  {
    "id": 203,
    "source": "examtopics",
    "question": "Your preview application, deployed on a single-zone Google Kubernetes Engine (GKE) cluster in us-central1, has gained popularity. You are now ready to make the application generally available. You need to deploy the application to production while ensuring high availability and resilience. You also want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Use the gcloud container clusters create command with the options --enable-multi-networking and --enable-autoscaling to create an autoscaling zonal cluster and deploy the application to it.",
      "B": "Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it.",
      "C": "Use the gcloud container clusters update command with the option --region us-central1 to update the cluster and deploy the application to it.",
      "D": "Use the gcloud container clusters update command with the option --node-locations us-central1-a,us-central1-b to update the cluster and deploy the application to the nodes."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SkyZeroZx Highly Voted 1Â year ago\nSelected Answer: B\nIf you got this far, you have the will.\nGood luck on your exam"
      },
      {
        "index": 2,
        "text": "Anonymous: dead1407 Most Recent 4Â months, 2Â weeks ago\nSelected Answer: D\nFor high availability and resilience, Google-recommended practice is to run your GKE cluster across multiple zones within a region. Using --node-locations allows you to specify multiple zones (e.g., us-central1-a,us-central1-b), ensuring your application remains available even if one zone fails."
      },
      {
        "index": 3,
        "text": "Anonymous: sahuprashant123 1Â year, 1Â month ago\nSelected Answer: D\nWhy B is not correct -\nB. Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it: Autopilot clusters manage infrastructure automatically, but it does not specifically address the need for multi-zone deployment. Autopilot clusters are typically easier to manage but may not provide as much control over specific zone selection. 26b39bb 1Â year, 1Â month ago\nStill the autopilot will provide at least regional scalability by default. With option B you only covering two regions which are not even all the zones in that region"
      },
      {
        "index": 4,
        "text": "Anonymous: 26b39bb 1Â year, 1Â month ago\nSelected Answer: B\nAutopilot cluster will ensure global scalability: https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview\ncreate-auto: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster"
      },
      {
        "index": 5,
        "text": "Anonymous: Ciupaz 1Â year, 1Â month ago\nSelected Answer: B\nB is the correct answer."
      }
    ]
  },
  {
    "id": 204,
    "source": "examtopics",
    "question": "You are developing an application that will be deployed on Google Cloud. The application will use a service account to retrieve data from BigQuery. Before you deploy your application, you want to test the permissions of this service account from your local machine to ensure there will be no authentication issues. You want to ensure that you use the most secure method while following Google-recommended practices. What should you do?",
    "options": {
      "A": "Generate a service account key, and configure the gcloud CLI to use this key. Issue a relevant BigQuery request through the gdoud CLI to test the access.",
      "B": "Grant the service account the BigQuery Administrator IAM role to ensure the service account has all required access.",
      "C": "Configure the gcloud CLI to use service account impersonation. Issue a relevant BigQuery request through the gcloud CLI to test the access.",
      "D": "Configure the gcloud CLI with Application Default Credentials using your user account. Issue a relevant BigQuery request through the gcloud CLI to test the access."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AdelElagawany 3Â months ago\nSelected Answer: C"
      },
      {
        "index": 1,
        "text": "Get the key of the SA: gcloud iam service-accounts keys create <type-any-name-for-the-key> --iam-account=<Your SA>"
      },
      {
        "index": 2,
        "text": "Impersonate the SA: gcloud auth activate-service-account <Your SA> --key-file <Your SA Key File Name>"
      },
      {
        "index": 3,
        "text": "Run a query on the SA behalf: bq query --use_legacy_sql=false 'SELECT * FROM mytable limit1;'"
      },
      {
        "index": 2,
        "text": "Anonymous: AdelElagawany 3Â months ago\nSelected Answer: A"
      },
      {
        "index": 1,
        "text": "Get the key of the SA: gcloud iam service-accounts keys create <type-any-name-for-the-key> --iam-account=<Your SA>"
      },
      {
        "index": 2,
        "text": "Impersonate the SA: gcloud auth activate-service-account <Your SA> --key-file <Your SA Key File Name>"
      },
      {
        "index": 3,
        "text": "Run a query on the SA behalf: bq query --use_legacy_sql=false 'SELECT * FROM mytable limit1;'"
      },
      {
        "index": 3,
        "text": "Anonymous: Mika_Ro 10Â months, 3Â weeks ago\nSelected Answer: C\nC is the best way to securely test the permissions of the service account from your local machine"
      },
      {
        "index": 4,
        "text": "Anonymous: Esteban08 10Â months, 3Â weeks ago\nSelected Answer: C\nGoogle-recommended practices advise against distributing or storing long-lived service account keys because they can be a security risk. Instead, service account impersonation allows you to use your own credentials to \"borrow\" the identity of the service account without needing to download a key file. This method is more secure."
      }
    ]
  },
  {
    "id": 205,
    "source": "examtopics",
    "question": "Your organization is migrating to Google Cloud. You want only users with company-issued Google accounts to access your Google Cloud environment. You must ensure that users of the same department can only access resources within their own department. You want to minimize operational costs while following Google-recommended practices. What should you do?",
    "options": {
      "A": "Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Periodically identify and remove non-company issued Google accounts.",
      "B": "Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Use organization policies to block non-company issued emails.",
      "C": "Create a folder for each department in Resource Manager. Grant the users of each department the Folder Admin role on the folder of their department.",
      "D": "Create a folder for each department in Resource Manager. Grant all company users the Folder Admin role on the organization level."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: B\nB is definetely the right answer."
      },
      {
        "index": 2,
        "text": "Anonymous: SajadAhm 3Â weeks ago\nSelected Answer: B\nGoogle groups and the use of company policies are aligned with best practices in google and prevent manual overhead"
      },
      {
        "index": 3,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nWhy B is the best answer:\na) Using Google Groups and IAM roles:\nGoogle Groups provides efficient management of user permissions\nIAM roles allow granular control over resource access\nThis is a Google-recommended practice for managing permissions at scale\nb) Organization policies to block non-company emails:\nOrganization policies provide a centralized way to enforce security controls\nCan specifically block non-company email domains\nPrevents unauthorized access attempts at the organization level\nAutomated enforcement reduces operational overhead"
      }
    ]
  },
  {
    "id": 206,
    "source": "examtopics",
    "question": "You are deploying an application to Cloud Run. Your application requires the use of an API that runs on Google Kubernetes Engine (GKE). You need to ensure that your Cloud Run service can privately reach the API on GKE, and you want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Deploy an ingress resource on the GKE cluster to expose the API to the internet. Use Cloud Armor to filter for IP addresses that can connect to the API. On the Cloud Run service, configure the application to fetch its public IP address and update the Cloud Armor policy on startup to allow this IP address to call the API on ports 80 and 443.",
      "B": "Create an ingress firewall rule on the VPC to allow connections from 0.0.0.0/0 on ports 80 and 443.",
      "C": "Create an egress firewall rule on the VPC to allow connections to 0.0.0.0/ on ports 80 and 443.",
      "D": "Deploy an internal Application Load Balancer to expose the API on GKE to the VPC. Configure Cloud DNS with the IP address of the internal Application Load Balancer. Deploy a Serverless VPC Access connector to allow the Cloud Run service to call the API through the FQDN on Cloud DNS."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: D\nCloud Run is a serverless product and does not live \"inside\" your VPC by default. A Serverless VPC Access connector (or the more modern Direct VPC Egress) is the required component that allows Cloud Run to send outbound (egress) traffic into your VPC network."
      },
      {
        "index": 2,
        "text": "Anonymous: SajadAhm 3Â weeks ago\nSelected Answer: D\nIf you want to grant permissions to a Cloud Run service to access resources inside a VPC, always use \"Serverless VPC Access connector\""
      },
      {
        "index": 3,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: D\nwhy this D is the best solution:\nSecurity Best Practices:\nThis solution maintains private connectivity without exposing the API to the internet\nIt follows the principle of least privilege\nIt uses internal networking rather than public IP addresses\nComponents and their roles:\nInternal Application Load Balancer: Exposes the GKE API internally within the VPC\nCloud DNS: Provides DNS resolution for the internal service\nServerless VPC Access connector: Enables Cloud Run to access VPC resources"
      }
    ]
  },
  {
    "id": 207,
    "source": "examtopics",
    "question": "Your company uses a multi-cloud strategy that includes Google Cloud. You want to centralize application logs in a third-party software-as-a-service (SaaS) tool from all environments. You need to integrate logs originating from Cloud Logging, and you want to ensure the export occurs with the least amount of delay possible. What should you do?",
    "options": {
      "A": "Create a Cloud Logging sink and configure BigQuery as the destination. Configure the SaaS tool to query BigQuery to retrieve the logs.",
      "B": "Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs.",
      "C": "Create a Cloud Logging sink and configure Cloud Storage as the destination. Configure the SaaS tool to read the Cloud Storage bucket to retrieve the logs.",
      "D": "Use a Cloud Scheduler cron job to trigger a Cloud Function that queries Cloud Logging and sends the logs to the SaaS tool."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: B\nSaaS tool can subscribe to the Pub/Sub topic and receive logs as they arrive"
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nThe correct answer is B: Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs.\nHere's why:\nRequirement for minimal delay:\nPub/Sub provides near real-time messaging capabilities\nMessages are delivered as soon as they are published\nSupports push and pull subscription models\nIntegration capabilities:\nPub/Sub is designed for event-driven architectures and real-time data streaming\nMany SaaS tools have built-in support for Pub/Sub integration\nProvides reliable message delivery with at-least-once semantics"
      }
    ]
  },
  {
    "id": 208,
    "source": "examtopics",
    "question": "You are planning to migrate a database and a backend application to a Standard Google Kubernetes Engine (GKE) cluster. You need to prevent data loss and make sure there are enough nodes available for your backend application based on the demands of your workloads. You want to follow Google-recommended practices and minimize the amount of manual work required. What should you do?",
    "options": {
      "A": "Run your database as a StatefulSet. Configure cluster autoscaling to handle changes in the demands of your workloads.",
      "B": "Run your database as a single Pod. Run the resize command when you notice changes in the demands of your workloads.",
      "C": "Run your database as a DaemonSet. Run the resize command when you notice changes in the demands of your workloads.",
      "D": "Run your database as a Deployment. Configure cluster autoscaling to handle changes in the demands of your workloads."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: A\nDatabases are stateful -> StatefulSet"
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: A\nPrevent data loss --> StatefulSet"
      }
    ]
  },
  {
    "id": 209,
    "source": "examtopics",
    "question": "You are the Organization Administrator for your company's Google Cloud resources. Your company has strict compliance rules that require you to be notified about any modifications to files and documents hosted on Cloud Storage. In a recent incident, one of your team members was able to modify files and you did not receive any notifications, causing other production jobs to fail. You must ensure that you receive notifications for all changes to files and documents in Cloud Storage while minimizing management overhead. What should you do?",
    "options": {
      "A": "View Cloud Audit logs for all Cloud Storage files in Logs Explorer. Filter by Admin Activity logs.",
      "B": "Enable Cloud Storage object versioning on your bucket. Configure Pub/Sub notifications for your Cloud Storage buckets.",
      "C": "Enable versioning on the Cloud Storage bucket. Set up a custom script that scans versions of Cloud Storage objects being modified and alert the admin by using the script.",
      "D": "Configure Object change notifications on the Cloud Storage buckets. Send the events to Pub/Sub."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: helloitsme123 1Â month ago\nSelected Answer: D\nhttps://docs.cloud.google.com/storage/docs/pubsub-notifications"
      },
      {
        "index": 2,
        "text": "Anonymous: guaose 2Â months, 1Â week ago\nSelected Answer: D\nObject change notifications allow you to receive real-time alerts when objects in a Cloud Storage bucket are created, updated, or deleted.\nBy sending these events to Pub/Sub, you can easily integrate with alerting systems, logging platforms, or custom workflows.\nThis approach is automated, scalable, and minimizes manual overhead, aligning with Google Cloud's best practices for monitoring and compliance."
      },
      {
        "index": 3,
        "text": "Anonymous: PythonPL 8Â months ago\nSelected Answer: B\nOption B is recommended by Google."
      },
      {
        "index": 4,
        "text": "Anonymous: 85c887f 9Â months, 3Â weeks ago\nSelected Answer: D\nOptions B and D both will work. Probably option D will be correct answer here in terms of just notifications without additional cost on versioning."
      },
      {
        "index": 5,
        "text": "Anonymous: Mika_Ro 10Â months, 3Â weeks ago\nSelected Answer: B\nObject change notification is an older method and deprecated in favor of Pub/Sub notifications"
      },
      {
        "index": 6,
        "text": "Anonymous: Ahamza 11Â months ago\nSelected Answer: B\nOption B follows Google-recommended best practices to ensure you receive notifications for all modifications to files and documents in Cloud Storage while minimizing management overhead:\nEnable Cloud Storage Object Versioning â†’ Ensures that previous versions of objects are retained, preventing data loss in case of accidental modifications.\nConfigure Pub/Sub Notifications for Cloud Storage â†’ Enables real-time alerts whenever a file is modified, deleted, or created.\nMinimizes manual effort â†’ Pub/Sub automates notifications without requiring manual log checking or custom scripts.\nThis setup ensures you are notified immediately when files are changed while also keeping old versions for compliance and recovery."
      }
    ]
  },
  {
    "id": 210,
    "source": "examtopics",
    "question": "Your company would like to store invoices and other financial documents in Google Cloud. You need to identify a Google-managed solution to store this information for your company. You must ensure that the documents are kept for a duration of three years. Your companyâ€™s analysts need frequent access to invoices from the past six months. After six months, invoices should be archived for audit purposes only. You want to minimize costs and follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Use Cloud Storage with Object Lifecycle Management to change the object storage class to Coldline after six months.",
      "B": "Use Cloud Storage with Object Lifecycle Management to change the object storage class to Standard after six months.",
      "C": "Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Coldline after six months.",
      "D": "Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Standard after six months."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: A\nCloud Storage is fully managed and the standard recommendation for document/object storage. Achival after 6 months then Coldline or Archive"
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: A\nCost optimization --> Cloud Storage\nArchiving after six month --> Coldline"
      }
    ]
  },
  {
    "id": 211,
    "source": "examtopics",
    "question": "You are planning to migrate your containerized workloads to Google Kubernetes Engine (GKE). You need to determine which GKE option to use. Your solution must have high availability, minimal downtime, and the ability to promptly apply security updates to your nodes. You also want to pay only for the compute resources that your workloads use without managing nodes. You want to follow Google-recommended practices and minimize operational costs. What should you do?",
    "options": {
      "A": "Configure a Standard regional GKE duster.",
      "B": "Configure a Standard zonal GKE duster.",
      "C": "Configure a Standard multi-zonal GKE cluster.",
      "D": "Configure an Autopilot GKE cluster."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gius3 4Â days, 8Â hours ago\nSelected Answer: D\nAnswer is D --> You also want to pay only for the compute resources that your workloads use without managing nodes"
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: D\nGoogle-recommended practices --> Autopilot"
      }
    ]
  },
  {
    "id": 212,
    "source": "examtopics",
    "question": "Your company stores data from multiple sources that have different data storage requirements. These data include:\n1. Customer data that is structured and read with complex queries\n2. Historical log data that is large in volume and accessed infrequently\n3. Real-time sensor data with high-velocity writes, which needs to be available for analysis but can tolerate some data loss\n\nYou need to design the most cost-effective storage solution that fulfills all data storage requirements. What should you do?",
    "options": {
      "A": "Use Firestore for customer data, Cloud Storage (Nearline) for historical logs, and Bigtable for sensor data.",
      "B": "Use Cloud SQL for customer data. Cloud Storage (Coldline) for historical logs, and BigQuery for sensor data.",
      "C": "Use Cloud SQL for customer data. Cloud Storage (Archive) for historical logs, and Bigtable for sensor data.",
      "D": "Use Spanner for all data."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: C\nBigtable âœ… for sensor data (high-velocity writes)."
      },
      {
        "index": 2,
        "text": "Anonymous: gummybearcik 1Â month, 3Â weeks ago\nSelected Answer: C\nCloud SQL for customer data. Cloud Storage (Archive) for historical logs, and Bigtable for sensor data.\n- Customer data: Complex queries and relational structure are best served by Cloud SQL rather than Firestore; Spanner is overkill and costlier unless you need global consistency and horizontal scale.\n- Historical logs: â€œAccessed infrequentlyâ€ points to Archive as the most costâ€‘effective storage. If you anticipate more frequent reads (e.g., monthly), Coldline could be chosen instead, but Archive minimizes cost when reads are rare.\n- Sensor data: Bigtable excels at highâ€‘throughput, timeâ€‘series ingestion and can handle eventual analyses with tolerance for some data loss; BigQuery is not ideal for sustained, ultraâ€‘high write rates."
      },
      {
        "index": 3,
        "text": "Anonymous: 763609c 1Â month, 3Â weeks ago\nSelected Answer: A\nif I consider it infrequent more than once a year"
      },
      {
        "index": 4,
        "text": "Anonymous: AdelElagawany 3Â months, 1Â week ago\nSelected Answer: C\n- Structured Transactional data (Cloud SQL)\n- Logs + Infrequent access (Cloud Storage Archive Class)\n- Sensor Data (Big Table)"
      },
      {
        "index": 5,
        "text": "Anonymous: NJBC 3Â months, 2Â weeks ago\nSelected Answer: C\nStructured - CloudSQL , Firestore is no-SQL (So not A)\nLarge Volume and Infrequent - Coldline/Archive - It doesn't say how infrequent, but safe bet for 'historical logs' is Coldline at least or Archive (So not A)\nAlso: Coldline Storage\nUse Case: Suitable for data that is infrequently accessed, like historical records, compliance archives, and disaster recovery data\nReal-time and write high velocity - BigTable is for high-performance, scalable db for real-time operations applications and massive amounts of data that require fast reads and writes. BigQuery's strength is NOT real-time transactional processing or frequent individual record updates/deletions. (So not B)\nC is the only viable answer left. (D doesn't make sense from the needs of the question)."
      },
      {
        "index": 6,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: A\nStructured and complex queries --> Firestore can handle it\nLarge volume and infrequent --> Cloud storage Nearline\nReal-time and write high velocity --> Bigtable"
      }
    ]
  },
  {
    "id": 213,
    "source": "examtopics",
    "question": "You work for a financial services company that operates as a stock market broker. Your company is planning to migrate to Google Cloud. You need to plan the network design in Google Cloud. Your design must:\nâ€¢ Minimize the latency between all production systems.\nâ€¢ Minimize costs related to your development environment.\n\nWhat should you do?",
    "options": {
      "A": "Create a VPC in the Standard Tier and one in the Premium Tier. Deploy production workloads in the Standard Tier and development workloads in the Premium Tier.",
      "B": "Create a VPC in the Standard Tier and one in the Premium Tier. Deploy development workloads in the Standard Tier and production workloads in the Premium Tier.",
      "C": "Create a VPC in the Premium Tier, and deploy both production and development workloads on this VPC.",
      "D": "Create a VPC in the Standard Tier, and deploy both production and development workloads on this VPC."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks, 1Â day ago\nSelected Answer: B\nPremium Tier uses Googleâ€™s global, high-performance backbone, which is the typical choice for latency-sensitive financial workloads.\nStandard Tier is cheaper for internet egress and is usually acceptable for dev/test environments where ultra-low latency isnâ€™t critical."
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nItsss B"
      }
    ]
  },
  {
    "id": 214,
    "source": "examtopics",
    "question": "You are managing a fleet of Compute Engine Linux instances in a Google Cloud project. Your company's engineering team requires SSH access to all instances to perform routine maintenance tasks. You need to manage the SSH access for the engineering team, and you want to minimize operational overhead when engineers join or leave the team. What should you do?",
    "options": {
      "A": "Create a single SSH key pair to be shared by all engineering team members. Add the public SSH key to project metadata.",
      "B": "Create an SSH key pair for each engineer on the team, and add the public SSH key to the metadata of the relevant instances.",
      "C": "Create a Google Group for all engineering team members, and grant them the Compute Viewer IAM role. Manage group membership when engineers join or leave the team.",
      "D": "Create a Google Group for all engineering team members, and set up OS Login for this group on the project. Manage group membership when engineers join or leave the team."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AdelElagawany 3Â months, 1Â week ago\nSelected Answer: D\nGoogle Cloud Identity Group + OsLogin enabled"
      }
    ]
  },
  {
    "id": 215,
    "source": "examtopics",
    "question": "Your company was recently impacted by a service disruption that caused multiple Dataflow jobs to get stuck, resulting in significant downtime in downstream applications and revenue loss. You were able to resolve the issue by identifying and fixing an error you found in the code. You need to design a solution with minimal management effort to identify when jobs are stuck in the future to ensure that this issue does not occur again. What should you do?",
    "options": {
      "A": "Update the Dataflow job configurations to send messages to a Pub/Sub topic when there are delays. Configure a backup Dataflow job to process jobs that are delayed. Use Cloud Tasks to trigger an alert when messages are pushed to the Pub/Sub topic.",
      "B": "Set up Cloud Monitoring alerts on the data freshness metric for the Dataflow jobs to receive a notification when a certain threshold is reached.",
      "C": "Set up Error Reporting to identify stack traces that indicate slowdowns in Dataflow jobs. Set up alerts based on these log entries.",
      "D": "Use the Personalized Service Health dashboard to identify issues with Dataflow jobs across regions."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: B\nData freshness metric: This is a built-in Dataflow metric that measures how current/recent the data being processed is. If a job gets stuck, data freshness increases (data becomes stale)\nCloud Monitoring alerts: Native integration with Dataflow metrics - no custom code required"
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nData freshness is a key indicator of whether a job is processing data in a timely manner or getting stuck"
      }
    ]
  },
  {
    "id": 216,
    "source": "examtopics",
    "question": "Your company is modernizing its applications and refactoring them to containerized microservices. You need to deploy the infrastructure on Google Cloud so that teams can deploy their applications. The applications cannot be exposed publicly. You want to minimize management and operational overhead. What should you do?",
    "options": {
      "A": "Provision a Google Kubernetes Engine (GKE) Autopilot cluster.",
      "B": "Provision a fleet of Compute Engine instances and install Kubernetes.",
      "C": "Provision a Standard regional Google Kubernetes Engine (GKE) cluster.",
      "D": "Provision a Standard zonal Google Kubernetes Engine (GKE) cluster."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AdelElagawany 3Â months, 1Â week ago\nSelected Answer: A\nIMHO A is correct:\nMinimize management and operational overhead => Autopilot Cluster"
      }
    ]
  },
  {
    "id": 217,
    "source": "examtopics",
    "question": "You have an application running inside a Compute Engine instance. You want to provide the application with secure access to a BigQuery dataset. You must ensure that credentials are only valid for a short period of time, and your application will only have access to the intended BigQuery dataset. You want to follow Google-recommended practices and minimize your operational costs. What should you do?",
    "options": {
      "A": "Attach a new service account to the instance every hour, and grant the service account the BigQuery Data Viewer IAM role on the project.",
      "B": "Attach a custom service account to the instance, and grant the service account the BigQuery Data Viewer IAM role on the dataset.",
      "C": "Attach a new service account to the instance every hour, and grant the service account the BigQuery Data Viewer IAM role on the dataset.",
      "D": "Attach a custom service account to the instance, and grant the service account the BigQuery Data Viewer IAM role on the project."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: b02d5eb 5Â months, 3Â weeks ago\nSelected Answer: B\nCorrect answear is B.\nA&D - BigQuery Data Viewer IAM role on the project (user will have access to all datasets)\nC - Constantly rotating service accounts is impractical, introduces risk, and doesn't actually reduce credential lifetime"
      },
      {
        "index": 2,
        "text": "Anonymous: ahannora 6Â months ago\nSelected Answer: B\nB: Least privilege, secure access, and minimal effort."
      },
      {
        "index": 3,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nTemp credentials --> Service account\nLeast privilege --> on dataset"
      }
    ]
  },
  {
    "id": 218,
    "source": "examtopics",
    "question": "You have an application that is currently processing transactions by using a group of managed VM instances. You need to migrate the application so that it is serverless and scalable. You want to implement an asynchronous transaction processing system, while minimizing management overhead. What should you do?",
    "options": {
      "A": "Install Kafka on VM instances to acknowledge incoming transactions. Use Cloud Run to process transactions.",
      "B": "Use Pub/Sub to acknowledge incoming transactions. Use VM instances to process transactions.",
      "C": "Use Pub/Sub to acknowledge incoming transactions. Use Cloud Run to process transactions.",
      "D": "Install Kafka on VM instances to acknowledge incoming transactions. Use VM instances to process transactions."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: BRDA 1Â month, 1Â week ago\nSelected Answer: C\nserverless -> cloudrun"
      },
      {
        "index": 2,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: C\ncloudrun is serverless."
      }
    ]
  },
  {
    "id": 219,
    "source": "examtopics",
    "question": "Your company has many legacy third-party applications that rely on a shared NFS server for file sharing between these workloads. You want to modernize the NFS server by using a Google Cloud managed service. You need to select the solution that requires the least amount of change to the application. What should you do?",
    "options": {
      "A": "Create a Compute Engine instance and configure an NFS server on the instance. Point all NFS mounts to the Compute Engine instance.",
      "B": "Deploy a Filestore instance. Replace all NFS mounts with a Filestore mount.",
      "C": "Configure Firestore. Configure all applications to use Firestore instead of the NFS server.",
      "D": "Create a Cloud Storage bucket. Configure all applications to use Cloud Storage client libraries instead of the NFS server."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 3Â weeks ago\nSelected Answer: B\nFilestore is the shared file server provided by google."
      }
    ]
  },
  {
    "id": 220,
    "source": "examtopics",
    "question": "Your company is seeking a scalable solution to retain and explore application logs hosted on Compute Engine. You must be able to analyze your logs with SQL queries, and you want to be able to create charts to identify patterns and trends in your logs over time. You want to follow Google-recommended practices and minimize your operational costs. What should you do?",
    "options": {
      "A": "Use a custom script to push your application logs to BigQuery for exploration.",
      "B": "Ingest your application logs to Cloud Logging by using Ops Agent, and explore your logs in Logs Explorer.",
      "C": "Ingest your application logs to Cloud Logging by using Ops Agent, and explore your logs with Log Analytics.",
      "D": "Use a custom script to push your application logs to Cloud SQL for exploration."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: C\nSQL Queries --> Log Analytics"
      }
    ]
  },
  {
    "id": 221,
    "source": "examtopics",
    "question": "You want to send and consume Cloud Pub/Sub messages from your App Engine application. The Cloud Pub/Sub API is currently disabled. You will use a service account to authenticate your application to the API. You want to make sure your application can use Cloud Pub/Sub. What should you do?",
    "options": {
      "A": "Enable the Cloud Pub/Sub API in the API Library on the GCP Console.",
      "B": "Rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.",
      "C": "Use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.",
      "D": "Grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/ Sub."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 4Â months ago\nCorrect Answer is (A)\nQuickstart: using the Google Cloud Console\nThis page shows you how to perform basic tasks in Pub/Sub using the Google Cloud Console.\nNote: If you are new to Pub/Sub, we recommend that you start with the interactive tutorial.\nBefore you begin\nSet up a Cloud Console project.\nSet up a project\nClick to:\nCreate or select a project.\nEnable the Pub/Sub API for that project.\nYou can view and manage these resources at any time in the Cloud Console.\nInstall and initialize the Cloud SDK.\nNote: You can run the gcloud tool in the Cloud Console without installing the Cloud SDK. To run the gcloud tool in the Cloud Console, use Cloud Shell .\nhttps://cloud.google.com/pubsub/docs/quickstart-console"
      },
      {
        "index": 2,
        "text": "Anonymous: Bharathy Highly Voted 4Â years, 8Â months ago\nWe need to enable the pub/sub API, if we are going to use it in your project... then APP engine can able to access it with required ServiceAccount"
      },
      {
        "index": 3,
        "text": "Anonymous: Cloudmoh Most Recent 11Â months, 1Â week ago\nSelected Answer: A\nYes, the Cloud pub/sub-API should be enabled."
      },
      {
        "index": 4,
        "text": "Anonymous: Romio2023 1Â year, 1Â month ago\nhello test"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nYes A, is more correct as first you need to enable the API itself"
      },
      {
        "index": 6,
        "text": "Anonymous: bidyut123 1Â year, 7Â months ago\nSelected Answer: A\nANSWER SHOULD BE A."
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: A\nAnswer A is correct. Enable the Cloud Pub/Sub API in the API Library on the GCP Console.\nSince the Cloud Pub/Sub API is currently disabled, the first step is to enable it. This can be done through the API Library on the GCP Console. Once the API is enabled, the service account can be used to authenticate the App Engine application to the Cloud Pub/Sub API.\nAnswer B is incorrect because there is no automatic enablement of APIs when a service account accesses them. The API needs to be enabled manually in the API Library or through the command-line interface.\nAnswer C is incorrect because enabling APIs through Deployment Manager requires that the APIs be enabled in the project before Deployment Manager can use them.\nAnswer D is incorrect because granting the App Engine Default service account the Cloud Pub/Sub Admin role could be a security risk, and it is not necessary to enable the API."
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nA is right"
      },
      {
        "index": 9,
        "text": "Anonymous: sedado77 2Â years, 7Â months ago\nSelected Answer: A\nYup its A"
      },
      {
        "index": 10,
        "text": "Anonymous: haroldbenites 2Â years, 7Â months ago\ngo for A"
      }
    ]
  },
  {
    "id": 222,
    "source": "examtopics",
    "question": "You need to monitor resources that are distributed over different projects in Google Cloud Platform. You want to consolidate reporting under the same Stackdriver\nMonitoring dashboard. What should you do?",
    "options": {
      "A": "Use Shared VPC to connect all projects, and link Stackdriver to one of the projects.",
      "B": "For each project, create a Stackdriver account. In each project, create a service account for that project and grant it the role of Stackdriver Account Editor in all other projects.",
      "C": "Configure a single Stackdriver account, and link all projects to the same account.",
      "D": "Configure a single Stackdriver account for one of the projects. In Stackdriver, create a Group and add the other project names as criteria for that Group."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sahedge Highly Voted 5Â years ago\nFirst of all D is incorrect, Groups are used to define alerts on set of resources(such as VM instances, databases, and load balancers). FYI tried adding Two projects into a group it did not allowed me as the \"AND\"/\"OR\" criteria for the group failed with this combination of resources.\nC is correct because,\nWhen you intially click on Monitoring(Stackdriver Monitoring) it creates a workspac(a stackdriver account) linked to the ACTIVE(CURRENT) Project from which it was clicked.\nNow if you change the project and again click onto Monitoring it would create an another workspace(a stackdriver account) linked to the changed ACTIVE(CURRENT) Project, we don't want this as this would not consolidate our result into a single dashboard(workspace/stackdriver account).\nIf you have accidently created two diff workspaces merge them under Monitoring > Settings > Merge Workspaces > MERGE.\nIf we have only one workspace and two projects we can simply add other GCP Project under\nMonitoring > Settings > GCP Projects > Add GCP Projects.\nIn both of these cases we did not create a GROUP, we just linked GCP Project to the workspace(stackdriver account)."
      },
      {
        "index": 2,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\nC is correct not D"
      },
      {
        "index": 3,
        "text": "Anonymous: KC_go_reply Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nThe correct answer is **C. Configure a single Stackdriver account, and link all projects to the same account.**\nStackdriver Monitoring (now Google Cloud Monitoring) doesn't work on a per-project account basis. There's a single Monitoring service that spans your entire Google Cloud organization. All projects within the organization can automatically report metrics to this single Monitoring instance. You don't need to create separate accounts or use complex workarounds like Shared VPC or inter-project service accounts. Option C directly addresses this by leveraging the inherent design of Cloud Monitoring.\nOptions A, B, and D are incorrect because they introduce unnecessary complexity and don't utilize the built-in functionality of Google Cloud Monitoring for consolidated reporting across multiple projects. They might even lead to permission issues and difficulties in maintaining a unified view of your resources."
      },
      {
        "index": 4,
        "text": "Anonymous: RJ78 1Â year, 4Â months ago\nSelected Answer: D\nThis method provides a centralized and flexible way to monitor resources across different projects, without requiring complex network configurations or additional accounts."
      },
      {
        "index": 5,
        "text": "Anonymous: edoo 1Â year, 11Â months ago\nSelected Answer: C\nD is incorrect because a stackdriver group can group resources but not projects. Creating a single Stackdriver account in one project and creating a Group with other project names as criteria does not automatically aggregate and monitor metrics from different projects in a single dashboard. While creating a Group can help organize and manage metrics, it does not provide a solution for linking and monitoring metrics from different projects. You would still need to separately configure and link each project to the Stackdriver account to monitor their respective metrics."
      },
      {
        "index": 6,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: C\nC is more correct, just link them into one"
      },
      {
        "index": 7,
        "text": "Anonymous: Yad_datatonic 2Â years, 4Â months ago\nSelected Answer: C\nOption C is the recommended approach because it allows you to configure a single Stackdriver account and link all your projects to this account. This way, you can centralise monitoring, create custom dashboards, set up alerts, and gain a unified view of your resources distributed across different projects in a more straightforward and consolidated manner. It provides a single point of access and management for monitoring across all projects, which is typically the desired outcome for multi-project environments."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC is more correct, just link them into one"
      },
      {
        "index": 9,
        "text": "Anonymous: SanjeevKumar1983 2Â years, 4Â months ago\nSelected Answer: D\nhttps://cloud.google.com/monitoring/settings\nBest practices for scoping projects\nWe recommend that you use a new Google Cloud project or one without resources as the scoping project when you want to view metrics for multiple Google Cloud projects or AWS accounts.\nWhen a metrics scope contains monitored projects, to chart or monitor only those metrics stored in the scoping project, you must specify filters that exclude metrics from the monitored projects. The requirement to use filters increases the complexity of chart and alerting policy, and it increases the possibility of a configuration error. The recommendation ensures that these scoping projects don't generate metrics, so there are no metrics in the projects to chart or monitor."
      },
      {
        "index": 10,
        "text": "Anonymous: shreykul 2Â years, 6Â months ago\nSelected Answer: C\nC is correct"
      }
    ]
  },
  {
    "id": 223,
    "source": "examtopics",
    "question": "You are deploying an application to a Compute Engine VM in a managed instance group. The application must be running at all times, but only a single instance of the VM should run per GCP project. How should you configure the instance group?",
    "options": {
      "A": "Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1.",
      "B": "Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 1.",
      "C": "Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 2.",
      "D": "Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 2."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sahedge Highly Voted 5Â years ago First of all D is incorrect, Groups are used to define alerts on set of resources(such as VM instances, databases, and load balancers). FYI tried adding Two projects into a group it did not allowed me as the \"AND\"/\"OR\" criteria for the group failed with this combination of resources.\nC is correct because,\nWhen you intially click on Monitoring(Stackdriver Monitoring) it creates a workspac(a stackdriver account) linked to the ACTIVE(CURRENT) Project from which it was clicked. Now if you change the project and again click onto Monitoring it would create an another workspace(a stackdriver account) linked to the changed ACTIVE(CURRENT) Project, we don't want this as this would not consolidate our result into a single dashboard(workspace/stackdriver account). If you have accidently created two diff workspaces merge them under Monitoring > Settings > Merge Workspaces > MERGE.\nIf we have only one workspace and two projects we can simply add other GCP Project under Monitoring > Settings > GCP Projects > Add GCP Projects.\nIn both of these cases we did not create a GROUP, we just linked GCP Project to the workspace(stackdriver account). ..."
      },
      {
        "index": 2,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago C is correct not D ..."
      },
      {
        "index": 3,
        "text": "Anonymous: KC_go_reply Most Recent 1Â year, 3Â months ago Selected Answer: C The correct answer is **C. Configure a single Stackdriver account, and link all projects to the same account.**\nStackdriver Monitoring (now Google Cloud Monitoring) doesn't work on a per-project account basis. There's a single Monitoring service that spans your entire Google Cloud organization. All projects within the organization can automatically report metrics to this single Monitoring instance. You don't need to create separate accounts or use complex workarounds like Shared VPC or inter-project service accounts. Option C directly addresses this by leveraging the inherent design of Cloud Monitoring.\nOptions A, B, and D are incorrect because they introduce unnecessary complexity and don't utilize the built-in functionality of Google Cloud Monitoring for consolidated reporting across multiple projects. They might even lead to permission issues and difficulties in maintaining a unified view of your resources. ..."
      },
      {
        "index": 4,
        "text": "Anonymous: RJ78 1Â year, 4Â months ago Selected Answer: D This method provides a centralized and flexible way to monitor resources across different projects, without requiring complex network configurations or additional accounts. ..."
      },
      {
        "index": 5,
        "text": "Anonymous: edoo 1Â year, 11Â months ago Selected Answer: C D is incorrect because a stackdriver group can group resources but not projects. Creating a single Stackdriver account in one project and creating a Group with other project names as criteria does not automatically aggregate and monitor metrics from different projects in a single dashboard. While creating a Group can help organize and manage metrics, it does not provide a solution for linking and monitoring metrics from different projects. You would still need to separately configure and link each project to the Stackdriver account to monitor their respective metrics. ..."
      },
      {
        "index": 6,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago Selected Answer: C C is more correct, just link them into one ..."
      },
      {
        "index": 7,
        "text": "Anonymous: Yad_datatonic 2Â years, 4Â months ago Selected Answer: C Option C is the recommended approach because it allows you to configure a single Stackdriver account and link all your projects to this account. This way, you can centralise monitoring, create custom dashboards, set up alerts, and gain a unified view of your resources distributed across different projects in a more straightforward and consolidated manner. It provides a single point of access and management for monitoring across all projects, which is typically the desired outcome for multi-project environments. ..."
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago Selected Answer: C C is more correct, just link them into one ..."
      },
      {
        "index": 9,
        "text": "Anonymous: SanjeevKumar1983 2Â years, 4Â months ago Selected Answer: D https://cloud.google.com/monitoring/settings\nBest practices for scoping projects\nWe recommend that you use a new Google Cloud project or one without resources as the scoping project when you want to view metrics for multiple Google Cloud projects or AWS accounts.\nWhen a metrics scope contains monitored projects, to chart or monitor only those metrics stored in the scoping project, you must specify filters that exclude metrics from the monitored projects. The requirement to use filters increases the complexity of chart and alerting policy, and it increases the possibility of a configuration error. The recommendation ensures that these scoping projects don't generate metrics, so there are no metrics in the projects to chart or monitor. ..."
      },
      {
        "index": 10,
        "text": "Anonymous: shreykul 2Â years, 6Â months ago Selected Answer: C C is correct ..."
      }
    ]
  },
  {
    "id": 224,
    "source": "examtopics",
    "question": "You want to verify the IAM users and roles assigned within a GCP project named my-project. What should you do?",
    "options": {
      "A": "Run gcloud iam roles list. Review the output section.",
      "B": "Run gcloud iam service-accounts list. Review the output section.",
      "C": "Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles.",
      "D": "Navigate to the project and then to the Roles section in the GCP Console. Review the roles and status."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 4Â months ago\nCorrect answer is C as IAM section provides the list of both Members and Roles.Option A is wrong as it would provide information about the roles only.Option B is wrong as it would provide only the service accounts.Option D is wrong as it would provide information about the roles only."
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 3Â months ago\nC is the correct answer"
      },
      {
        "index": 3,
        "text": "Anonymous: Cloudmoh Most Recent 11Â months, 1Â week ago\nSelected Answer: C\nOption C contains steps on to check for Members and Roles"
      },
      {
        "index": 4,
        "text": "Anonymous: Dinya_jui 1Â year, 6Â months ago\nC is the correct answer as it will details regarding the users as well as the Roles"
      },
      {
        "index": 5,
        "text": "Anonymous: JB28 1Â year, 7Â months ago\nOption c"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nc seems more legit"
      },
      {
        "index": 7,
        "text": "Anonymous: Shweta2jun 2Â years, 2Â months ago\nSelected Answer: C\nC is correct answer"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: C\nAnswer C is the correct answer.\nTo verify the IAM users and roles assigned within a GCP project, you can navigate to the project and then to the IAM section in the GCP Console. In the IAM section, you can review the members and roles assigned to the project. This will allow you to see who has what level of access to the project resources.\nAnswer A is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles.\nAnswer B is incorrect because it lists the service accounts in the project, but it does not show the IAM users and roles assigned to those service accounts.\nAnswer D is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles."
      },
      {
        "index": 9,
        "text": "Anonymous: leogor 2Â years, 9Â months ago\nC. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles."
      },
      {
        "index": 10,
        "text": "Anonymous: SaiSaiA 3Â years ago\nSelected Answer: C\nC is the only logical answers. If you go IAM & Admin > IAM: You can see Principals and Roles. Users, groups, service accounts are Principals"
      }
    ]
  },
  {
    "id": 225,
    "source": "examtopics",
    "question": "You need to create a new billing account and then link it with an existing Google Cloud Platform project. What should you do?",
    "options": {
      "A": "Verify that you are Project Billing Manager for the GCP project. Update the existing project to link it to the existing billing account.",
      "B": "Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project.",
      "C": "Verify that you are Billing Administrator for the billing account. Create a new project and link the new project to the existing billing account.",
      "D": "Verify that you are Billing Administrator for the billing account. Update the existing project to link it to the existing billing account."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 4Â months ago Correct answer is C as IAM section provides the list of both Members and Roles.Option A is wrong as it would provide information about the roles only.Option B is wrong as it would provide only the service accounts.Option D is wrong as it would provide information about the roles only. ..."
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 3Â months ago C is the correct answer ..."
      },
      {
        "index": 3,
        "text": "Anonymous: Cloudmoh Most Recent 11Â months, 1Â week ago Selected Answer: C Option C contains steps on to check for Members and Roles ..."
      },
      {
        "index": 4,
        "text": "Anonymous: Dinya_jui 1Â year, 6Â months ago C is the correct answer as it will details regarding the users as well as the Roles ..."
      },
      {
        "index": 5,
        "text": "Anonymous: JB28 1Â year, 7Â months ago Option c ..."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago Selected Answer: C c seems more legit ..."
      },
      {
        "index": 7,
        "text": "Anonymous: Shweta2jun 2Â years, 2Â months ago Selected Answer: C C is correct answer ..."
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago Selected Answer: C Answer C is the correct answer.\nTo verify the IAM users and roles assigned within a GCP project, you can navigate to the project and then to the IAM section in the GCP Console. In the IAM section, you can review the members and roles assigned to the project. This will allow you to see who has what level of access to the project resources.\nAnswer A is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles.\nAnswer B is incorrect because it lists the service accounts in the project, but it does not show the IAM users and roles assigned to those service accounts.\nAnswer D is incorrect because it lists the roles available in the project, but it does not show the IAM users and roles assigned to those roles. ..."
      },
      {
        "index": 9,
        "text": "Anonymous: leogor 2Â years, 9Â months ago C. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles. ..."
      },
      {
        "index": 10,
        "text": "Anonymous: SaiSaiA 3Â years ago Selected Answer: C C is the only logical answers. If you go IAM & Admin > IAM: You can see Principals and Roles. Users, groups, service accounts are Principals ..."
      }
    ]
  },
  {
    "id": 226,
    "source": "examtopics",
    "question": "You have one project called proj-sa where you manage all your service accounts. You want to be able to use a service account from this project to take snapshots of VMs running in another project called proj-vm. What should you do?",
    "options": {
      "A": "Download the private key from the service account, and add it to each VMs custom metadata.",
      "B": "Download the private key from the service account, and add the private key to each VM's SSH keys.",
      "C": "Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.",
      "D": "When creating the VMs, set the service account's API scope for Compute Engine to read/write."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: jackdbd Highly Voted 3Â years, 6Â months ago\nC is the correct answer.\nIt took me a while to figure it out because I didn't understand how service accounts work across project. This article made it clear for me. https://gtseres.medium.com/using-service-accounts-across-projects-in-gcp-cf9473fef8f0\nYou create the service account in proj-sa and take note of the service account email, then you go to proj-vm in IAM > ADD and add the service account's email as new member and give it the Compute Storage Admin role. JelloMan 2Â years, 9Â months ago\nAs of now, service accounts may be impersonated (new-term). AKA, you can create a service account in one project and then impersonate it in others. Essentially, it involves the same steps as what the medium article suggests (create a service account in the principal (main) project and then add the email of the main project to the project you want to impersonate) https://cloud.google.com/iam/docs/impersonating-service-accounts#impersonate-sa-level SaiSaiA 2Â years, 6Â months ago\nI have tried C, it doesn't work. Also, this refers to a different Principal (user) impersonating a Service Account which is a different case from what is in the question."
      },
      {
        "index": 2,
        "text": "Anonymous: kishoredeena Highly Voted 4Â years, 7Â months ago\nOption C is the right one"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nC seems more correct, because you want to use it, you need access for it"
      },
      {
        "index": 4,
        "text": "Anonymous: sthapit 1Â year, 5Â months ago\nC is the answer"
      },
      {
        "index": 5,
        "text": "Anonymous: findsidd 1Â year, 5Â months ago\nC is the correct answer.\nCompute Storage Admin (roles/compute.storageAdmin) has permissions to create, modify, and delete disks, images, and snapshots.\nFor example, if your company has someone who manages project images and you don't want them to have the editor role on the project, then grant this role to their account on the project.\nThe most common way to let an application authenticate as a service account is to attach a service account to the resource running the application. For example, you can attach a service account to a Compute Engine instance so that applications running on that instance can authenticate as the service account. Then, you can grant the service account IAM roles to let the service accountâ€”and, by extension, applications on the instanceâ€”access Google Cloud resources."
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: C\nAnswer C is correct. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.\nTo take snapshots of VMs running in another project, you need to grant the service account that will take the snapshots the necessary IAM role to perform the action. In this case, granting the service account in the proj-sa project the Compute Storage Admin role in the proj-vm project will allow it to take snapshots of VMs running in that project.\nAnswers A and B are incorrect because they involve downloading and adding the private key of the service account to each VM, which is not necessary and potentially risky.\nAnswer D is also incorrect because setting the service account's API scope for Compute Engine to read/write only grants it permission to perform actions on resources within the same project.\nhttps://cloud.google.com/iam/docs/creating-managing-service-accounts\nhttps://cloud.google.com/iam/docs/granting-roles-to-service-accounts"
      },
      {
        "index": 7,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nC. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm."
      },
      {
        "index": 8,
        "text": "Anonymous: habros 2Â years, 5Â months ago\nSafe to eliminate any options that demand transferring of private keys. NOT SAFE\nHence, C. theBestStudent 2Â years, 3Â months ago\nhighly agree with this thoughts! transferring private keys is a big no no here."
      },
      {
        "index": 9,
        "text": "Anonymous: RanjithK 2Â years, 6Â months ago\nAnswer is C"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nC. is the correct answer\nCompute Storage Admin\n(roles/compute.storageAdmin)\nPermissions to create, modify, and delete disks, images, and snapshots.\nFor example, if your company has someone who manages project images and you don't want them to have the editor role on the project, then grant this role to their account on the project.\nLowest-level resources where you can grant this role:\nDisk\nImage\nSnapshot Beta"
      }
    ]
  },
  {
    "id": 227,
    "source": "examtopics",
    "question": "You created a Google Cloud Platform project with an App Engine application inside the project. You initially configured the application to be served from the us- central region. Now you want the application to be served from the asia-northeast1 region. What should you do?",
    "options": {
      "A": "Change the default region property setting in the existing GCP project to asia-northeast1.",
      "B": "Change the region property setting in the existing App Engine application from us-central to asia-northeast1.",
      "C": "Create a second App Engine application in the existing GCP project and specify asia-northeast1 as the region to serve your application.",
      "D": "Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Bharathy Highly Voted 5Â years, 10Â months ago\nOption D is correct, as there can be only one App Engine application inside a project . C is incorrect, as GCP can't have two app engine applications.. jcloud965 4Â years, 6Â months ago\nYes, and you can't change an App Engine application region once created"
      },
      {
        "index": 2,
        "text": "Anonymous: saurabh1805 Highly Voted 5Â years, 7Â months ago\nEach Cloud project can contain only a single App Engine application, and once created you cannot change the location of your App Engine application.\nhttps://cloud.google.com/appengine/docs/flexible/nodejs/managing-projects-apps-billing#create"
      },
      {
        "index": 3,
        "text": "Anonymous: devjay24 Most Recent 9Â months, 3Â weeks ago\nSelected Answer: C\na project can have more than one app engine - but the only condition is there should be one app engine per region. So option C should make sense"
      },
      {
        "index": 4,
        "text": "Anonymous: garg.vnay 1Â year, 5Â months ago\nSelected Answer: D\nWhy showing correct answer as C????"
      },
      {
        "index": 5,
        "text": "Anonymous: AmirJZsecENG 1Â year, 10Â months ago\nSelected Answer: D\nwhy the correct answer is D? AmirJZsecENG 1Â year, 10Â months ago\nand why here is mentioning that C is correct?"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD seems more correct , as a project can only have a single app engine application"
      },
      {
        "index": 7,
        "text": "Anonymous: bobthebuilder_karkedikhayenge 2Â years, 4Â months ago\nSelected Answer: D\nEach Google Cloud project can contain only a single App Engine application, and once created you cannot change the location of your App Engine application"
      },
      {
        "index": 8,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nD as you cannot have more than one APP engine"
      },
      {
        "index": 9,
        "text": "Anonymous: Neha_Pallavi 2Â years, 5Â months ago\nOption D is correct.\nThere can be only one App Engine application inside a project\nBesides, you cannot change an app's region after you set it.\nhttps://cloud.google.com/appengine/docs/standard/locations"
      },
      {
        "index": 10,
        "text": "Anonymous: findsidd 2Â years, 5Â months ago\nOption D is correct.\nThere can be only one App Engine application inside a project\nBesides, you cannot change an app's region after you set it.\nhttps://cloud.google.com/appengine/docs/standard/locations"
      }
    ]
  },
  {
    "id": 228,
    "source": "examtopics",
    "question": "You need to grant access for three users so that they can view and edit table data on a Cloud Spanner instance. What should you do?",
    "options": {
      "A": "Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to the role.",
      "B": "Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role.",
      "C": "Run gcloud iam roles describe roles/spanner.viewer - -project my-project. Add the users to the role.",
      "D": "Run gcloud iam roles describe roles/spanner.viewer - -project my-project. Add the users to a new group. Add the group to the role."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: yasu Highly Voted 4Â years, 10Â months ago\nI think it should be B, setup a group first are suggested way from Google."
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 4Â years, 9Â months ago\nB is the correct option"
      },
      {
        "index": 3,
        "text": "Anonymous: warbon Most Recent 12Â months ago\nSelected Answer: A\nAdding users to a group (Options B and D) is unnecessary unless you specifically want to manage access via a group."
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 1Â year, 1Â month ago\nSelected Answer: B\nB\nas per the best practice."
      },
      {
        "index": 5,
        "text": "Anonymous: gsmasad 1Â year, 2Â months ago\nSelected Answer: B\nB is correct Because adding a group instead to the user is a GCP best practice"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nb seems more legit as it will add in the group and they need edit access also"
      },
      {
        "index": 7,
        "text": "Anonymous: Fajmayor 1Â year, 4Â months ago\nSelected Answer: B\nSetup group and add role to it"
      },
      {
        "index": 8,
        "text": "Anonymous: sthapit 1Â year, 5Â months ago\nI go with B but TO have more controlled access, A is correct as well"
      },
      {
        "index": 9,
        "text": "Anonymous: findsidd 1Â year, 5Â months ago\nSelected Answer: B\nGoogle groups can help you manage users at scale. Each member of a Google group inherits the Identity and Access Management (IAM) roles granted to that group. This inheritance means that you can use a group's membership to manage users' roles instead of granting IAM roles to individual users.\nhttps://cloud.google.com/iam/docs/groups-in-cloud-console#:~:text=To%20add%20members%3A%20Click%20person,add%20them%20to%20the%20group."
      },
      {
        "index": 10,
        "text": "Anonymous: Ash_34 1Â year, 6Â months ago\nSelected Answer: B\nB is the correct option as spanner users are grouped into a single group and can be added to the IAM role. Easy for management work."
      }
    ]
  },
  {
    "id": 229,
    "source": "examtopics",
    "question": "You create a new Google Kubernetes Engine (GKE) cluster and want to make sure that it always runs a supported and stable version of Kubernetes. What should you do?",
    "options": {
      "A": "Enable the Node Auto-Repair feature for your GKE cluster.",
      "B": "Enable the Node Auto-Upgrades feature for your GKE cluster.",
      "C": "Select the latest available cluster version for your GKE cluster.",
      "D": "Select ×’â‚¬Container-Optimized OS (cos)×’â‚¬ as a node image for your GKE cluster."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Lush Highly Voted 4Â years, 8Â months ago\nThe answer is B\nhttps://cloud.google.com/kubernetes-engine/versioning-and-upgrades"
      },
      {
        "index": 2,
        "text": "Anonymous: 4bsolut Highly Voted 4Â years, 6Â months ago\n\"Creating or upgrading a cluster by specifying the version as <latest> does not provide automatic upgrades. Enable automatic node upgrades to ensure that the nodes in your cluster up to date with the latest stable version.\" --source: https://cloud.google.com/kubernetes-engine/versioning-and-upgrades\n-Correct answer: B"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nb is corrent , as auto updates provide the more stable version"
      },
      {
        "index": 4,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: B\nAnswer B is correct. Google Kubernetes Engine (GKE) supports multiple versions of Kubernetes, and new versions are regularly released. To ensure that your GKE cluster runs a supported and stable version of Kubernetes, it is recommended to enable the Node Auto-Upgrades feature. This feature automatically upgrades the Kubernetes version of each node in the cluster to the latest stable version.\nAnswer A, enabling the Node Auto-Repair feature, is focused on repairing or replacing nodes in case they become unresponsive, but it doesn't address the need for running a supported and stable version of Kubernetes.\nAnswer C, selecting the latest available cluster version, may not always be the best option as new versions may have bugs or issues that have not yet been identified.\nAnswer D, selecting Container-Optimized OS (cos) as a node image, is focused on using a lightweight and secure operating system optimized for running containers, but it doesn't address the need for running a supported and stable version of Kubernetes. sthapit 1Â year, 5Â months ago\nTrue. B is the correct choice"
      },
      {
        "index": 5,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nSelected Answer: B\nB. Enable the Node Auto-Upgrades"
      },
      {
        "index": 6,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nB is correct.\nCreating or upgrading a cluster by specifying the version as latest does not provide automatic upgrades. Enable node auto-upgrades to ensure that the nodes in your cluster are up-to-date with the latest stable version."
      },
      {
        "index": 7,
        "text": "Anonymous: pfabio 2Â years, 7Â months ago\nSelected Answer: B\nNode auto-upgrades help you keep the nodes in your cluster up-to-date with the cluster control plane version when your control plane is updated on your behalf. When you create a new cluster or node pool with Google Cloud console or the gcloud command, node auto-upgrade is enabled by default."
      },
      {
        "index": 8,
        "text": "Anonymous: Harbeeb 2Â years, 8Â months ago\nSelected Answer: B\nhttps://cloud.google.com/kubernetes-engine/versioning-and-upgrades"
      },
      {
        "index": 9,
        "text": "Anonymous: shawnkkk 3Â years, 2Â months ago\nB. Enable the Node Auto-Upgrades feature for your GKE cluster."
      },
      {
        "index": 10,
        "text": "Anonymous: vishnukumartr 3Â years, 2Â months ago\nB. Enable the Node Auto-Upgrades feature for your GKE cluster."
      }
    ]
  },
  {
    "id": 230,
    "source": "examtopics",
    "question": "You have an instance group that you want to load balance. You want the load balancer to terminate the client SSL session. The instance group is used to serve a public web application over HTTPS. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Configure an HTTP(S) load balancer.",
      "B": "Configure an internal TCP load balancer.",
      "C": "Configure an external SSL proxy load balancer.",
      "D": "Configure an external TCP proxy load balancer."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gini Highly Voted 5Â years, 9Â months ago\nAccording to the documentation of SSL Proxy Load Balacing on Google, \"SSL Proxy Load Balancing is intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use HTTP(S) Load Balancing.\" in my opinion A should be the most suitable choice. yvinisiupacuando 4Â years, 8Â months ago\nAgree with you but, A is not the most suitable choice, it is the only choice, as the other Load Balancers cannot route HTTP(S) traffic."
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 9Â months ago\nFor HTTP(s) Load balancer, the client SSL session terminates at the load balancer. A is the correct option."
      },
      {
        "index": 3,
        "text": "Anonymous: d6685b2 Most Recent 1Â year, 8Â months ago\nwhy A is correcte ? JackSkeletonCoder 1Â year, 4Â months ago\nIn opt B, Internal tcp only deals with ip and ports in a vpc i.e 'internally' and does not deal with http/s or ssl/tls\nIn opt C external SSL can serve application over https but not the ssl termination part.\nIn opt D external TCP only deals with web application over HTTP and not the secured one. To top it off, it can't even terminate SSL session.\nHence the only viable option is HTTP(S) load balancer."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\ngoogle recommend https for terminate the ssl session so A seems more legit"
      },
      {
        "index": 5,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nQuestion mentions HTTPS, SO A is the correct answer."
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: A\nAnswer A is correct. Google recommends using an HTTP(S) load balancer for terminating SSL sessions and load-balancing traffic to an instance group serving a public web application over HTTPS.\nAnswer B is incorrect because it is an internal load balancer, which is not suitable for serving public web applications. Internal load balancers are used for private/internal applications.\nAnswer C is incorrect because SSL proxy load balancers do not terminate the SSL session, instead they pass the SSL traffic directly to the backends without decrypting it. SSL proxy load balancers are used when you need to ensure that SSL is used end-to-end between the client and the backend, and when you want to offload SSL processing from the backends.\nAnswer D is incorrect because TCP proxy load balancers do not terminate SSL sessions. TCP proxy load balancers are used for non-HTTP traffic and can balance traffic at the TCP layer, but they do not have the ability to terminate SSL sessions. chikorita 2Â years, 9Â months ago\nyo never fail us, my lord!"
      },
      {
        "index": 7,
        "text": "Anonymous: leogor 3Â years, 2Â months ago\nSelected Answer: A\nHTTP(S) load balancer."
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3Â years, 7Â months ago\nI will go with A"
      },
      {
        "index": 9,
        "text": "Anonymous: Akash7 3Â years, 7Â months ago\nA is correct.\nAccording to this guide for setting up an HTTP (S) load balancer in GCP: The client SSL session terminates at the load balancer. Sessions between the load balancer and the instance can either be HTTPS (recommended) or HTTP."
      },
      {
        "index": 10,
        "text": "Anonymous: haroldbenites 3Â years, 7Â months ago\nGo for C.\nIt dont say Global balancer."
      }
    ]
  },
  {
    "id": 231,
    "source": "examtopics",
    "question": "You are deploying an application to Google Kubernetes Engine (GKE). The application needs to make API calls to a private Cloud Storage bucket. You need to configure your application Pods to authenticate to the Cloud Storage API, but your organization policy prevents the usage of service account keys. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Create the GKE cluster with Workload Identity Federation. Configure the default node service account to access the bucket. Deploy the application into the cluster so the application can use the node service account permissions. Use Identity and Access Management (IAM) to grant the service account access to the bucket.",
      "B": "Create the GKE cluster with Workload Identity Federation. Create a Google service account and a Kubernetes ServiceAccount, and configure both service accounts to use Workload Identity Federation. Attach the Kubernetes ServiceAccount to the application Pods and configure the Google service account to access the bucket with Identity and Access Management (IAM).",
      "C": "Create the GKE cluster and deploy the application. Request a security exception to create a Google service account key. Set the constraints/iam.serviceAccountKeyExpiryHours organization policy to 24 hours.",
      "D": "Create the GKE cluster and deploy the application. Request a security exception to create a Google service account key. Set the constraints/iam.serviceAccountKeyExpiryHours organization policy to 8 hours."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: B\nWorkload Identity is Google's recommended way to access Google Cloud services from within GKE applications. It's more secure than using service account keys and follows security best practices."
      }
    ]
  },
  {
    "id": 232,
    "source": "examtopics",
    "question": "Your companyâ€™s developers use an automation that you recently built to provision Linux VMs in Compute Engine within a Google Cloud project to perform various tasks. You need to manage the Linux account lifecycle and access for these users. You want to follow Google-recommended practices to simplify access management while minimizing operational costs. What should you do?",
    "options": {
      "A": "Enable OS Login for all VMs. Use IAM roles to grant user permissions.",
      "B": "Require your developers to create public SSH keys. Write custom startup scripts to update user permissions.",
      "C": "Require your developers to create public SSH keys. Make the owner of the public key the root user.",
      "D": "Enable OS Login for all VMs. Write custom startup scripts to update user permissions."
    },
    "correct": "A",
    "discussions": []
  },
  {
    "id": 233,
    "source": "examtopics",
    "question": "You are managing the security configuration of your companyâ€™s Google Cloud organization. The Operations team needs specific permissions on both a Google Kubernetes Engine (GKE) cluster and a Cloud SQL instance. Two predefined Identity and Access Management (IAM) roles exist that contain a subset of the permissions needed by the team. You need to configure the necessary IAM permissions for this team while following Google-recommended practices. What should you do?",
    "options": {
      "A": "Create a custom IAM role that combines the permissions from the two relevant predefined roles.",
      "B": "Grant the team the two predefined IAM roles.",
      "C": "Create a custom IAM role that includes only the required permissions from the predefined roles.",
      "D": "Grant the team the IAM roles of Kubernetes Engine Admin and Cloud SQL Admin."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: MohannadSamir Highly Voted 8Â months ago\nSelected Answer: C\ncontain a subset of the permissions needed --> Can't use predefined IAM roles\nFor least privilege take the neccesary permission only\nThat's why it's C\n\"In collaboration with Tito\""
      },
      {
        "index": 2,
        "text": "Anonymous: psou7 Most Recent 2Â weeks ago\nSelected Answer: B\nNot C: why? Custom roles should only be used when predefined roles cannot meet the requirement. They require ongoing maintenance as permissions evolve.\nRoles already exist."
      },
      {
        "index": 3,
        "text": "Anonymous: ECruz 2Â months ago\nSelected Answer: C\nIt's B"
      },
      {
        "index": 4,
        "text": "Anonymous: guaose 2Â months, 1Â week ago\nSelected Answer: B\nGoogle-recommended practice is to use predefined IAM roles whenever possible, as they are maintained and updated by Google to reflect best practices and service changes.\nIf two predefined roles already contain the necessary permissions, granting both roles is simpler, more maintainable, and avoids the overhead of managing custom roles.\nThis approach also ensures least privilege access, assuming the roles are scoped appropriately (e.g., at the resource level rather than project-wide)."
      },
      {
        "index": 5,
        "text": "Anonymous: b02d5eb 5Â months, 3Â weeks ago\nSelected Answer: C\nCorrect answer is C"
      }
    ]
  },
  {
    "id": 234,
    "source": "examtopics",
    "question": "Your organization has decided to deploy all its compute workloads to Kubernetes on Google Cloud and two other cloud providers. You want to build an Infrastructure-as-code solution to automate the provisioning process for all cloud resources. What should you do?",
    "options": {
      "A": "Build the solution by using Config Connector, and provision the resources.",
      "B": "Build the solution by using Terraform, and provision the resources.",
      "C": "Build solution by using Python and the cloud SDKs from all providers to provision the resources.",
      "D": "Build the solution by using YAML manifests, and provision the resources."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 235,
    "source": "examtopics",
    "question": "You are planning to deploy an application to Google Cloud. Your application processes asynchronous events from Google services and must be accessible from the public Internet. You need to identify how to deploy your application. You want to follow a standardized process while minimizing development costs. You also want to have no costs when your workloads are not in use. What should you do?",
    "options": {
      "A": "Deploy your code to GKE. Use Pub/Sub for event delivery.",
      "B": "Deploy your code to Compute Engine. Use Pub/Sub for event delivery.",
      "C": "Deploy your code to GKE. Use Eventarc for event delivery.",
      "D": "Deploy your code to Cloud Run. Use Eventarc for event delivery."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: toasty 6Â months, 1Â week ago\nSelected Answer: D\nCloud Run and Evenarc are the serverless solutions.\nAlso, Eventarc should be used for public internet."
      },
      {
        "index": 2,
        "text": "Anonymous: MohannadSamir 8Â months ago\nSelected Answer: D\nPublic internet --> Eventarc"
      }
    ]
  },
  {
    "id": 236,
    "source": "examtopics",
    "question": "Your company is migrating its workloads to Google Cloud due to an expiring data center contract. The on-premises environment and Google Cloud are not connected. You have decided to follow a lift-and-shift approach, and you plan to modernize the workloads in a future project. Several old applications connect to each other through hard-coded internal IP addresses. You want to migrate these workloads quickly without modifying the application code. You also want to maintain all functionality. What should you do?",
    "options": {
      "A": "Migrate your DNS server first. Configure Cloud DNS with a forwarding zone to your migrated DNS server. Then migrate all other workloads with ephemeral internal IP addresses.",
      "B": "Create a VPC with non-overlapping CIDR ranges compared to your on-premises network. When migrating individual workloads, assign each workload a new static internal IP address.",
      "C": "Create a VPC with the same CIDR ranges as your on-premises network. When migrating individual workloads, assign each workload the same static internal IP address.",
      "D": "Migrate all workloads to a single VPC subnet. Configure Cloud NAT for the subnet and manually assign a static IP address to the Cloud NAT gateway."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 237,
    "source": "examtopics",
    "question": "You are migrating your companyâ€™s on-premises compute resources to Google Cloud. You need to deploy batch processing jobs that run every night. The jobs require significant CPU and memory for several hours but can tolerate interruptions. You must ensure that the deployment is cost-effective. What should you do?",
    "options": {
      "A": "Use the M1 machine series on Compute Engine.",
      "B": "Containerize the batch processing jobs and deploy them on Compute Engine.",
      "C": "Use Spot VMs on Compute Engine.",
      "D": "Use custom machine types on Compute Engine."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: C\nwhenever you hear batch and low cost, go for spot VMs"
      }
    ]
  },
  {
    "id": 238,
    "source": "examtopics",
    "question": "Your company has a rapidly growing social media platform and a user base primarily located in North America. Due to increasing demand, your current on-premises PostgreSQL database, hosted in your United States headquarters data center, no longer meets your needs. You need to identify a cloud-based database solution that offers automatic scaling, multi-region support for future expansion, and maintains low latency. What should you do?",
    "options": {
      "A": "Use BigQuery.",
      "B": "Use Spanner.",
      "C": "Use Cloud SQL for PostgreSQL.",
      "D": "Use Bigtable."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: B\nwhen it says multi regional with future expansion, then choose spanner"
      }
    ]
  },
  {
    "id": 239,
    "source": "examtopics",
    "question": "You are migrating your on-premises workload to Google Cloud. Your company is implementing its Cloud Billing configuration and requires access to a granular breakdown of its Google Cloud costs. You need to ensure that the Cloud Billing datasets are available in BigQuery so you can conduct a detailed analysis of costs. What should you do?",
    "options": {
      "A": "Enable Cloud Billing data export to BigQuery when you create a Cloud Billing account.",
      "B": "Enable Cloud Billing on the project, and link a Cloud Billing account. Then view the billing data table in the BigQuery dataset.",
      "C": "Create a Cloud Billing account. Enable the BigQuery Data Transfer Service API to export pricing data.",
      "D": "Enable the BigQuery API, and ensure that the BigQuery User IAM role is selected. Change the BigQuery dataset to select a data location."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: A\nthe \"Export\" is the golden word"
      }
    ]
  },
  {
    "id": 240,
    "source": "examtopics",
    "question": "Your companyâ€™s accounting department needs to run an overnight batch workload every day. You must implement a solution that minimizes the cost to run this workload and automatically retries the batch if an execution fails. What should you do?",
    "options": {
      "A": "Develop an application that runs the batch workload, and deploy the application as a Google Kubernetes Engine (GKE) CronJob.",
      "B": "Develop a web application that listens for incoming requests, and deploy the application as a Cloud Run service. Use Cloud Scheduler to call the HTTP endpoint.",
      "C": "Develop an application that runs the batch workload, and deploy the application as a Cloud Run job. Use Cloud Scheduler to trigger the job.",
      "D": "Develop a web application that listens for incoming requests, and deploy the application as a Google Kubernetes Engine (GKE) Deployment. Use Cloud Scheduler to call the HTTP endpoint."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 241,
    "source": "examtopics",
    "question": "You need to migrate multiple PostgreSQL databases from your on-premises data center to Google Cloud. You want to significantly improve the performance of your databases while minimizing changes to your data schema and application code. You expect to exceed 150 TB of data per geographical region. You want to follow Google-recommended practices and minimize your operational costs. What should you do?",
    "options": {
      "A": "Migrate your data to AlloyDB.",
      "B": "Migrate your data to Spanner.",
      "C": "Migrate your data to Firebase.",
      "D": "Migrate your data to Bigtable."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: psou7 2Â weeks ago\nSelected Answer: A\necause you want PostgreSQL compatibility with minimal schema/app changes and a significant performance improvement while keeping ops costs low, AlloyDB for PostgreSQL is Googleâ€™s managed, PostgreSQL-compatible option designed for exactly that.\nSpanner would require some schema/app changes"
      },
      {
        "index": 2,
        "text": "Anonymous: SajadAhm 3Â weeks ago\nSelected Answer: A\nWhile a single AlloyDB cluster currently has a storage limit (often cited around 128 TiB per cluster), your requirement is for multiple databases exceeding 150 TB per region. By deploying multiple AlloyDB clusters, you can easily exceed the 150 TB regional requirement while maintaining full PostgreSQL compatibility for each individual database."
      },
      {
        "index": 3,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: A\nAlloyDB is a fully managed, PostgreSQL-compatible database from Google Cloud that delivers very high performance, automatic scaling, and enterprise reliability, while still behaving like PostgreSQL for developers."
      },
      {
        "index": 4,
        "text": "Anonymous: Bobaka 1Â month, 3Â weeks ago\nSelected Answer: B\nB. Migrate your data to Spanner."
      },
      {
        "index": 5,
        "text": "Anonymous: AdelElagawany 3Â months, 1Â week ago\nSelected Answer: B\nThe default value for Resource quotas on storage for Alloydb is 16 TiB per cluster. The maximum supported value is 128 TiB per cluster: https://cloud.google.com/alloydb/quotas#cluster-storage-quotas\nAdditionally, Cloud SQL has also a limit of 64TiB. The Doc explicitly outlines Creating that \"increasing storage capacity to 64 TB might increase latency of common operations, such as backups, dependent on your workload\": https://cloud.google.com/sql/docs/quotas#storage_limits\nWith 150 GB TiB requirement in mind, so I will go for Spanner"
      },
      {
        "index": 6,
        "text": "Anonymous: toasty 6Â months, 2Â weeks ago\nSelected Answer: A\nSignificant performance improvement: AlloyDB delivers this for PostgreSQL workloads.\nMinimizing changes to data schema and application code: AlloyDB is PostgreSQL-compatible, making this the best option. Spanner, Firebase, and Bigtable would all require substantial changes.\nExceed 150 TB of data per geographical region: AlloyDB is designed for large-scale enterprise workloads and can scale. While Spanner and Bigtable can handle this more easily, they fail on the schema/code change requirement.\nGoogle-recommended practices: AlloyDB is Google's flagship PostgreSQL-compatible service.\nMinimize operational costs: As a fully managed service, AlloyDB reduces operational overhead."
      }
    ]
  },
  {
    "id": 242,
    "source": "examtopics",
    "question": "Your company's machine learning team requires a scalable and flexible platform to fine-tune large language models utilizing a large volume of proprietary data on Google Cloud. You are tasked with building a solution for this team. What should you do?",
    "options": {
      "A": "Use Dataflow as a platform to run the fine-tuning jobs",
      "B": "Use a Compute Engine managed instance group as a platform to deploy Jupyter Notebooks and run fine-tuning jobs.",
      "C": "Use Cloud Run and GPU as a platform to run the fine-tuning jobs.",
      "D": "Use Google Kubernetes Engine (GKE) and hardware accelerators as a platform to run the fine-tuning jobs."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: toasty 6Â months, 1Â week ago\nSelected Answer: D\nGKE with hardware accelerators provides the robust, scalable, and flexible platform required by a machine learning team for fine-tuning large language models. It is the industry-standard for orchestrating containerized machine learning workloads at scale, offering the necessary control and automation for complex, distributed training jobs. The other options are either not designed for this purpose (Dataflow) or lack the comprehensive orchestration and scalability features of GKE for this specific use case (Compute Engine MIGs and Cloud Run)."
      }
    ]
  },
  {
    "id": 243,
    "source": "examtopics",
    "question": "You recently discovered an issue with your rolling update in Google Kubernetes Engine (GKE). You now need to roll back a rolling update. What should you do?",
    "options": {
      "A": "Delete the deployment.",
      "B": "Use the kubectl rollout restart command to revert the deployment.",
      "C": "Use the kubectl rollout undo command.",
      "D": "Manually scale down the new Pods and scale up the old Pods."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: C\nkubectl rollout undo is the Kubernetes-native way to roll back a deployment to a previous revision after a faulty update."
      }
    ]
  },
  {
    "id": 244,
    "source": "examtopics",
    "question": "You are deploying an application to Google Kubernetes Engine (GKE) that needs to call an external third-party API. You need to provide the external API vendor with a list of IP addresses for their firewall to allow traffic from your application. You want to follow Google-recommended practices and avoid any risk of interrupting traffic to the API due to IP address changes. What should you do?",
    "options": {
      "A": "Configure your GKE cluster with one node, and set the node to have a static external IP address. Ensure that the GKE cluster autoscaler is off. Send the external IP address of the node to the vendor to be added to the allowlist.",
      "B": "Configure your GKE cluster with private nodes. Configure a Cloud NAT instance with static IP addresses. Provide these IP addresses to the vendor to be added to the allowlist.",
      "C": "Configure your GKE cluster with private nodes. Configure a Cloud NAT instance with dynamic IP addresses. Provide these IP addresses to the vendor to be added to the allowlist.",
      "D": "Configure your GKE cluster with public nodes. Write a Cloud Function that pulls the public IP addresses of each node in the cluster, Trigger the function to run every day with Cloud Scheduler. Send the list to the vendor by email every day."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: B\ncloudnat is a managed service in gcp that enables resources to have an static egress (outgoing) ip address."
      }
    ]
  },
  {
    "id": 245,
    "source": "examtopics",
    "question": "You are planning to move your company's website and a specific asynchronous background job to Google Cloud. Your website contains only static HTML content. The background job is started through an HTTP endpoint and generates monthly invoices for your customers. Your website needs to be available in multiple geographic locations and requires autoscaling. You want to have no costs when your workloads are not in use and follow recommended practices. What should you do?",
    "options": {
      "A": "Move your website to Google Kubernetes Engine (GKE), and move your background job to Cloud Functions.",
      "B": "Move both your website and background job to Compute Engine.",
      "C": "Move both your website and background job to Cloud Run.",
      "D": "Move your website to Google Kubernetes Engine (GKE), and move your background job to Compute Engine."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 246,
    "source": "examtopics",
    "question": "Your company wants to provide engineers with access to explore Google Cloud freely in a sandbox environment. The total budget for testing across your organization is $1,000. You need to ensure that engineers are notified when the budget is about to be reached. You want to automate your solution as much as possible. What should you do?",
    "options": {
      "A": "Create a separate Cloud Billing account for all sandbox projects. Link a credit card with a limit of $1,000 to this billing account. Ensure all sandbox projects are linked to this new Cloud Billing account.",
      "B": "Create a Google Cloud Folder and ensure that all sandbox projects are located under that Folder. Create a Budget Alert for $1,000 and scope it to the Folder. Configure an email alert to billing administrators and users once the budget is 90% reached.",
      "C": "Create an email template reminding people to regularly check their Google Cloud spend. Create a Cloud Function that sends the email to all the project owners. Create a daily job in Cloud Scheduler that triggers the Cloud Function. Deploy this solution for each sandbox.",
      "D": "Configure a billing data export to a BigQuery dataset on the Cloud Billing account. Create a dashboard for all costs related to the sandbox experiments. Share the dashboard with all engineers."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 247,
    "source": "examtopics",
    "question": "You are planning to migrate your on-premises VMs to Google Cloud. You need to set up a landing zone in Google Cloud before migrating the VMs. You must ensure that all VM in your production environment can communicate with each other through private IP addresses. You need to allow all VMs in your Google Cloud organization to accept connections on specific TCP ports. You want to follow Google-recommended practices, and you need to minimize your operational costs. What should you do?",
    "options": {
      "A": "Create individual VPCs per Google Cloud project. Peer all he VPC together. Apply organization policies on the organization level.",
      "B": "Create individual VPCs for each Google Cloud project. Peer ail ne VPCs together. Apply hierarchical firewall policies on the organization level.",
      "C": "Create a host VPC project with each production project as its service project. Apply organization policies on the organization level.",
      "D": "Create a host VPC project with each production project as its service project. Apply hierarchical firewall policies on the organization level."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: D\nOrg policies cannot define firewall port rules."
      },
      {
        "index": 2,
        "text": "Anonymous: epuser4791 1Â month, 4Â weeks ago\nSelected Answer: C\nI think C because of the organization policy, and in shared VPC the VM-s can see each other."
      },
      {
        "index": 3,
        "text": "Anonymous: toasty 6Â months, 1Â week ago\nSelected Answer: D\nthe combination that meets all the requirements is the use of a Shared VPC (Create a host VPC project with each production project as its service project) and Hierarchical Firewall Policies (Apply hierarchical firewall policies on the organization level). This approach aligns perfectly with Google Cloud's best practices for enterprise-level landing zones, providing the necessary network connectivity, centralized security control, and operational simplicity."
      }
    ]
  },
  {
    "id": 248,
    "source": "examtopics",
    "question": "You assist different engineering teams in deploying their infrastructure on Google Cloud. Your company has defined certain practices required for all workloads. You need to provide the engineering teams with a solution that enables teams to deploy their infrastructure independently without having to know all implementation details of the companyâ€™s required practices. What should you do?",
    "options": {
      "A": "Configure organization policies to enforce your company's required practices. Ask the teams to provision their infrastructure by using the Google Cloud console.",
      "B": "Create a service account per team, and grant the service account the Project Editor role. Ask the teams to provision their infrastructure through the Google Cloud CLI (gcloud CL), while impersonating their dedicated service account.",
      "C": "Write Terraform modules for each component that are compliant with the company's required practices, and ask teams to implement their infrastructure through these modules.",
      "D": "Provide training for all engineering teams you work with to understand the companyâ€™s required practices. Allow the engineering teams to provision the infrastructure to best meet their needs."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: C\nTerraform modules can include all policies, like blueprints, and you can reuse them"
      }
    ]
  },
  {
    "id": 249,
    "source": "examtopics",
    "question": "You ate managing an application deployed on Cloud Run. The development team has released a new version of the application. You want to deploy and redirect traffic to this new version of the application. To ensure traffic to the new version of the application is served with no startup time, you want to ensure that there are two idle instances available for incoming traffic before adjusting the traffic flow. You also want to minimize administrative overhead. What should you do?",
    "options": {
      "A": "Ensure the checkbox â€œServe this revision immediatelyâ€ is unchecked when deploying the new revision. Before changing the traffic rules, use a traffic simulation tool to send load to the new revision.",
      "B": "Configure service autoscaling and set the minimum number of instances to 2.",
      "C": "Configure revision autoscaling for the new revision and set the minimum number of instances to 2.",
      "D": "Configure revision autoscaling for the existing revision and set the minimum number of instances to 2."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SajadAhm 1Â month, 2Â weeks ago\nSelected Answer: C\nIn Cloud Run, autoscaling settings (including minimum instances) are configured per revision, not at the service level"
      }
    ]
  },
  {
    "id": 250,
    "source": "examtopics",
    "question": "You are managing a stateful application deployed on Google Kubernetes Engine (GKE) that can only have one replica. You recently discovered that the application becomes unstable at peak times. You have identified that the application needs more CPU than what has been configured in the manifest at these peak times. You want Kubernetes to allocate the application sufficient CPU resources during these peak times, while ensuring cost efficiency during off-peak periods. What should you do?",
    "options": {
      "A": "Enable node auto-provisioning on the GKE cluster.",
      "B": "Configure a Vertical Pod Autoscaler on the Deployment.",
      "C": "Configure a Horizontal Pod Autoscaler on the Deployment.",
      "D": "Enable cluster autoscaling on the GKE cluster."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 251,
    "source": "examtopics",
    "question": "Your company runs a variety of applications and workloads on Google Cloud, and you are responsible for managing cloud costs. You need to identify a solution that enables you to perform detailed cost analysis. You also must be able to visualize the cost data in multiple ways on the same dashboard. What should you do?",
    "options": {
      "A": "Use the cost breakdown report with the available filters from Cloud Billing to visualize the data.",
      "B": "Enable the Cloud Billing export to BigQuery, and use Looker Studio to visualize the data.",
      "C": "Run queries in Cloud Monitoring. Create dashboards to visualize the billing metrics.",
      "D": "Enable Cloud Monitoring metrics export to BigQuery, and use Looker to visualize the data."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 252,
    "source": "examtopics",
    "question": "Your digital media company stores a large number of video files on-premises. Each video file ranges from 100 MB to 100 GB. You are currently storing 150 TB of video data in your on-premises network, with no room for expansion. You need to migrate all infrequently accessed video files older than one year to Cloud Storage to ensure that on-premises storage remains available for new files. You must also minimize costs and control bandwidth usage. What should you do?",
    "options": {
      "A": "Use Storage Transfer Service to move the data from the selected on-premises file storage systems to a Cloud Storage bucket.",
      "B": "Use Transfer Appliance to request an appliance. Load the data locally, and ship the appliance back to Google for ingestion into Cloud Storage.",
      "C": "Set up a Cloud Interconnect connection between the on-premises network and Google Cloud. Establish a private endpoint for Filestore access. Transfer the data from the existing Network File System (NFS) to Filestore.",
      "D": "Create a Cloud Storage bucket. Establish an Identity and Access Management (IAM) role with write permissions to the bucket. Use the gsutil tool to directly copy files over the network to Cloud Storage."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gius3 1Â day, 10Â hours ago\nSelected Answer: B\nActually the question does not provide information about the size of data to be transferred, so it is not possible to make a choice between A and B. I choose B as since even 20% of overall basket justifies Transfer Appliance"
      },
      {
        "index": 2,
        "text": "Anonymous: 763609c 1Â month, 3Â weeks ago\nSelected Answer: B\nChoose Transfer Appliance for large on-premises data transfers (over 10 TB)"
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany 3Â months ago\nSelected Answer: A\nThe most accurate answer is [A] since Storage Transfer Service allow you to control the bandwidth.\nStorage options:"
      },
      {
        "index": 1,
        "text": "Data Size < 1TB ==> gcloud storage"
      },
      {
        "index": 2,
        "text": "Data Size > 1 TB And enough bandwidth ==> STS"
      },
      {
        "index": 3,
        "text": "Data Size > 1TB and between buckets in GCP ==> STS\nFrom other Cloud Providers => STS\nThe only recommended way to use the storage appliance when (Date Size > 1TB) and (Not enough bandwidth to meet your project deadline)\n- Control Bandwidth: https://cloud.google.com/storage-transfer/docs/obtaining-bandwidth-on-prem\n- Data transfer options: https://cloud.google.com/storage-transfer/docs/transfer-options\n- Data transfer options:https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"
      },
      {
        "index": 4,
        "text": "Anonymous: Isfaten7 3Â months, 1Â week ago\nSelected Answer: B\n150 TB is very large â€” transferring that amount of data over the internet (even with Storage Transfer Service or gsutil) would take weeks and use a lot of bandwidth"
      },
      {
        "index": 5,
        "text": "Anonymous: reiga 4Â months, 1Â week ago\nSelected Answer: A\nStorage Transfer Service is a managed service designed specifically for use cases like this (large-scale online data migration). It automatically manages data transfers from on-premises file systems or other cloud storage to Cloud Storage, including scheduling, bandwidth throttling, and retry handling."
      }
    ]
  },
  {
    "id": 253,
    "source": "examtopics",
    "question": "You are implementing a company-wide standard to control SSH access for your Google Cloud projects. You want to simplify SSH access management to your Compute Engine instances while maintaining audit compliance and eliminating as many manual steps as possible. What should you do?",
    "options": {
      "A": "Configure a service account to add SSH keys for all VMs.",
      "B": "Configure metadata SSH keys to manage sudo access to instances.",
      "C": "Enable OS Login by using an organization policy for each Google Cloud project.",
      "D": "Enable OS Login with two-factor authentication for the domain."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 254,
    "source": "examtopics",
    "question": "You are developing an internet of things (IoT) application that captures sensor data from multiple devices that have already been set up. You need to identify the global data storage product your company should use to store this data. You must ensure that the storage solution you choose meets your requirements of sub-millisecond latency. What should you do?",
    "options": {
      "A": "Store the IoT data in Spanner. Use caches to speed up the process and avoid latencies.",
      "B": "Store the IoT data in Bigtable.",
      "C": "Capture IoT data in BigQuery datasets.",
      "D": "Store the IoT data in Cloud Storage. Implement caching by using Cloud CDN."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 255,
    "source": "examtopics",
    "question": "Your company uses Pub/Sub for event-driven workloads. You have a subscription named email-updates attached to the new-orders topic. You need to fetch and acknowledge waiting messages from this subscription. What should you do?",
    "options": {
      "A": "Use the gcloud pubsub subscriptions seek email-updates command.",
      "B": "Use the gcloud pubsub topics describe new-orders command.",
      "C": "Use the gcloud pubsub subscriptions pull email-updates --auto-ack command.",
      "D": "Use the gcloud pubsub topics list-subscriptions new-orders --filter=\"email-updates\" command."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 256,
    "source": "examtopics",
    "question": "You need to create and manage service accounts for your workloads running on Google Cloud. You want to follow Google-recommended practices. What should you do? (Choose two.)",
    "options": {
      "A": "Create as few service accounts as possible.",
      "B": "Delete any unused service accounts immediately.",
      "C": "Create single-purpose service accounts.",
      "D": "Manage service accounts as resources."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: AdelElagawany 3Â months ago\nSelected Answer: CD\nAs per the public documentation [1], below are the 5 best practices of using the SA:\nBest practices:\n- Manage service accounts as resources.\n- Create single-purpose service accounts.\n- Follow a naming and documentation convention.\n- Identify and disable unused service accounts.\n- Disable unused service accounts before deleting them.\nhttps://cloud.google.com/iam/docs/best-practices-service-accounts#manage-service-accounts"
      },
      {
        "index": 2,
        "text": "Anonymous: ladyerina 3Â months, 1Â week ago\nSelected Answer: BC\nFollowing Google-recommended practices:\nDelete unused service accounts immediately to reduce security risks.\nCreate single-purpose service accounts to limit permissions to specific tasks, improving security and maintainability."
      },
      {
        "index": 3,
        "text": "Anonymous: gs23mi 5Â months ago\nSelected Answer: CD\nsee https://cloud.google.com/iam/docs/best-practices-service-accounts"
      }
    ]
  },
  {
    "id": 257,
    "source": "examtopics",
    "question": "Your company wants to migrate your data from an on-premises relational database to Google Cloud. Your current database can no longer scale with respect to the growth of your users, and you expect the number of users to rapidly grow. You need to choose a relational database that allows you to globally scale while minimizing your management and administration efforts. You also want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Use Cloud SQL.",
      "B": "Use Spanner.",
      "C": "Use Firestore.",
      "D": "Use BigQuery."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 258,
    "source": "examtopics",
    "question": "You are the Google Cloud systems administrator for your organization. User A reports that they received an error when attempting to access the Cloud SQL database in their Google Cloud project, while User B can access the database. You need to troubleshoot the issue for User A, while following Google-recommended practices. What should you do first?",
    "options": {
      "A": "Confirm that network firewall rules are not blocking traffic for User A.",
      "B": "Verify that User A has the Identity and Access Management (IAM) Project Owner role assigned.",
      "C": "Review recent configuration changes that may have caused unintended modifications to permissions.",
      "D": "Review the error message that User A received."
    },
    "correct": "D",
    "discussions": []
  },
  {
    "id": 259,
    "source": "examtopics",
    "question": "You manage a VPC network in Google Cloud with a subnet that is rapidly approaching its private IP address capacity. You expect the number of Compute Engine VM instances in the same region to double within a week. You need to implement a Google-recommended solution that minimizes operational costs and does not require downtime. What should you do?",
    "options": {
      "A": "Create a second VPC with the same subnet IP range, and connect this VPC to the existing VPC by using VPC Network Peering.",
      "B": "Delete the existing subnet, and create a new subnet with double the IP range available.",
      "C": "Permit additional traffic from the expected range of private IP addresses to reach your VMs by configuring firewall rules.",
      "D": "Use the Google Cloud CLI tool to expand the primary IP range of your subnet."
    },
    "correct": "D",
    "discussions": []
  },
  {
    "id": 260,
    "source": "examtopics",
    "question": "You have developed a web application that serves traffic for a local event and are expecting unpredictable traffic. You have containerized the application, and you now want to deploy the application on Google Cloud. You also want to minimize costs. What should you do?",
    "options": {
      "A": "Deploy the web application as a Cloud Run job.",
      "B": "Deploy the web application on Google Kubernetes Engine in Standard mode.",
      "C": "Deploy the web application as a Cloud Run service.",
      "D": "Deploy the web application on Google Kubernetes Engine in Autopilot mode."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 261,
    "source": "examtopics",
    "question": "You host your website on Compute Engine. The number of global users visiting your website is rapidly expanding. You need to minimize latency and support user growth in multiple geographical regions. You also want to follow Google-recommended practices and minimize operational costs. Which two actions should you take? (Choose two.)",
    "options": {
      "A": "Use an external Application Load Balancer in Regional mode.",
      "B": "Deploy your VMs in multiple Google Cloud regions closest to your userâ€™s geographical locations.",
      "C": "Use a Network Load Balancer.",
      "D": "Use an external Application Load Balancer in Global mode."
    },
    "correct": "D",
    "discussions": []
  },
  {
    "id": 262,
    "source": "examtopics",
    "question": "You are writing a shell script that includes a few gcloud CLI commands to access some Google Cloud resources. You want to test the script in your local development environment with a service account in the most secure way. What should you do?",
    "options": {
      "A": "Generate an ID token for the service account. Use the token with the gcloud CLI commands.",
      "B": "Enable service account impersonation, and use the gcloud config set command to use it by default.",
      "C": "Download the service account key file and save it in a secure location. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the key file.",
      "D": "Download the service account key file, and use it to generate an access token. Use the token with the gcloud CLI commands."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 263,
    "source": "examtopics",
    "question": "Your company is active in the European Economic Area (EEA), and will adopt Google Cloud for its workloads. Projects are currently structured within different folders. You need to ensure any resources that will be deployed are using Google Cloud locations within the EEA by using the Organization Policy Service resource locations constraint. What should you do?",
    "options": {
      "A": "Configure the policy at the folder level, and add all allowed locations to the policy.",
      "B": "Configure the policy at the organization level, and add all allowed locations to the policy.",
      "C": "Configure the policy at the folder level, and add all disallowed locations to the policy.",
      "D": "Configure the policy at the organization level, and add all disallowed locations to the policy."
    },
    "correct": "B",
    "discussions": []
  },
  {
    "id": 264,
    "source": "examtopics",
    "question": "You are deploying a large, multi-tiered application with more than 1,000 IP addresses in a Google Cloud project that needs to be securely isolated. The application includes the:\n\n1. web tier with frontend servers for public traffic,\n2. application tier with servers running core application logic that only need access from the web tier, and\n3. database tier with database servers that only need access from the application tier.\n\nYou want to minimize cost, complexity, and administrative overhead in the network architecture. What should you do?",
    "options": {
      "A": "Create a /24 Shared VPC with separate subnets for each tier. Use firewall rules that reference network tags to control traffic.",
      "B": "Create one custom mode /16 VPC with three subnets. Place each tier in its own subnet and use firewall rules that reference IP subnets to control traffic.",
      "C": "Deploy each tier into a separate custom mode /16 VPUse VPC Network Peering to securely connect each custom mode VPManage firewall rules individually in each VPC.",
      "D": "Deploy each tier in a /24 VPC by using network tags to identify instances. Implement firewall rules for fine-grained network segmentation."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: chukwud3b3 2Â weeks, 6Â days ago\nSelected Answer: B\nB is right"
      }
    ]
  },
  {
    "id": 265,
    "source": "examtopics",
    "question": "Your company is closely monitoring their cloud spend. You need to allow different teams to monitor their Google Cloud costs. You must ensure that team members receive notifications when their cloud spend reaches certain thresholds and give team members the ability to create dashboards for additional insights with detailed billing data. You want to follow Google-recommended practices and minimize engineering costs. What should you do?",
    "options": {
      "A": "Deploy Grafana to Compute Engine. Create a dashboard for each team that uses the data from the Cloud Billing API. Ask each team to create their own alerts in Cloud Monitoring.",
      "B": "Set up alerts for each team based on required thresholds. Create a shell script to read data from the Cloud Billing API, and push the results to BigQuery. Grant team members access to BigQuery.",
      "C": "Deploy Grafana to Compute Engine. Create a dashboard for each team that uses the data from the Cloud Billing Budget API. Ask each team to create their own alerts in Grafana.",
      "D": "Set up alerts for each team based on required thresholds. Set up billing exports to BigQuery. Grant team members access to BigQuery."
    },
    "correct": "D",
    "discussions": []
  },
  {
    "id": 266,
    "source": "examtopics",
    "question": "Your company plans to migrate its on-premises PostgreSQL database to Google Cloud. The workloads are demanding, requiring fast transactional and analytical performance. You need to select a fully managed database service on Google Cloud. Your solution must also be able to synchronously replicate and optimize the storage layer. What should you do?",
    "options": {
      "A": "Migrate the database to Cloud SQL for PostgreSQL by using Database Migration Service.",
      "B": "Use the psql client installed on a Compute Engine instance. Connect to the Cloud SQL instance to perform the database migration.",
      "C": "Migrate the database to AlloyDB for PostgreSQL by using Database Migration Service.",
      "D": "Create a Compute Engine instance. Install and configure PostgreSQL on the instance, and migrate the database."
    },
    "correct": "C",
    "discussions": []
  },
  {
    "id": 267,
    "source": "examtopics",
    "question": "You have 32 GB of data in a single file that you need to upload to a Nearline Storage bucket. The WAN connection you are using is rated at 1 Gbps, and you are the only one on the connection. You want to use as much of the rated 1 Gbps as possible to transfer the file rapidly. How should you upload the file?",
    "options": {
      "A": "Use the GCP Console to transfer the file instead of gsutil.",
      "B": "Enable parallel composite uploads using gsutil on the file transfer.",
      "C": "Decrease the TCP window size on the machine initiating the transfer.",
      "D": "Change the storage class of the bucket from Nearline to Multi-Regional."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: leba Highly Voted 5Â years, 8Â months ago\nCorrect answer is B as the bandwidth is good and its a single file, gsutil parallel composite uploads can be used to split the large file and upload in parallel.Refer GCP documentation - Transferring Data to GCP &amp"
      },
      {
        "index": 2,
        "text": "Anonymous: berezinsn Highly Voted 5Â years, 7Â months ago\nTruly B is absolutely correct"
      },
      {
        "index": 3,
        "text": "Anonymous: sh00001 Most Recent 1Â year, 6Â months ago\nB- Enable parallel composite uploads using gsutil on the file transfer.\nThis option is correct because parallel composite uploads can break down a large file into smaller components, upload them in parallel, and recombine them into a single object in the cloud. This method takes advantage of the available bandwidth more efficiently than serial uploads, as it can simultaneously transmit multiple parts of the file over the network. The gsutil tool has a -o option that allows enabling of parallel composite uploads."
      },
      {
        "index": 4,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: B"
      },
      {
        "index": 5,
        "text": "Anonymous: kenjaixv 2Â years, 4Â months ago\nThe best option to upload the file is B. Enable parallel composite uploads using gsutil on the file transfer. This is because parallel composite uploads can speed up the upload of large files by dividing them into chue upload time.\nThe other options are not as effective or feasible as option B:\nOption A. Use the GCP Console to transfer the file instead of gsutil is not a good choice because the GCP Console has a limit of 5 GB per file upload.\nOption C. Decrease the TCP window size on the machine initiating the transfer is not advisable because it would reduce the amount of data that can be sent before receiving an acknowledgment, which could lead to lower throughput and higher latency.\nOption D. Change the storage class of the bucket from Nearline to Multi-Regional is not relevant to the upload speed, as it only affects the availability and cost of storing and accessing the data."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nb is legit correct as it helps you more to increase the speed."
      },
      {
        "index": 7,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nParallel composite is the right ans"
      },
      {
        "index": 8,
        "text": "Anonymous: Partha117 2Â years, 10Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: B\nAnswer B is correct. Enable parallel composite uploads using gsutil on the file transfer.\nThe most efficient way to upload the large file to Nearline Storage bucket using a single WAN connection rated at 1 Gbps is to enable parallel composite uploads using gsutil. By default, gsutil uses a single thread to upload a single object. But with parallel composite uploads, gsutil will split the file into smaller chunks and upload these chunks in parallel using multiple threads. This will allow the file to be uploaded faster and more efficiently.\nhttps://cloud.google.com/storage/docs/parallel-composite-uploads"
      },
      {
        "index": 10,
        "text": "Anonymous: Neo29 2Â years, 11Â months ago\nSelected Answer: B\nB is correct Answer"
      }
    ]
  },
  {
    "id": 268,
    "source": "examtopics",
    "question": "You've deployed a microservice called myapp1 to a Google Kubernetes Engine cluster using the YAML file specified below:\n\nYou need to refactor this configuration so that the database password is not stored in plain text. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Store the database password inside the Docker image of the container, not in the YAML file.",
      "B": "Store the database password inside a Secret object. Modify the YAML file to populate the DB_PASSWORD environment variable from the Secret.",
      "C": "Store the database password inside a ConfigMap object. Modify the YAML file to populate the DB_PASSWORD environment variable from the ConfigMap.",
      "D": "Store the database password in a file inside a Kubernetes persistent volume, and use a persistent volume claim to mount the volume to the container."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: rramani7 Highly Voted 5Â years, 1Â month ago\nit is good practice to use Secrets for confidential data (like API keys) and ConfigMaps for non-confidential data (like port numbers). B is correct."
      },
      {
        "index": 2,
        "text": "Anonymous: saurabh1805 Highly Voted 5Â years, 1Â month ago\nB is correct answer\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/secret hjyhf 3Â years, 11Â months ago\n\"Storing sensitive data in Secrets is more secure than in plaintext ConfigMaps or in Pod specifications\""
      },
      {
        "index": 3,
        "text": "Anonymous: 68f26bd Most Recent 10Â months, 1Â week ago\nSelected Answer: C\nI think c is correct."
      },
      {
        "index": 4,
        "text": "Anonymous: 559b96d 1Â year, 1Â month ago\nHow could this possibly be C over B?\n\"ConfigMap is similar to Secret except that you use a Secret for sensitive information and you use a ConfigMap to store non-sensitive data such as connection strings, public credentials, hostnames, and URLs.\""
      },
      {
        "index": 5,
        "text": "Anonymous: subha.elumalai 1Â year, 1Â month ago\nCorrect Answer: C"
      },
      {
        "index": 6,
        "text": "Anonymous: Sandy8 1Â year, 6Â months ago\nIn my opinion also B is correct answer as secret manager will keep secret of all credentials and confidentiality."
      },
      {
        "index": 7,
        "text": "Anonymous: Mohit__ 1Â year, 6Â months ago\nwhy most answer by examtopics are wrong"
      },
      {
        "index": 8,
        "text": "Anonymous: gsmasad 1Â year, 8Â months ago\nSelected Answer: B\nB is correct because storing passwords in secrets is the GKE best practice"
      },
      {
        "index": 9,
        "text": "Anonymous: bearfromoso 1Â year, 10Â months ago\nStoring database passwords, or any sensitive credentials, inside a ConfigMap is not recommended from a security standpoint. \"B\" it is!"
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nb is correct as it good pracits to use secrrets for the passwords"
      }
    ]
  },
  {
    "id": 269,
    "source": "examtopics",
    "question": "You are running an application on multiple virtual machines within a managed instance group and have autoscaling enabled. The autoscaling policy is configured so that additional instances are added to the group if the CPU utilization of instances goes above 80%. VMs are added until the instance group reaches its maximum limit of five VMs or until CPU utilization of instances lowers to 80%. The initial delay for HTTP health checks against the instances is set to 30 seconds.\nThe virtual machine instances take around three minutes to become available for users. You observe that when the instance group autoscales, it adds more instances then necessary to support the levels of end-user traffic. You want to properly maintain instance group sizes when autoscaling. What should you do?",
    "options": {
      "A": "Set the maximum number of instances to 1.",
      "B": "Decrease the maximum number of instances to 3.",
      "C": "Use a TCP health check instead of an HTTP health check.",
      "D": "Increase the initial delay of the HTTP health check to 200 seconds."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ac89l Highly Voted 2Â years, 2Â months ago\nI think exam will be ended by the time you finish reading the question glitterunicorn 1Â year, 2Â months ago\nyes more like a story than a question"
      },
      {
        "index": 2,
        "text": "Anonymous: berezinsn Highly Voted 5Â years, 7Â months ago\nD is correct answer."
      },
      {
        "index": 3,
        "text": "Anonymous: wota Most Recent 1Â year, 1Â month ago\nSelected Answer: D\nGemini says \"In GCP's MIG auto-scaling, HTTP health checks generally take priority over CPU utilization\"\nSo D is right"
      },
      {
        "index": 4,
        "text": "Anonymous: Yinkus 1Â year, 1Â month ago\nSelected Answer: D\nIf is going to be taking 180 seconds for additional vm to be avaliable then health check of 30second interval would not be ideal.\nOver provisioning will occur."
      },
      {
        "index": 5,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: D"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD is more correct because it gives you time to check the available instace"
      },
      {
        "index": 7,
        "text": "Anonymous: Ashish_Tayal 2Â years, 9Â months ago\nSelected Answer: D\nFirst Health check must be done after proper boot of VM."
      },
      {
        "index": 8,
        "text": "Anonymous: Partha117 2Â years, 10Â months ago\nSelected Answer: D\nIncrease delay to check all instances are available"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: D\nAnswer D is the correct solution to maintain instance group sizes when autoscaling.\nWhen autoscaling is enabled, new instances are added based on a metric or metrics (such as CPU utilization) when certain thresholds are met. When adding new instances, it is important to ensure that only the necessary number of instances are added to the instance group and that the group size is properly maintained to prevent overprovisioning and unnecessary costs.\nIn this scenario, the instance group is adding more instances than necessary when autoscaling due to the initial delay of HTTP health checks. Increasing the initial delay to 200 seconds will ensure that the health check properly reflects the actual availability of the instances and prevent overprovisioning.\nAnswers A and B limit the maximum number of instances, which could cause issues when scaling to support higher levels of end-user traffic.\nAnswer C suggests using a TCP health check instead of an HTTP health check, but it does not address the issue of overprovisioning when autoscaling."
      },
      {
        "index": 10,
        "text": "Anonymous: leogor 3Â years, 2Â months ago\nSelected Answer: D\nIncrease the initial delay"
      }
    ]
  },
  {
    "id": 270,
    "source": "examtopics",
    "question": "You need to select and configure compute resources for a set of batch processing jobs. These jobs take around 2 hours to complete and are run nightly. You want to minimize service costs. What should you do?",
    "options": {
      "A": "Select Google Kubernetes Engine. Use a single-node cluster with a small instance type.",
      "B": "Select Google Kubernetes Engine. Use a three-node cluster with micro instance types.",
      "C": "Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type.",
      "D": "Select Compute Engine. Use VM instance types that support micro bursting."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: gcper Highly Voted 4Â years, 9Â months ago\nAs everyone has said the answer is C but here is the source for the information. \"For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.\"\nsrouce: https://cloud.google.com/compute/docs/instances/preemptible"
      },
      {
        "index": 2,
        "text": "Anonymous: vnxt Highly Voted 5Â years, 2Â months ago\nI woud say C is the correct answer"
      },
      {
        "index": 3,
        "text": "Anonymous: subha.elumalai Most Recent 1Â year, 1Â month ago\nCorrect Answer: C"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nc is the correct answer, use preemptible for the compute engine"
      },
      {
        "index": 5,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: C\nANSWER C. Select Compute Engine. Use preemptible VM instances of the appropriate standard machine type.\nPreemptible VM instances offer the lowest cost for batch processing jobs in the Google Cloud Platform. Preemptible VM instances are computed instances that can run for a maximum of 24 hours and provide no availability guarantees. Preemptible VM instances are up to 80% cheaper than standard compute instances, making them an excellent choice for batch-processing workloads that can be interrupted.\nThe small instance type in a single-node cluster (ANSWER A) would not provide enough resources for batch processing jobs, and the micro instance types in a three-node cluster (ANSWER B) may not provide enough resources for the batch processing jobs to complete within the allotted time. VM instance types that support micro-bursting (ANSWER D) may not provide enough sustained CPU performance to complete batch processing jobs within the desired time frame."
      },
      {
        "index": 6,
        "text": "Anonymous: RAVI321 2Â years, 10Â months ago\nbatch processing jobs can run on preemptible instances. if some of those instances stop during processing, the job slows but does not completely stop. preemptible instances camplete your batch processing tasks without placing additional worklods on your existing instances and without requring you to pay full price for additional normal instances\""
      },
      {
        "index": 7,
        "text": "Anonymous: RAVI321 2Â years, 11Â months ago\nhey guys tell me one important thing i am learning GCP but did not get anything i mean whatever you guys are discussing in this forum shykot 2Â years, 7Â months ago\nas you said you are learning, it takes time to master"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3Â years ago\nC is right .\nIf your apps are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances."
      },
      {
        "index": 9,
        "text": "Anonymous: sedado77 3Â years, 1Â month ago\nSelected Answer: C\nYup, C for batch and cost"
      },
      {
        "index": 10,
        "text": "Anonymous: haroldbenites 3Â years, 1Â month ago\nGo for C"
      }
    ]
  },
  {
    "id": 271,
    "source": "examtopics",
    "question": "You recently deployed a new version of an application to App Engine and then discovered a bug in the release. You need to immediately revert to the prior version of the application. What should you do?",
    "options": {
      "A": "Run gcloud app restore.",
      "B": "On the App Engine page of the GCP Console, select the application that needs to be reverted and click Revert.",
      "C": "On the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version.",
      "D": "Deploy the original version as a separate application. Then go to App Engine settings and split traffic between applications so that the original version serves 100% of the requests."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\ncorrect is C NOT D.\nOption A is wrong as gcloud app restore was used for backup and restore and has been deprecated.Option B is wrong as there is no application revert functionality available.Option D is wrong as App Engine maintains version and need not be redeployed."
      },
      {
        "index": 2,
        "text": "Anonymous: Bharathy Highly Voted 5Â years, 10Â months ago\nApp engine maintains versions and to revert back to previous version, traffic can be set to 100% for the prior version.. hence correct answer is C"
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nC is the correct answer"
      },
      {
        "index": 4,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: C\nC is correct because app engine maintains version"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nc is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: YomanB 2Â years, 4Â months ago\nCorrect option is C."
      },
      {
        "index": 7,
        "text": "Anonymous: RobAlt 2Â years, 4Â months ago\nSelected Answer: C\nApp Engine Version page and route 100% to the previous version"
      },
      {
        "index": 8,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nC is faster. Stick with C"
      },
      {
        "index": 9,
        "text": "Anonymous: Partha117 2Â years, 10Â months ago\nSelected Answer: C\nApp engine allows versioning"
      },
      {
        "index": 10,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: C\nANSWER C. On the App Engine Versions page of the GCP Console, route 100% of the traffic to the previous version.\nTo immediately revert to the prior version of an application in App Engine, you can route 100% of the traffic to the previous version. This can be done through the App Engine Versions page of the GCP Console by selecting the desired version and selecting \"Migrate traffic\" and moving the slider to 100%. This will ensure that all traffic is directed to the prior version until the bug is fixed and the new version can be safely redeployed.\nhttps://cloud.google.com/appengine/docs/flexible/migrating-traffic\nANSWER A (Run gcloud app restore) and ANSWER B (Click Revert on GCP Console) are not valid actions to revert to the prior version of the application. ANSWER D (Deploy the original version as a separate application) is not necessary and would complicate the environment by requiring a split traffic configuration."
      }
    ]
  },
  {
    "id": 272,
    "source": "examtopics",
    "question": "You deployed an App Engine application using gcloud app deploy, but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do?",
    "options": {
      "A": "Check the app.yaml file for your application and check project settings.",
      "B": "Check the web-application.xml file for your application and check project settings.",
      "C": "Go to Deployment Manager and review settings for deployment of applications.",
      "D": "Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Bharathy Highly Voted 5Â years, 10Â months ago\nI would opt option D : as it would help to check the config details and Option A is not correct, as app.yaml would have only the runtime and script to run parameters and not the Project details alejandrombc 4Â years, 3Â months ago\nWhy would you choose Cloud Shell if its not even mention on the question? (what if the person did the command on its own computer?, this would not work) zaxxon 4Â years, 3Â months ago\ngcloud app deploy means sdk csrazdan 3Â years, 6Â months ago\nRegardless if you use your computer or cloud shell, you have to use SDK for gcloud command-line interface. gcloud uses a configuration file which contains default project, region and zone details so that command line can omit these parameters and use default. Seleth 1Â year, 5Â months ago\nThe first line of the question says: \"You deployed an App Engine application using gcloud app deploy\""
      },
      {
        "index": 2,
        "text": "Anonymous: ahmed812 Highly Voted 5Â years, 9Â months ago\nOption D - The config list will give the name of the project\nC:\\GCP\\appeng>gcloud config list\n[core]\naccount = xxx@gmail.com\ndisable_usage_reporting = False\nproject = my-first-demo-xxxx"
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nD is the correct answer."
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nSelected Answer: A\nA\napp.yaml will have the project details. JackSkeletonCoder 1Â year, 4Â months ago\nnope, app.yaml has only runtime and script to run the parameters"
      },
      {
        "index": 5,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: D\nD is correct because gcloud config list will give you the current project name & rest all options talks about examining the yaml file which is not a best practice"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\noption d as it will give you full information why it dont get deployed to the intended project"
      },
      {
        "index": 7,
        "text": "Anonymous: Nxt_007 2Â years, 5Â months ago\nSelected Answer: D\nOption D is the appropriate choice for diagnosing why the App Engine application did not deploy to the intended project. By running gcloud config list in Cloud Shell, you can view the current configuration settings, including the project ID, region, and other relevant settings used for deployment.\nOptions A and B involve checking the configuration files for the application (app.yaml and web-application.xml), but they may not directly provide information about where the application deployed or why it didn't deploy to the intended project.\nOption C involves Deployment Manager, which is a tool for creating, deploying, and managing resources in Google Cloud Platform, but it's not specifically related to App Engine deployments and may not provide the necessary insights in this context."
      },
      {
        "index": 8,
        "text": "Anonymous: Vamshi_Krishna 2Â years, 8Â months ago\nSelected Answer: D\nD is CORRECT"
      },
      {
        "index": 9,
        "text": "Anonymous: Zahir1004 2Â years, 10Â months ago\nSelected Answer: D\nI VOTE FOR D"
      },
      {
        "index": 10,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: D\nANSWER D. CORRECT. Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.\nRunning gcloud config list in the Cloud Shell will show the currently active configuration that was used for the deployment. This can help identify if the wrong project was selected or if the configuration was set up incorrectly.\nhttps://cloud.google.com/sdk/gcloud/reference/config/list\nANSWER A may be helpful to ensure that the project and deployment settings are correctly specified, but it does not provide information on where the application was actually deployed.\nANSWER B is not relevant for App Engine deployments as this is an XML configuration file typically used in Java web applications deployed to servlet containers.\nANSWER C is also not relevant for App Engine deployments, as Deployment Manager is typically used to create and manage deployments of cloud infrastructure resources such as virtual machines, load balancers, and databases."
      }
    ]
  },
  {
    "id": 273,
    "source": "examtopics",
    "question": "You want to configure 10 Compute Engine instances for availability when maintenance occurs. Your requirements state that these instances should attempt to automatically restart if they crash. Also, the instances should be highly available including during system maintenance. What should you do?",
    "options": {
      "A": "Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.",
      "B": "Create an instance template for the instances. Set 'Automatic Restart' to off. Set 'On-host maintenance' to Terminate VM instances. Add the instance template to an instance group.",
      "C": "Create an instance group for the instances. Set the 'Autohealing' health check to healthy (HTTP).",
      "D": "Create an instance group for the instance. Verify that the 'Advanced creation options' setting for 'do not retry machine creation' is set to off."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: lio123 Highly Voted 4Â years, 10Â months ago\nA\nhttps://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options\nonHostMaintenance: Determines the behavior when a maintenance event occurs that might cause your instance to reboot.\n[Default] MIGRATE, which causes Compute Engine to live migrate an instance when there is a maintenance event.\nTERMINATE, which stops an instance instead of migrating it.\nautomaticRestart: Determines the behavior when an instance crashes or is stopped by the system.\n[Default] true, so Compute Engine restarts an instance if the instance crashes or is stopped.\nfalse, so Compute Engine does not restart an instance if the instance crashes or is stopped."
      },
      {
        "index": 2,
        "text": "Anonymous: Imdeepak12 Highly Voted 4Â years, 3Â months ago\nSeems like it was a very obvious option i.e. A...Who selected B, I want to know his/her location? Kickbuttowski_ 4Â years, 2Â months ago\nNikki singh. aldrinzee 3Â years, 1Â month ago\nlol, yeah i want to examine their brain as well"
      },
      {
        "index": 3,
        "text": "Anonymous: iooj Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nYou don't even need to set anything because 'Automatic Restart' = 'On' and 'On-host maintenance' = 'Migrate' will be set BY DEFAULT!"
      },
      {
        "index": 4,
        "text": "Anonymous: DavMllt 1Â year, 5Â months ago\nB IS correct since if you set automatic restart to on , then your instance would be shut down during maintenance event, which Cancels the migrate on maintenance event setting that IS required for availibility purpose."
      },
      {
        "index": 5,
        "text": "Anonymous: VJ26 1Â year, 6Â months ago\nSelected Answer: A\nA looks obvious. B doesnt sound correct"
      },
      {
        "index": 6,
        "text": "Anonymous: pythonigger 2Â years ago\nsimple...\nAutomatic Restart ON vs OFF (obvious ON),\nOn Host Maintainence MIGRATE vs TERMINATE (really??!)"
      },
      {
        "index": 7,
        "text": "Anonymous: junkyaard 2Â years ago\nA is correct because automatic restart will restart the instance if it crashes and setting on host maintenance to migrate the instance will not let the application go down during maintenance. It fulfills the requirements of automatically restarting the instances if they crash and ensuring that they are not lost during system maintenance activity. By setting the 'Automatic Restart' to on, the instances will attempt to automatically restart if they crash. By setting the 'On-host maintenance' to Migrate VM instance, the instances will be migrated to another host during system maintenance, preventing any downtime."
      },
      {
        "index": 8,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: A\nA is correct because you need HA of VMs during mainetence"
      },
      {
        "index": 9,
        "text": "Anonymous: nik005 2Â years, 3Â months ago\nSelected Answer: A\n1\nThe best answer is A. Create an instance template for the instances. Set the 'Automatic Restart' to on. Set the 'On-host maintenance' to Migrate VM instance. Add the instance template to an instance group.\nThis will ensure that your instances are automatically restarted if they crash and that they are migrated during system maintenance, which will keep them highly available.\nThe other options are not as effective:\nOption B is not as effective because it will prevent your instances from being automatically restarted if they crash.\nOption C is not as effective because it will not migrate your instances during system maintenance, which could lead to downtime.\nOption D is not as effective because it does not guarantee that your instances will be automatically migrated during system maintenance."
      },
      {
        "index": 10,
        "text": "Anonymous: nnecode 2Â years, 4Â months ago\nSelected Answer: A\nI choose A"
      }
    ]
  },
  {
    "id": 274,
    "source": "examtopics",
    "question": "You host a static website on Cloud Storage. Recently, you began to include links to PDF files on this site. Currently, when users click on the links to these PDF files, their browsers prompt them to save the file onto their local system. Instead, you want the clicked PDF files to be displayed within the browser window directly, without prompting the user to save the file locally. What should you do?",
    "options": {
      "A": "Enable Cloud CDN on the website frontend.",
      "B": "Enable 'Share publicly' on the PDF file objects.",
      "C": "Set Content-Type metadata to application/pdf on the PDF file objects.",
      "D": "Add a label to the storage bucket with a key of Content-Type and value of application/pdf."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 2Â years, 5Â months ago\nSelected Answer: C\nANSWER A, enabling Cloud CDN on the website frontend, is not relevant to displaying PDF files in the browser. Cloud CDN is a content delivery network that caches content at edge locations around the world to reduce latency and improve website performance.\nANSWER B, enabling \"Share publicly\" on the PDF file objects, only controls whether or not the files are accessible to users without authentication. It does not affect how the files are displayed in the browser.\nANSWER D, adding a label to the storage bucket with a key of Content-Type and value of application/pdf, is not the correct way to set the Content-Type metadata for individual objects. Labels are used for organizing resources, while metadata is used to provide information about the data itself.\nTherefore, ANSWER C, setting Content-Type metadata to application/pdf on the PDF file objects, is the correct answer. dnur 2Â years, 4Â months ago\nMany thanks for clear explanations! :)"
      },
      {
        "index": 2,
        "text": "Anonymous: berezinsn Highly Voted 5Â years, 1Â month ago\nC is correct"
      },
      {
        "index": 3,
        "text": "Anonymous: Dinya_jui Most Recent 1Â year, 6Â months ago\nC makes much sense out of the remaining"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nanswer is c , as other are not relevant"
      },
      {
        "index": 5,
        "text": "Anonymous: YomanB 1Â year, 10Â months ago\nC. Set Content-Type metadata to application/pdf on the PDF file objects.\nExplanation: The Content-Type metadata indicates the media type of the content and helps the browser understand how to handle the file. In this case, by setting the Content-Type metadata of the PDF files to \"application/pdf,\" you're informing the browser that the files are in PDF format, and the browser will attempt to display them directly within the browser window, rather than prompting the user to download them."
      },
      {
        "index": 6,
        "text": "Anonymous: Nxt_007 1Â year, 11Â months ago\nSelected Answer: C\nC is correct\nSetting the Content-Type metadata to application/pdf on the PDF file objects instructs the web browser to treat these files as PDF documents and display them inline, rather than prompting the user to download them."
      },
      {
        "index": 7,
        "text": "Anonymous: Paulo_Jorge 2Â years, 7Â months ago\nOption C:\nTo display PDF files directly within the browser window on a website hosted on Cloud Storage, you can follow these steps:\nIn the Google Cloud Console, navigate to the Cloud Storage section and select the \"Buckets\" page.\nSelect the bucket that contains the static website and the PDF files.\nFrom the \"Actions\" menu, select \"Edit bucket\" and then go to the \"Website\" tab.\nIn the \"Website Configuration\" section, select the \"Serve objects with this content type\" option and enter \"application/pdf\" in the text field. This will cause PDF files to be served with the correct content type.\nSave the changes to the bucket configuration.\nAfter completing these steps, the PDF files on your website will be served with the correct content type and will be displayed directly within the browser window when clicked, without prompting the user to save the file locally."
      },
      {
        "index": 8,
        "text": "Anonymous: leogor 2Â years, 9Â months ago\nSelected Answer: C\nC. Set Content-Type metadata to application/pdf"
      },
      {
        "index": 9,
        "text": "Anonymous: Untamables 2Â years, 9Â months ago\nSelected Answer: C\nFYI\nImportance of setting the correct MIME type\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_Types#importance_of_setting_the_correct_mime_type"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years ago\nC is correct. Edit the PDF objects in Cloud Storage and reconfigure their Content-Type metadata into application/pdf."
      }
    ]
  },
  {
    "id": 275,
    "source": "examtopics",
    "question": "You have a virtual machine that is currently configured with 2 vCPUs and 4 GB of memory. It is running out of memory. You want to upgrade the virtual machine to have 8 GB of memory. What should you do?",
    "options": {
      "A": "Rely on live migration to move the workload to a machine with more memory.",
      "B": "Use gcloud to add metadata to the VM. Set the key to required-memory-size and the value to 8 GB.",
      "C": "Stop the VM, change the machine type to n1-standard-8, and start the VM.",
      "D": "Stop the VM, increase the memory to 8 GB, and start the VM."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cesar7816 Highly Voted 5Â years, 10Â months ago\ncoldpar, why are you getting the people confused? you need to stop teh VM and modify the RAM, that's all iambatmanadarkknight 4Â years, 3Â months ago\nwho is coldpar spatidar2711 3Â years, 6Â months ago\nHe deleted his comment chikorita 2Â years, 9Â months ago\ni dont think we can delete our comments"
      },
      {
        "index": 2,
        "text": "Anonymous: CarlS Highly Voted 5Â years, 9Â months ago\nD is correct. If you pay attention to the question, option C mentions n1-standard-8. That instance type has 8vCPUs and 30 GB RAM, and we only need 8GB. On top of that, it is possible to use custom machine type to adjust current VM RAM to the value we need. Got the answer from this course I did to prepare the exam: https://www.udemy.com/course/google-cloud-associate-engineer-exam-practice-tests/?couponCode=21CDE6A4C2B95F79BD97\ngood luck! Veera_Venkata_Satyanarayana 3Â years, 6Â months ago\nHow to use coupon code carls"
      },
      {
        "index": 3,
        "text": "Anonymous: josecouva Most Recent 1Â month, 1Â week ago\nSelected Answer: C\nThe question doesn't specify whether the instance type is predefined or custom. If it's predefined, the answer is C, and if it's custom, the answer is D. In my opinion, since it's not specified, the answer is C because Google Cloud creates instances linked to a machine type. This type specifies the CPUs and memory."
      },
      {
        "index": 4,
        "text": "Anonymous: warbon 12Â months ago\nSelected Answer: C\nIn Google Cloud, you cannot directly modify the memory allocation for an existing VM. Instead, you must change the VM's machine type to one with the desired memory and vCPU configuration (e.g., n1-standard-8, which has 8 GB of memory). The VM needs to be stopped before making this change, and then it can be restarted.\nWhy not other options?\nA (Live migration): Live migration moves workloads during maintenance but does not change the VM's memory allocation.\nB (Metadata change): Adding metadata does not affect memory allocation.\nD (Directly increasing memory): You cannot manually increase only the memory of a VM; you must select a predefined machine type."
      },
      {
        "index": 5,
        "text": "Anonymous: martin2099 1Â year, 2Â months ago\nC. Esto implica detener la mÃ¡quina virtual, cambiar el tipo de mÃ¡quina a uno que tenga 8 GB de memoria (como n1-standard-8), y luego reiniciarla."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD is the correct answer"
      },
      {
        "index": 7,
        "text": "Anonymous: gpais 2Â years, 5Â months ago\nYou can add extended memory only to custom machine types. Predefined machine types are not supported."
      },
      {
        "index": 8,
        "text": "Anonymous: samrat46 2Â years, 9Â months ago\nD is correct.\nC.n1 standard8 has 30GB RAM.\nA&B- No vm instance stop, Hence can't be updated."
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: D\nANSWER D is correct because it is the correct process to follow to increase the memory of a virtual machine in the Google Cloud Platform.\nTo increase the memory of a virtual machine, you need to first stop the VM, since it is not possible to modify the memory of a running VM. Then, you can increase the memory of the VM by editing the machine type and selecting a machine type with more memory. Once you have made the change, you can start the VM again.\nANSWER A is not the best approach as it relies on live migration which can be a risky operation.\nANSWER B is incorrect because adding metadata to the VM will not change the amount of memory allocated to the VM.\nANSWER C is incorrect because changing the machine type to n1-standard-8 would also increase the number of vCPUs to 8, which may not be necessary and could result in overprovisioning of resources. In addition, changing the machine type would also affect the cost of the VM instance, which may not be desired. Since the primary concern in this scenario is to increase memory."
      },
      {
        "index": 10,
        "text": "Anonymous: cslince 3Â years, 1Â month ago\nSelected Answer: D\nOption D"
      }
    ]
  },
  {
    "id": 276,
    "source": "examtopics",
    "question": "You have production and test workloads that you want to deploy on Compute Engine. Production VMs need to be in a different subnet than the test VMs. All the\nVMs must be able to reach each other over Internal IP without creating additional routes. You need to set up VPC and the 2 subnets. Which configuration meets these requirements?",
    "options": {
      "A": "Create a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range.",
      "B": "Create a single custom VPC with 2 subnets. Create each subnet in the same region and with the same CIDR range.",
      "C": "Create 2 custom VPCs, each with a single subnet. Create each subnet in a different region and with a different CIDR range.",
      "D": "Create 2 custom VPCs, each with a single subnet. Create each subnet in the same region and with the same CIDR range."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JamesBond Highly Voted 5Â years, 10Â months ago\nA is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: nwk Highly Voted 5Â years, 2Â months ago\nVote A\nhttps://cloud.google.com/vpc/docs/using-vpc#subnet-rules\nPrimary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks."
      },
      {
        "index": 3,
        "text": "Anonymous: Ciupaz Most Recent 1Â year, 2Â months ago\nSelected Answer: A\nB is wrong because is not possible to have two subnets with the same CIDR range within a VPC, as it would cause an IP conflict."
      },
      {
        "index": 4,
        "text": "Anonymous: jayflesher 1Â year, 2Â months ago\nThis question and answer doesn't add up to me.\nWhy would you need to have a different CIDR range and a different region?\nFor example, if I had 10.0.1.0/24 in us-central1 region, why can't i just use 10.0.2.0/24? in the same region and \"/24\" is the same CIDR range. The correct answer does not make logical\nsense to me. sh4dw4rri0r 11Â months, 4Â weeks ago\nGood perspective. Theoretically it should be possible.\nBut CIDR Range includes both IP Address and Subnet Mask.\nYou are saying that \"/24\" is the same CIDR range. (but you are referring to subnet mask)\nBut 10.0.1.0/24 and 10.0.2.0/24 are different CIDR Range if compared.\nHence option A is relevant, Different CIDR nsabir 1Â year ago\nYou can do this. But this option isn't available. The correct option just happens to create the subnets in different regions. That's all."
      },
      {
        "index": 5,
        "text": "Anonymous: fighter001 1Â year, 8Â months ago\nWrong question with wrong answer"
      },
      {
        "index": 6,
        "text": "Anonymous: ranjitsinhgutte 2Â years, 3Â months ago\nA is correct\nIf you create more than one subnet in a VPC, the CIDR blocks of the subnets cannot overlap. For example, if you create a VPC with CIDR block 10.0. 0.0/24 , it supports 256 IP addresses. You can break this CIDR block into two subnets, each supporting 128 IP addresses."
      },
      {
        "index": 7,
        "text": "Anonymous: certboss 2Â years, 4Â months ago\nFor anyone new to the business, prod and test networks should never talk to each other.... The requirement in this question (that both envs can reach each other) is completely against best practice and common sense... There should always be complete network isolation between prod and non-prod environments. iooj 1Â year, 4Â months ago\nin reality, business leads will push you to make such a connection, for example, because the test environment doesn't have enough data for testing..."
      },
      {
        "index": 8,
        "text": "Anonymous: fraiacca 2Â years, 4Â months ago\nSelected Answer: A\nI tried to create a VPC with 2 subnets in same regione and same CIDR\nI got the following error\nOperation type [insert] failed with message \"Invalid IPCidrRange: 10.0.0.0/28 conflicts with existing subnetwork 'subnet-1' in region 'asia-east1'.\" Seleth 1Â year, 5Â months ago\n\"same CIDR\" means the same range of addresses. You cannot have two networks overlapping anywhere because the IPs will conflict."
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nA is correct as it help to make sure they have a diffenret subnets"
      },
      {
        "index": 10,
        "text": "Anonymous: raselsys 2Â years, 10Â months ago\nSelected Answer: A\nA is the correct Answer. People voting for B need to improve their networking knowledge."
      }
    ]
  },
  {
    "id": 277,
    "source": "examtopics",
    "question": "You need to create an autoscaling managed instance group for an HTTPS web application. You want to make sure that unhealthy VMs are recreated. What should you do?",
    "options": {
      "A": "Create a health check on port 443 and use that when creating the Managed Instance Group.",
      "B": "Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.",
      "C": "In the Instance Template, add the label 'health-check'.",
      "D": "In the Instance Template, add a startup script that sends a heartbeat to the metadata server."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cesar7816 Highly Voted 5Â years, 10Â months ago\nI'll go with A, MIGs support autohealing, load balancing, autoscaling, and auto-updating. no the Images templates, this is set up in the MIG"
      },
      {
        "index": 2,
        "text": "Anonymous: tanito83 Highly Voted 4Â years, 7Â months ago\nThe correct answer is A. Please, modify it."
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nSelected Answer: A\nA is the correct answer."
      },
      {
        "index": 4,
        "text": "Anonymous: XNap 1Â year, 10Â months ago\nSelected Answer: A\nThe correct answer is A. Create a health check on port 443 and use that when creating the Managed Instance Group.\nOption B is related to the availability and distribution of instances across multiple zones, but it does not directly address the requirement of recreating unhealthy VMs.\nOption C is incorrect because labels are not used for configuring health checks in GCP.\nOption D is an alternative method for health checking, but it is not as straightforward as using the built-in health check functionality provided by GCP for managed instance groups."
      },
      {
        "index": 5,
        "text": "Anonymous: fraiacca 2Â years, 4Â months ago\nSelected Answer: A\nOnly A answer has some sense"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\n443 means http A seems more correct"
      },
      {
        "index": 7,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nC is incomplete. A all the way"
      },
      {
        "index": 8,
        "text": "Anonymous: Nxt_007 2Â years, 5Â months ago\nSelected Answer: A\nOption A is the correct choice because it involves creating a health check specifically on port 443, which is the standard port for HTTPS traffic. This health check will monitor the health of the instances based on their ability to respond to HTTPS requests. When creating the Managed Instance Group, you would configure it to use this health check to determine the health of the instances\nOptions B, C, and D are not directly related to setting up proper health checks for autoscaling and ensuring unhealthy instances are recreated"
      },
      {
        "index": 9,
        "text": "Anonymous: Backlander 2Â years, 7Â months ago\nA for A-Game let's goooo!"
      },
      {
        "index": 10,
        "text": "Anonymous: Vamshi_Krishna 2Â years, 8Â months ago\nSelected Answer: A\nC is definitely incorrect. Adding a label does not recreate unhealthy VMs.\nA is CORRECT."
      }
    ]
  },
  {
    "id": 278,
    "source": "examtopics",
    "question": "Your company has a Google Cloud Platform project that uses BigQuery for data warehousing. Your data science team changes frequently and has few members.\nYou need to allow members of this team to perform queries. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "1. Create an IAM entry for each data scientist's user account. 2. Assign the BigQuery jobUser role to the group.",
      "B": "1. Create an IAM entry for each data scientist's user account. 2. Assign the BigQuery dataViewer user role to the group.",
      "C": "1. Create a dedicated Google group in Cloud Identity. 2. Add each data scientist's user account to the group. 3. Assign the BigQuery jobUser role to the group.",
      "D": "1. Create a dedicated Google group in Cloud Identity. 2. Add each data scientist's user account to the group. 3. Assign the BigQuery dataViewer user role to the group."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cesar7816 Highly Voted 5Â years, 10Â months ago I'll go with A, MIGs support autohealing, load balancing, autoscaling, and auto-updating. no the Images templates, this is set up in the MIG ..."
      },
      {
        "index": 2,
        "text": "Anonymous: tanito83 Highly Voted 4Â years, 7Â months ago The correct answer is A. Please, modify it. ..."
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago Selected Answer: A A is the correct answer. ..."
      },
      {
        "index": 4,
        "text": "Anonymous: XNap 1Â year, 10Â months ago Selected Answer: A The correct answer is A. Create a health check on port 443 and use that when creating the Managed Instance Group.\nOption B is related to the availability and distribution of instances across multiple zones, but it does not directly address the requirement of recreating unhealthy VMs.\nOption C is incorrect because labels are not used for configuring health checks in GCP.\nOption D is an alternative method for health checking, but it is not as straightforward as using the built-in health check functionality provided by GCP for managed instance groups. ..."
      },
      {
        "index": 5,
        "text": "Anonymous: fraiacca 2Â years, 4Â months ago Selected Answer: A Only A answer has some sense ..."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago Selected Answer: A 443 means http A seems more correct ..."
      },
      {
        "index": 7,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago C is incomplete. A all the way ..."
      },
      {
        "index": 8,
        "text": "Anonymous: Nxt_007 2Â years, 5Â months ago Selected Answer: A Option A is the correct choice because it involves creating a health check specifically on port 443, which is the standard port for HTTPS traffic. This health check will monitor the health of the instances based on their ability to respond to HTTPS requests. When creating the Managed Instance Group, you would configure it to use this health check to determine the health of the instances\nOptions B, C, and D are not directly related to setting up proper health checks for autoscaling and ensuring unhealthy instances are recreated ..."
      },
      {
        "index": 9,
        "text": "Anonymous: Backlander 2Â years, 7Â months ago A for A-Game let's goooo! ..."
      },
      {
        "index": 10,
        "text": "Anonymous: Vamshi_Krishna 2Â years, 8Â months ago Selected Answer: A C is definitely incorrect. Adding a label does not recreate unhealthy VMs.\nA is CORRECT. ..."
      }
    ]
  },
  {
    "id": 279,
    "source": "examtopics",
    "question": "Your company has a 3-tier solution running on Compute Engine. The configuration of the current infrastructure is shown below.\n\nEach tier has a service account that is associated with all instances within it. You need to enable communication on TCP port 8080 between tiers as follows:\n* Instances in tier #1 must communicate with tier #2.\n* Instances in tier #2 must communicate with tier #3.\nWhat should you do?",
    "options": {
      "A": "1. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances ×’â‚¬Â¢ Source filter: IP ranges (with the range set to 10.0.2.0/24) ×’â‚¬Â¢ Protocols: allow all 2. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances ×’â‚¬Â¢ Source filter: IP ranges (with the range set to 10.0.1.0/24) ×’â‚¬Â¢ Protocols: allow all",
      "B": "1. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances with tier #2 service account ×’â‚¬Â¢ Source filter: all instances with tier #1 service account ×’â‚¬Â¢ Protocols: allow TCP:8080 2. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances with tier #3 service account ×’â‚¬Â¢ Source filter: all instances with tier #2 service account ×’â‚¬Â¢ Protocols: allow TCP: 8080",
      "C": "1. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances with tier #2 service account ×’â‚¬Â¢ Source filter: all instances with tier #1 service account ×’â‚¬Â¢ Protocols: allow all 2. Create an ingress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances with tier #3 service account ×’â‚¬Â¢ Source filter: all instances with tier #2 service account ×’â‚¬Â¢ Protocols: allow all",
      "D": "1. Create an egress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances ×’â‚¬Â¢ Source filter: IP ranges (with the range set to 10.0.2.0/24) ×’â‚¬Â¢ Protocols: allow TCP: 8080 2. Create an egress firewall rule with the following settings: ×’â‚¬Â¢ Targets: all instances ×’â‚¬Â¢ Source filter: IP ranges (with the range set to 10.0.1.0/24) ×’â‚¬Â¢ Protocols: allow TCP: 8080"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: obeythefist Highly Voted 3 years, 4 months ago This question is designed to waste your time during the exam by forcing you to read long answers. Part of exam technique is understanding multiple-choice patterns, not just knowing the product. When two answers are very similar, the correct answer is often one of them. Here, all answers look similar, so we eliminate the wrong ones. Two answers mention opening all ports, while two mention port 8080. Since only port 8080 is required, we eliminate the options that open all ports. That leaves two choices: one using ingress and one using egress. Since egress traffic is allowed by default and ingress is not, the correct answer must be the ingress rule. Therefore, option B is the only valid choice. Upvoted 101 times."
      },
      {
        "index": 2,
        "text": "Anonymous: kopper2019 Highly Voted 4 years, 3 months ago If you look closely, port 8080 and service accounts are required. So B is the answer without needing to read all options. Upvoted 27 times."
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1 year, 10 months ago Selected Answer: B B is the correct answer. Upvoted 1 time."
      },
      {
        "index": 4,
        "text": "Anonymous: Buruguduystunstugudunstuy 2 years, 5 months ago Selected Answer: B Answer B is correct because it creates ingress firewall rules that allow communication between instances in different tiers on TCP port 8080, based on service accounts. The first rule allows Tier 1 to communicate with Tier 2, and the second allows Tier 2 to communicate with Tier 3. This ensures only the correct instances can communicate. Answer A is incorrect because subnet-based rules allow unintended instances to communicate. Answer C is incorrect because it allows all protocols, which is insecure. Answer D is incorrect because it creates egress rules, while ingress rules are required. Upvoted 5 times."
      },
      {
        "index": 5,
        "text": "Anonymous: cslince 2 years, 7 months ago Selected Answer: B B is the correct answer. Upvoted 1 time."
      },
      {
        "index": 6,
        "text": "Anonymous: leogor 2 years, 9 months ago Selected Answer: B B is correct obviously. Upvoted 1 time."
      },
      {
        "index": 7,
        "text": "Anonymous: abirroy 2 years, 11 months ago Selected Answer: B B is the correct answer. Upvoted 1 time."
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3 years ago B is right. We need firewall rules that allow port 8080 and it should not be wide open like a /24 network. Upvoted 2 times."
      },
      {
        "index": 9,
        "text": "Anonymous: haroldbenites 3 years, 1 month ago Go for B. Upvoted 1 time."
      },
      {
        "index": 10,
        "text": "Anonymous: Jerickson 3 years, 5 months ago Selected Answer: B B is correct."
      }
    ]
  },
  {
    "id": 280,
    "source": "examtopics",
    "question": "You are given a project with a single Virtual Private Cloud (VPC) and a single subnetwork in the us-central1 region. There is a Compute Engine instance hosting an application in this subnetwork. You need to deploy a new instance in the same project in the europe-west1 region. This new instance needs access to the application. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "1. Create a subnetwork in the same VPC, in europe-west1. 2. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.",
      "B": "1. Create a VPC and a subnetwork in europe-west1. 2. Expose the application with an internal load balancer. 3. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint.",
      "C": "1. Create a subnetwork in the same VPC, in europe-west1. 2. Use Cloud VPN to connect the two subnetworks. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.",
      "D": "1. Create a VPC and a subnetwork in europe-west1. 2. Peer the 2 VPCs. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cesar7816 Highly Voted 5Â years, 10Â months ago I'll go with A, MIGs support autohealing, load balancing, autoscaling, and auto-updating. no the Images templates, this is set up in the MIG ..."
      },
      {
        "index": 2,
        "text": "Anonymous: tanito83 Highly Voted 4Â years, 7Â months ago The correct answer is A. Please, modify it. ..."
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago Selected Answer: A A is the correct answer. ..."
      },
      {
        "index": 4,
        "text": "Anonymous: XNap 1Â year, 10Â months ago Selected Answer: A The correct answer is A. Create a health check on port 443 and use that when creating the Managed Instance Group.\nOption B is related to the availability and distribution of instances across multiple zones, but it does not directly address the requirement of recreating unhealthy VMs.\nOption C is incorrect because labels are not used for configuring health checks in GCP.\nOption D is an alternative method for health checking, but it is not as straightforward as using the built-in health check functionality provided by GCP for managed instance groups. ..."
      },
      {
        "index": 5,
        "text": "Anonymous: fraiacca 2Â years, 4Â months ago Selected Answer: A Only A answer has some sense ..."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago Selected Answer: A 443 means http A seems more correct ..."
      },
      {
        "index": 7,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago C is incomplete. A all the way ..."
      },
      {
        "index": 8,
        "text": "Anonymous: Nxt_007 2Â years, 5Â months ago Selected Answer: A Option A is the correct choice because it involves creating a health check specifically on port 443, which is the standard port for HTTPS traffic. This health check will monitor the health of the instances based on their ability to respond to HTTPS requests. When creating the Managed Instance Group, you would configure it to use this health check to determine the health of the instances\nOptions B, C, and D are not directly related to setting up proper health checks for autoscaling and ensuring unhealthy instances are recreated ..."
      },
      {
        "index": 9,
        "text": "Anonymous: Backlander 2Â years, 7Â months ago A for A-Game let's goooo! ..."
      },
      {
        "index": 10,
        "text": "Anonymous: Vamshi_Krishna 2Â years, 8Â months ago Selected Answer: A C is definitely incorrect. Adding a label does not recreate unhealthy VMs.\nA is CORRECT. ..."
      }
    ]
  },
  {
    "id": 281,
    "source": "examtopics",
    "question": "Your projects incurred more costs than you expected last month. Your research reveals that a development GKE container emitted a huge number of logs, which resulted in higher costs. You want to disable the logs quickly using the minimum number of steps. What should you do?",
    "options": {
      "A": "1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource.",
      "B": "1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE Cluster Operations resource.",
      "C": "1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Logging.",
      "D": "1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Monitoring."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "author": "Anonymous: Gini",
        "meta": "Highly Voted â€¢ 5 years, 8 months ago",
        "text": "The question mentioned that \"GKE container emitted a huge number of logs\", in my opinion A is correct.",
        "upvotes": 53
      },
      {
        "index": 2,
        "author": "Anonymous: JackGlemins",
        "meta": "Highly Voted â€¢ 4 years, 11 months ago",
        "text": "I think A is right.\nhttps://cloud.google.com/logging/docs/api/v2/resource-list\n\nGKE Containers have more logs than GKE Cluster Operations:\n\nGKE Container:\n- cluster_name: An immutable name for the cluster the container is running in.\n- namespace_id: Immutable ID of the cluster namespace the container is running in.\n- instance_id: Immutable ID of the GCE instance the container is running in.\n- pod_id: Immutable ID of the pod the container is running in.\n- container_name: Immutable name of the container.\n- zone: The GCE zone in which the instance is running.\n\nGKE Cluster Operations:\n- project_id: The identifier of the GCP project associated with this resource.\n- cluster_name: The name of the GKE Cluster.\n- location: The location in which the GKE Cluster is running.",
        "upvotes": 18
      },
      {
        "index": 3,
        "author": "Anonymous: Samii150406",
        "meta": "Most Recent â€¢ 1 year, 4 months ago",
        "selectedAnswer": "A",
        "text": "Your projects incurred more costs than expected due to a development GKE container emitting a huge number of logs. To disable the logs quickly using the minimum number of steps, the correct choice is A.\n\nA. Disable the log source for the GKE container resource in the Logs ingestion window.\nB. Disable the GKE Cluster Operations log source.\nC. Delete and recreate the cluster without legacy Stackdriver Logging.\nD. Delete and recreate the cluster without legacy Stackdriver Monitoring.",
        "upvotes": 1
      },
      {
        "index": 4,
        "author": "Anonymous: Captain1212",
        "meta": "2 years, 4 months ago",
        "selectedAnswer": "A",
        "text": "A seems more correct as it uses the fewest steps.",
        "upvotes": 3
      },
      {
        "index": 5,
        "author": "Anonymous: _Sande",
        "meta": "2 years, 9 months ago",
        "text": "Just as a side note: Stackdriver is now called Google Cloud Operations Suite.",
        "upvotes": 9
      },
      {
        "index": 6,
        "author": "Anonymous: Buruguduystunstugudunstuy",
        "meta": "2 years, 11 months ago",
        "selectedAnswer": "A",
        "text": "The correct answer is A. Disabling the GKE container log source directly reduces log volume and cost.\n\nB does not stop container logs.\nC and D are not recommended because deleting and recreating clusters is unnecessary and inefficient.",
        "upvotes": 8
      },
      {
        "index": 7,
        "author": "Anonymous: cslince",
        "meta": "3 years, 1 month ago",
        "selectedAnswer": "A",
        "text": "A is correct.",
        "upvotes": 1
      },
      {
        "index": 8,
        "author": "Anonymous: leogor",
        "meta": "3 years, 2 months ago",
        "selectedAnswer": "A",
        "text": "A can do it with the fewest steps.",
        "upvotes": 1
      },
      {
        "index": 9,
        "author": "Anonymous: Untamables",
        "meta": "3 years, 3 months ago",
        "selectedAnswer": "B",
        "text": "Currently B is correct. Stackdriver Logging has been renamed to Cloud Logging.\nhttps://cloud.google.com/stackdriver/docs/solutions/gke/installing#migrating",
        "upvotes": 1
      },
      {
        "index": 10,
        "author": "Anonymous: abirroy",
        "meta": "3 years, 5 months ago",
        "selectedAnswer": "A",
        "text": "Go for A.",
        "upvotes": 1
      }
    ]
  },
  {
    "id": 282,
    "source": "examtopics",
    "question": "You have a website hosted on App Engine standard environment. You want 1% of your users to see a new test version of the website. You want to minimize complexity. What should you do?",
    "options": {
      "A": "Deploy the new version in the same application and use the --migrate option.",
      "B": "Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version.",
      "C": "Create a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.",
      "D": "Create a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: yasu Highly Voted 4Â years, 9Â months ago\nI will prefer B as the answer.. why we need create new application? YAS007 3Â years, 5Â months ago\nmore over, in app engine we cannot create \"new application\", we have to create a new Project to do that, an app engine projet has 1 application (which can have multiple versions and services) nmnm22 1Â year, 2Â months ago\n\"yasu\"...nana? sanhoo 3Â years, 7Â months ago\nAgree B is correct. creating a new application in the same project for app engine is anyways not possible."
      },
      {
        "index": 2,
        "text": "Anonymous: Gini Highly Voted 4Â years, 9Â months ago\nI agree with yasu. And only one app engine can exist in one project. B is the best choice, simple and easy."
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 4,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: B\nThe correct answer is B.\nBy using the App Engine's traffic splitting feature, we can easily direct a certain percentage of traffic to a specific version of our application. In this case, we want to send 1% of traffic to the new test version and keep the remaining 99% on the current version. This can be achieved by deploying the new version in the same application and using the `--splits` option to give a weight of 99 to the current version and a weight of 1 to the new version.\nAnswer A is incorrect because the `--migrate` option is used for migrating traffic to a new version after it has been fully tested and is ready for full deployment.\nAnswer C is incorrect because it requires additional configuration to proxy requests to the new version, increasing complexity unnecessarily.\nAnswer D is incorrect because it involves configuring a network load balancer, which is not necessary for this use case and adds unnecessary complexity. Jelly_Wang 1Â year, 8Â months ago\nWhile I agree with your choice and your explanation of B. I also believe C and D are wrong simply because you can only have one App Engine within a project https://cloud.google.com/appengine/docs/flexible/managing-projects-apps-billing#:~:text=Important%3A%20Each%20Cloud%20project%20can,of%20your%20App%20Engine%20application."
      },
      {
        "index": 5,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nSelected Answer: B\nB. deploy new version with --splits option"
      },
      {
        "index": 7,
        "text": "Anonymous: Cornholio_LMC 2Â years, 3Â months ago\nhad this question today"
      },
      {
        "index": 8,
        "text": "Anonymous: habros 2Â years, 5Â months ago\nB! A very natural answerâ€¦ Perfect for switching users over to new version. Imagine creating multiple projects to update App Engine deployments, isnâ€™t that logically unnecessary?"
      },
      {
        "index": 9,
        "text": "Anonymous: Madj 2Â years, 6Â months ago\nHint:\nOne app engine per project. So Option C,D eliminated. this hint will help in many similar questions.\nSplitting traffic hint will help as well"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nB is right."
      }
    ]
  },
  {
    "id": 283,
    "source": "examtopics",
    "question": "You have a web application deployed as a managed instance group. You have a new version of the application to gradually deploy. Your web application is currently receiving live web traffic. You want to ensure that the available capacity does not decrease during the deployment. What should you do?",
    "options": {
      "A": "Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.",
      "B": "Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0.",
      "C": "Create a new managed instance group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new managed instance group are healthy, delete the old managed instance group.",
      "D": "Create a new instance template with the new application version. Update the existing managed instance group with the new instance template. Delete the instances in the managed instance group to allow the managed instance group to recreate the instance using the new instance template."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: CarlS Highly Voted 5Â years, 3Â months ago\nCorrect option is B. We need to ensure the global capacity remains intact, for that reason we need to establish maxUnavailable to 0. On the other hand, we need to ensure new instances can be created. We do that by establishing the maxSurge to 1. Option C is more expensive and more difficult to set up and option D won't meet requirements since it won't keep global capacity intact. yeanlingmedal71 2Â years, 7Â months ago\nmaxSurge- configure how many new instances the MIG can create above its targetSize during an automated update. For example, if you set maxSurge to 5, the MIG uses the new instance template to create up to 5 new instances above your target size. Setting a higher maxSurge value speeds up your update, at the cost of additional instances space_cadet 2Â years, 4Â months ago\nThanks for this.\nAnd setting it to one makes sense, seeing that we want a gradual update"
      },
      {
        "index": 2,
        "text": "Anonymous: JavierCorrea Highly Voted 4Â years, 10Â months ago\nI take my own previous comment back. It's definitely B."
      },
      {
        "index": 3,
        "text": "Anonymous: PrivateHulk Most Recent 1Â year, 5Â months ago\nI vote for A.\nA rolling update with maxSurge set to 0 ensures that no additional instances beyond the desired size are created during the update.\nBy setting maxUnavailable to 1, only one instance is taken down at a time, minimizing the impact on the available capacity during the deployment.\nThis approach allows the rolling deployment to proceed in a controlled manner, with each new version gradually replacing instances without decreasing the overall capacity and with zero downtime.\nNot Option B\nbecause setting maxSurge to 1 and maxUnavailable to 0, could lead to temporarily increased capacity during the update, and it might result in higher resource usage than necessary. This option may not guarantee zero downtime or minimize the impact on the available capacity during the deployment."
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 1Â year, 8Â months ago\nSelected Answer: B\nB is the clean way: https://medium.com/@bubu.tripathy/understanding-maxsurge-and-maxunavailable-4966dfafc8ba"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: B\nAnswer B is the correct answer because it allows for a safe and controlled rolling deployment with zero downtime and without reducing the available capacity during the deployment.\nThe `maxSurge` parameter controls the maximum number of new instances that can be created above the desired number of instances during the update process. By setting `maxSurge` to 1, the new version of the application can be gradually rolled out while maintaining the same number of available instances.\nThe `maxUnavailable` parameter controls the maximum number of instances that can be unavailable during the update process. By setting `maxUnavailable` to 0, at least one instance of the previous version will be available at all times, ensuring that there is no decrease in available capacity during the deployment.\nBy performing a rolling update with `maxSurge` set to 1 and `maxUnavailable` set to 0, the new version of the application can be gradually deployed with zero downtime and no decrease in available capacity. Buruguduystunstugudunstuy 2Â years, 5Â months ago\nAnswer A is incorrect because setting maxSurge to 0 means that no additional instances are created beyond the existing number of instances in the group, which can potentially lead to a decrease in capacity. Also, setting maxUnavailable to 1 means that one instance can be unavailable at any given time, which can potentially lead to some users experiencing downtime.\nAnswer C is incorrect because creating a new managed instance group would require adding the new group to the backend service, which can take time and potentially cause downtime. Also, deleting the old managed instance group before ensuring that the new group is healthy can cause a decrease in capacity.\nAnswer D is incorrect because deleting instances in the managed instance group can cause a temporary decrease in capacity, and it may take some time for new instances to be created with the new instance template. Also, the new instances may take time to warm up, which can cause a delay in serving traffic."
      },
      {
        "index": 7,
        "text": "Anonymous: vlodia 2Â years, 6Â months ago\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.\nhttps://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable"
      },
      {
        "index": 8,
        "text": "Anonymous: rajivdutt 2Â years, 6Â months ago\nIf you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0. With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running."
      },
      {
        "index": 9,
        "text": "Anonymous: Mission94 2Â years, 7Â months ago\nHI all,\nif you guys have all the questions and answers please mail it to\nuntranslatable[dot]character@gmail[dot]com\nThanks in advance."
      },
      {
        "index": 10,
        "text": "Anonymous: Rubankumar 2Â years, 7Â months ago\nSelected Answer: B\nB is Correct"
      }
    ]
  },
  {
    "id": 284,
    "source": "examtopics",
    "question": "You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use?",
    "options": {
      "A": "Cloud SQL",
      "B": "Cloud Spanner",
      "C": "Cloud Firestore",
      "D": "Cloud Datastore"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Fidget_ Highly Voted 5Â years, 5Â months ago\nB\nCloud SQL for small relational data, scaled manually\nCloud Spanner for relational data, scaled automatically\nCloud Firestore for app-based data(?)\nCloud Datastore for non-relational data\nCorrect me if i'm wrong theBestStudent 3Â years, 7Â months ago\nJust one detail: Cloud Firestore for non relational data (noSql) KC_go_reply 2Â years, 9Â months ago\n'small relational data' as in 3 TB for Shared core or 64 TB for Dedicated core in Cloud SQL"
      },
      {
        "index": 2,
        "text": "Anonymous: karol_wu Highly Voted 5Â years, 10Â months ago\nin my opinion correct is B"
      },
      {
        "index": 3,
        "text": "Anonymous: Mohammed52 Most Recent 1Â year, 7Â months ago\nSelected Answer: B\nCloud spanner is correct as it will scaled automatically"
      },
      {
        "index": 4,
        "text": "Anonymous: Murli1 2Â years, 1Â month ago\nB is Correct Ans. Cloud Spanner provides a scalable online transaction processing (OLTP) database with high availability and strong consistency at a global scale."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: B\nB, for large and automatically scaled"
      },
      {
        "index": 6,
        "text": "Anonymous: keton 2Â years, 8Â months ago\nCorrect ans is B... Focus on two words \"Relational\" which means option C & D has been eliminated bcz these are non-relational DB.And another word 'Globally' which means Option A also eliminated bcz Cloud Sql does not support global deployments."
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: B\nThe best storage solution for this scenario would be Cloud Spanner. Cloud Spanner is a fully managed, scalable, relational database that is designed to handle global deployments with ease. It can handle large amounts of data and high transactional volumes. It also provides automatic sharding and synchronous replication, ensuring high availability and durability of data. Cloud Spanner supports SQL semantics and provides a familiar relational database experience to developers, which would make it easy to adopt in existing workflows.\nCloud SQL, on the other hand, has limits on scalability and does not support global deployments as well as Cloud Spanner.\nCloud Firestore and Cloud Datastore are NoSQL databases that are better suited for document-based data storage and not optimized for relational data storage."
      },
      {
        "index": 8,
        "text": "Anonymous: cslince 3Â years, 1Â month ago\nSelected Answer: B\ncorrect is B"
      },
      {
        "index": 9,
        "text": "Anonymous: Tmitchelltec919 3Â years, 2Â months ago\ncould someone please explain why the answer is not A"
      },
      {
        "index": 10,
        "text": "Anonymous: leogor 3Â years, 2Â months ago\nSelected Answer: B\nB. Spanner for autoscale"
      }
    ]
  },
  {
    "id": 285,
    "source": "examtopics",
    "question": "You are the organization and billing administrator for your company. The engineering team has the Project Creator role on the organization. You do not want the engineering team to be able to link projects to the billing account. Only the finance team should be able to link a project to a billing account, but they should not be able to make any other changes to projects. What should you do?",
    "options": {
      "A": "Assign the finance team only the Billing Account User role on the billing account.",
      "B": "Assign the engineering team only the Billing Account User role on the billing account.",
      "C": "Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.",
      "D": "Assign the engineering team the Billing Account User role on the billing account and the Project Billing Manager role on the organization."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Bharathy Highly Voted 5Â years, 10Â months ago\nOption A is correct, as we don't want the engineering team to link projects to billing account and want only the Finance team. Billing Account User role will help to link projects to the billing account... Hasaaaan 4Â years, 7Â months ago\nBilling Account User also enables the user to make changes in resources. pspandher 3Â years, 6Â months ago\nBilling Account User Role when granted in combination with the Project Billing Manager role, the two roles allow a user to link and unlink projects on the billing account on which the Billing Account User role is granted mwwoodm 5Â years, 4Â months ago\nOption A makes the most sense since Billing Account User can link projects to the billing account and the question reinforces principle of least privilege. Source: https://cloud.google.com/billing/docs/how-to/billing-access Nikki2424 1Â year, 8Â months ago\nYes, but in combination with Project Billing Manager. Also these two roles won't grant rights on any other resources, which is also intended in the question. naveedpk00 5Â years, 5Â months ago\nOption A is incorrect: Reason-\nThis role has very restricted permissions, so you can grant it broadly, typically in combination with Project Creator. These two roles allow a user to create new projects linked to the billing account on which the role is granted.\nReference: https://cloud.google.com/billing/docs/how-to/billing-access\nI will go with option C. willy_p 4Â years, 1Â month ago\nThe question states that the user should ONLY link projects to billing accounts and nothing more. This is why I think A would be the best answer for this scenario. Josephsundarraj 2Â years, 5Â months ago\nOption C gives permission on org level where fin team can modify other projects billing. Question clearly says they should not be able to do that. So I think option A is good here in my opinion. Nikki2424 1Â year, 8Â months ago\nFrom the documentation:\n\"Project Billing Manager: When granted in combination with the Billing Account User role, the Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources.\" fishnoodlesoup 4Â years, 1Â month ago\nThe question states that Finance department should ONLY be able to link projects to billing accounts.\nIf you look at the definition of Project Billing Creator:\nProject Billing Manager\n(roles/billing.projectManager) Link/unlink the project to/from a billing account.\nIt also gives permissions to unlink. Hence, A is correct. Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: measme Highly Voted 5Â years, 7Â months ago\nfor me is C:\nhttps://cloud.google.com/billing/docs/how-to/modify-project#permissions_required_for_this_task_2\n\"Roles with adequate permissions to perform this task:\n* Project Owner or Project Billing Manager on the project, AND Billing Account Administrator or Billing Account User for the target Cloud Billing account.\" obeythefist 3Â years, 10Â months ago\nThe question states that the finance group should not be able to make changes to existing projects. Granting the finance team organizational level Billing Account Administrator will allow them to make changes to other projects. C cannot be correct. Robertolo 3Â years, 3Â months ago\nProject Billing Manager does not allow to make any changes to projects. It's just about linking+unlinking projects to billing accounts\nOn the other hand, the single role \"billing account user\" does not grant any right to view projects. Even less likely to link them to any billing account. (see https://cloud.google.com/iam/docs/job-functions/billing \"The Billing Account User role gives the service account the permissions to enable billing (associate projects with the organization's billing account for all projects in the organization) and thereby permit the service account to enable APIs that require billing to be enabled.\"). Thus A is not the correct answer.\nThe right answer is C, without any kind of doubt Jake500 2Â years, 9Â months ago\n\"Project Billing Manager does not allow to make any changes to projects. It's just about linking+unlinking projects to billing accounts\"\nCorrect, but the problem states \"... You do not want the engineering team to be able to link projects to the billing account.\" So in that case, wouldn't it be option A? fracila 3Â years, 2Â months ago\nWe are assigning the finance team the Billing Account User role on the billing account, which allows them to create new projects linked to the billing account on which the role is granted. We are also assigning them the Project Billing Manager role on the organization (trickles down to the project as well) which lets them attach the project to the billing account, but does not grant any rights over resources. sarjan 1Â year, 3Â months ago\nCorrect"
      },
      {
        "index": 3,
        "text": "Anonymous: jmotisariya Most Recent 10Â months, 1Â week ago\nSelected Answer: C\nCorrect Answer Option C. Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.\nExplanation:\nThe Billing Account User role allows users to view and link projects to a billing account.\nThe Project Billing Manager role allows users to manage billing for projects but does not grant broader project management permissions.\nSince the goal is to ensure only the finance team can link projects to the billing account, they need both roles.\nThe engineering team should not have billing-related roles, ensuring they cannot link projects to the billing account."
      },
      {
        "index": 4,
        "text": "Anonymous: sh4dw4rri0r 11Â months, 4Â weeks ago\nSelected Answer: A\nA is correct\nEngineering Team with the Project Creator role will not be able to link projects to a billing account."
      },
      {
        "index": 5,
        "text": "Anonymous: SteveXs 1Â year ago\nSelected Answer: C\nWhen granted in combination with the Billing Account User role, the Project Billing Manager role lets users attach the project to the billing account, but doesn't grant any rights over resources. Project Owners can use this role to let someone else manage the billing for the project without granting them resource access."
      },
      {
        "index": 6,
        "text": "Anonymous: Roman1988 1Â year ago\nSelected Answer: C\nOption C. When granted in combination with the Billing Account User role, the Project Billing Manager role lets users attach the project to the billing account, but doesn't grant any rights over resources. Project Owners can use this role to let someone else manage the billing for the project without granting them resource access."
      },
      {
        "index": 7,
        "text": "Anonymous: zAbuQasen 1Â year, 1Â month ago\nSelected Answer: A\nThe Project Billing Manager role on the organization is unnecessary. The Billing Account User role alone is sufficient to allow the finance team to link projects to the billing account."
      },
      {
        "index": 8,
        "text": "Anonymous: user263263 1Â year, 1Â month ago\nSelected Answer: C\nFrom the documentation: \"Project Billing Manager (on Organization, folder, or project) when granted in combination with the Billing Account User role (on Organization or billing account) ... lets users attach the project to the billing account, but doesn't grant any rights over resources.\""
      },
      {
        "index": 9,
        "text": "Anonymous: Moin23 1Â year, 1Â month ago\nSelected Answer: A\nChatGPT and my opinion also, A kamee15 1Â year ago\nI checked with ChatGPT, the answer is C."
      },
      {
        "index": 10,
        "text": "Anonymous: calebeowsiany 1Â year, 2Â months ago\nSelected Answer: A\nA is a well suited answer and it's in accord with the least privilege principle"
      }
    ]
  },
  {
    "id": 286,
    "source": "examtopics",
    "question": "You have an application running in Google Kubernetes Engine (GKE) with cluster autoscaling enabled. The application exposes a TCP endpoint. There are several replicas of this application. You have a Compute Engine instance in the same region, but in another Virtual Private Cloud (VPC), called gce-network, that has no overlapping IP ranges with the first VPC. This instance needs to connect to the application on GKE. You want to minimize effort. What should you do?",
    "options": {
      "A": "1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.",
      "B": "1. In GKE, create a Service of type NodePort that uses the application's Pods as backend. 2. Create a Compute Engine instance called proxy with 2 network interfaces, one in each VPC. 3. Use iptables on this instance to forward traffic from gce-network to the GKE nodes. 4. Configure the Compute Engine instance to use the address of proxy in gce-network as endpoint.",
      "C": "1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created.",
      "D": "1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add a Cloud Armor Security Policy to the load balancer that whitelists the internal IPs of the MIG's instances. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: someoneinthecloud Highly Voted 5Â years, 5Â months ago\nI believe it's A. It's never mentioned in the question that traffic cannot go through the Internet but it's mentioned that effort should be minimized. A requires a lot less effort than C to accomplish the same (no VPC peering, per example). pgb54 3Â years, 10Â months ago\nTotally agree. I had the same thought and looked through the question for any indication that the traffic must be private. AmitKM 5Â years, 4Â months ago\nYeah, I feel the same. Nowhere does it say that the traffic has to be internal. But it does say \"minimal effort\" which I feel is option A. ShakthiGCP 4Â years, 10Â months ago\nAns: A . This sounds correct and avoids unnecessary steps in C. C is also correct but compared to it, A is much easier to achieve. Go over Kubernetes Loadbalancer concepts to get more details. Initially i was thinking C is the Answer. but after putting some time on K8's Network - changed my mind to A. ArtistS 2Â years, 3Â months ago\nA,C are ok for me. But this is a exam. Why the question mention the same region, no overlapping IP ranges means they suggest you to use VPC rather than public traffic. I 99% sure, if there is an official explaniation, there would be A is not correct there is a risk or error prone, sth like this."
      },
      {
        "index": 2,
        "text": "Anonymous: juancambb Highly Voted 5Â years, 8Â months ago\ni think C is better solution, the solution A pass trafic trought public internet, also C by internal network and the \"no overlap ips\" in the statament suggest that."
      },
      {
        "index": 3,
        "text": "Anonymous: fais1985 Most Recent 1Â year ago\nSelected Answer: A\nA. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.\nWhy Other Options Are Incorrect:\nB:\nUsing a proxy instance with multiple network interfaces and iptables for forwarding traffic is unnecessarily complex and requires significant manual configuration and maintenance.\nC:\nCreating an internal load balancer and peering the VPCs requires additional setup for VPC peering and route configurations. This violates the requirement to minimize effort.\nD:\nAdding a Cloud Armor Security Policy is unnecessary for this use case. Furthermore, the Compute Engine instance is in a different VPC, so using internal IPs of the GKE nodes is not possible without peering."
      },
      {
        "index": 4,
        "text": "Anonymous: psyll0n 1Â year, 2Â months ago\nSelected Answer: C\nC is the correct answer.\nReference: https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing-across-vpc-net psyll0n 1Â year, 2Â months ago\nReference: https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing"
      },
      {
        "index": 5,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: A"
      },
      {
        "index": 6,
        "text": "Anonymous: DWT33004 1Â year, 9Â months ago\nSelected Answer: A\nHere's why Option A might be preferred over Option C:\nSimplicity: Option A requires creating a LoadBalancer service in GKE and configuring the Compute Engine instance to use the load balancer's address. This is a straightforward setup and does not involve additional networking configurations.\nReduced Complexity: Peering two VPCs involves setting up and managing VPC peering configurations, which can be complex, especially if there are overlapping IP ranges. It also requires additional permissions and coordination between different teams.\nDirect Connectivity: Option A provides direct connectivity between the Compute Engine instance and the application running in GKE through the load balancer. Peering VPCs might introduce additional network hops, potentially impacting latency and network performance.\nScalability and Flexibility: Using a LoadBalancer service in GKE allows for scalability and flexibility, as the load balancer can automatically scale to handle increased traffic and can be easily configured to adapt to changing requirements."
      },
      {
        "index": 7,
        "text": "Anonymous: edoo 1Â year, 11Â months ago\nSelected Answer: C\nNot A, exposing the service with an external LoadBalancer (externalTrafficPolicy set to Cluster) and not peering VPCs or using an internal load balancer unnecessarily exposes the service to the internet, which is not required for inter-VPC communication and could lead to security concerns.\nAll the details in the question are pushing to answer C."
      },
      {
        "index": 8,
        "text": "Anonymous: ovokpus 2Â years, 3Â months ago\nSelected Answer: C\nOption A suggests creating an external LoadBalancer. This is not the most efficient method because you're exposing your GKE application to the internet just to allow communication between two internal resources.\nOption C suggests creating an internal LoadBalancer, which is the right approach. By using an internal LoadBalancer, the service is only exposed within the Google Cloud environment and won't be accessible from the internet. Peering the two VPCs ensures the two resources can communicate across the VPCs."
      },
      {
        "index": 9,
        "text": "Anonymous: ekta25 2Â years, 3Â months ago\nC. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created."
      },
      {
        "index": 10,
        "text": "Anonymous: SinghAnc 2Â years, 3Â months ago\nSelected Answer: C\nCorrect Answer is C\nOption A suggests setting the service's externalTrafficPolicy to Cluster. While this is a valid configuration, it's not directly related to the scenario described.\nIn the given scenario, the goal is to connect a Compute Engine instance from a different VPC to the application running in GKE. This involves networking configurations, peering the VPCs, and potentially setting up a LoadBalancer.\nSetting the externalTrafficPolicy to Cluster primarily affects how traffic is balanced across Pods within the cluster, but it doesn't directly address the requirement of connecting an external instance from a different VPC."
      }
    ]
  },
  {
    "id": 287,
    "source": "examtopics",
    "question": "Your organization is a financial company that needs to store audit log files for 3 years. Your organization has hundreds of Google Cloud projects. You need to implement a cost-effective approach for log file retention. What should you do?",
    "options": {
      "A": "Create an export to the sink that saves logs from Cloud Audit to BigQuery.",
      "B": "Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.",
      "C": "Write a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.",
      "D": "Export these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: yasu Highly Voted 5Â years, 9Â months ago\nWhy not B? cost effective lxs 4Â years, 2Â months ago\nBigQuery data after 90 days has the same cost for storage as Cloud Storage Nearline. Storing it in Cloud Storage adds more costs for data retrival if the class is i.e archival KerolesKhalil 2Â years, 7Â months ago\nthe options have cold-line storage not nearline.\nso B is the cheapest option. _Sande 2Â years, 9Â months ago\nThat seems to be the one... uganeshku 4Â years ago\nB is correct because Coldline Storage is the perfect service to store audit logs from all the projects and is very cost-efficient as well. Coldline Storage is a very low-cost, highly durable storage service for storing infrequently accessed data."
      },
      {
        "index": 2,
        "text": "Anonymous: Gini Highly Voted 5Â years, 9Â months ago\nif it is all about cost, B is the best. However, speaking of \"audit\" you probably need to access the data once in a while, which Coldline storage might not be ideal for this case I guess? I would go for A in the exam though. Ale1973 5Â years, 4Â months ago\nBe strong!!! If B is the best, go for B!!! pas77 4Â years, 5Â months ago\nThe question is clearly saying cost effect. BQ is one of the most expensive services in GCP. boof 4Â years, 3Â months ago\nI would play it safe and interpret the question literally, implying that they will only store the audit logs and not be accessing them a lot."
      },
      {
        "index": 3,
        "text": "Anonymous: pragneshpandya Most Recent 1Â year, 4Â months ago\nSelected Answer: B\nas its logs for 3 years , coldline is the correct"
      },
      {
        "index": 4,
        "text": "Anonymous: jungkook_1 1Â year, 8Â months ago\nBut the retention period of Coldline storage is 90 days, it's not meeting the requirement mentioned in ques to store for 3 years."
      },
      {
        "index": 5,
        "text": "Anonymous: jungkook_1 1Â year, 8Â months ago\nBut retention period of coldline storage is 90 days only, in ques they've mentioned for 3 years?"
      },
      {
        "index": 6,
        "text": "Anonymous: LSB56757 1Â year, 9Â months ago\nSelected Answer: B\nUsing Google Gemini, it suggests Option B."
      },
      {
        "index": 7,
        "text": "Anonymous: zameerb 1Â year, 12Â months ago\nBoth options (exporting to BigQuery and exporting to Coldline Storage) have their merits, and the choice depends on specific use cases, access patterns, and organizational preferences. If your organization values a more structured and analyzable format with SQL-like querying capabilities, BigQuery might be preferable. If the priority is on long-term, infrequent access with cost optimization, then Coldline Storage could be a suitable choice."
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is B."
      },
      {
        "index": 9,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: B\nB is correct. coldline storage it is cost-effective and for long-term storage"
      },
      {
        "index": 10,
        "text": "Anonymous: elviskimutai 2Â years, 4Â months ago\nB is correct. coldline storage it is cost-effective and for long-term storage"
      }
    ]
  },
  {
    "id": 288,
    "source": "examtopics",
    "question": "You want to run a single caching HTTP reverse proxy on GCP for a latency-sensitive website. This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost. How should you run this reverse proxy?",
    "options": {
      "A": "Create a Cloud Memorystore for Redis instance with 32-GB capacity.",
      "B": "Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.",
      "C": "Package it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.",
      "D": "Run it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: jzh Highly Voted 5Â years, 5Â months ago\nGo to cloud console and create instance\nselect Memorystore with Basic tier, select us-central1 and us-central1-a, and capacity 32GB, the cost estimate is $0.023/GB/hr\nselect VM instance with custom machine type with 6 vCPUs and 32 GB memory, the same region and zone as Memorystore setting, the cost estimate is $0.239/hr\nOption B will definitely cost more as it adds on CPU usage cost even it uses little in this scenario, but still charge you. So answer is A from real practice example. SSPC 5Â years, 5Â months ago\nI agree with you Rothmansua 4Â years, 3Â months ago\nand what about HTTP, how are you supporting that with Redis? obeythefist 3Â years, 10Â months ago\nA quick Bing search shows a number of solutions for caching HTTP services with Redis. smarty_arse 3Â years, 6Â months ago\nWho uses Bing at this present day and age? RNSS 3Â years, 2Â months ago\nbelieve me it is very good and clean. When I was doing my research I have used both google and bing. and find bing as more trusted and complete answer. mexblood1 5Â years, 4Â months ago\nUsing pricing calculator matching 730 hrs per month for both.. Memorystore is 537.28 per month and vm (6 cpus 32 gb memory) is 174.41 per month. So vm is still cheaper even with 6 cpus. FenixRa73 5Â years ago\n$0.023 * 32 = $0.736\nis it cheaper?"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer should be A:\nThe question mention \"You want to have a 30-GB in-memory cache, and\nneed an additional 2 GB of memory for the rest of the processes\"\nWhat is Google Cloud Memorystore?\nOverview. Cloud Memorystore for Redis is a fully managed Redis service for Google Cloud Platform. Applications running on Google Cloud Platform can achieve extreme performance by leveraging the highly scalable, highly available, and secure Redis service without the burden of managing complex Redis deployments. ESP_SAP 5Â years, 4Â months ago\nJust to complement the answer:\nWe are looking for \"latency-sensitive website\"\nWhat it's good for\nMemorystore for Redis provides a fast, in-memory store for use cases that require fast, real-time processing of data. From simple caching use cases to real time analytics, Memorystore for Redis provides the performance you need.\nCaching: Cache is an integral part of modern application architectures. Memorystore for Redis provides low latency access and high throughput for heavily accessed data, compared to accessing the data from a disk based backend store. Session management, frequently accessed queries, scripts, and pages are common examples of caching.\nhttps://cloud.google.com/memorystore/docs/redis/redis-overview#what_its_good_for"
      },
      {
        "index": 3,
        "text": "Anonymous: vaclavbenes1 Most Recent 11Â months, 2Â weeks ago\nSelected Answer: B\nBoth Gemini and Claude.ai wotes for B. You need to run a reverse proxy server in this can you can eliminate A."
      },
      {
        "index": 4,
        "text": "Anonymous: Sekar_1992 1Â year ago\nSelected Answer: B\nYou need 30 GB of in-memory cache plus an additional 2 GB for other processes.\nThe solution should minimize cost while fulfilling the memory requirements.\nOption Analysis:\nA. Create a Cloud Memorystore for Redis instance with 32-GB capacity:\nIncorrect: While Memorystore is designed for caching, it does not meet the requirement to host an HTTP reverse proxy. Memorystore is only a caching layer, not a compute platform for running processes.\nB. Run it on Compute Engine with a custom instance type:\nCorrect: Compute Engine allows you to specify a custom machine type tailored to your exact resource requirements (e.g., 6 vCPUs and 32 GB of memory). This minimizes cost compared to using a predefined machine type.\nThe reverse proxy can run directly on the Compute Engine instance, and the memory requirements are fulfilled."
      },
      {
        "index": 5,
        "text": "Anonymous: mnasruul 1Â year, 1Â month ago\nSelected Answer: B\nIm choice B because the question is `This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes`, they need additional 2 GB of memory for the rest of the processes and maybe cannot running at Redis."
      },
      {
        "index": 6,
        "text": "Anonymous: Noni_11 1Â year, 1Â month ago\nSelected Answer: D\nLas razones por las que cada opciÃ³n es o no adecuada:\nA: INCORRECTA. Cloud Memorystore es un servicio de cachÃ© administrado, no puede ejecutar un proxy HTTP.\nB: INCORRECTA. 6 vCPUs es excesivo ya que se especifica que consume casi nada de CPU. SerÃ­a un desperdicio de recursos y dinero.\nC: INCORRECTA. GKE serÃ­a una sobrecarga innecesaria para una Ãºnica instancia y n1-standard-32 es extremadamente sobredimensionado.\nD: CORRECTA. Una n1-standard-1 (1 vCPU, 3.75GB RAM) con un disco SSD de 32GB es la opciÃ³n mÃ¡s econÃ³mica que cumple los requisitos mÃ­nimos necesarios para el proxy.\nLa D es la correcta porque es la opciÃ³n mÃ¡s econÃ³mica que proporciona los recursos necesarios. Un SSD persistente puede usarse para swap si se necesita mÃ¡s memoria, aunque no serÃ¡ tan rÃ¡pido como la memoria RAM."
      },
      {
        "index": 7,
        "text": "Anonymous: user263263 1Â year, 1Â month ago\nSelected Answer: B\nA. Redis isn't used as storage for caching proxies - it is a kind of key-value store.\nB. fulfills the requirements, e.g. take nginx use a RAM disk for caching\nC. wrong machine type (32 vCPU), don't need k8s\nD. technically possible, but does not fulfill \"in memory\" requirement"
      },
      {
        "index": 8,
        "text": "Anonymous: nubelukita45852 1Â year, 4Â months ago\nSelected Answer: D\nLa n1-standard-1 es una instancia de bajo costo con 1 vCPU y 3.75 GB de memoria, suficiente para los procesos adicionales del proxy. Dado que el proxy inverso prÃ¡cticamente no consume CPU, no es necesario optar por una instancia mÃ¡s grande. El disco persistente SSD de 32 GB puede actuar como almacenamiento para la cachÃ© en lugar de usar costosas soluciones en memoria, lo que ayuda a minimizar costos, mientras proporciona un almacenamiento rÃ¡pido, suficiente para el sitio sensible a la latencia."
      },
      {
        "index": 9,
        "text": "Anonymous: spatters 1Â year, 5Â months ago\nA might be a fine answer, except that Redis is not an http reverse proxy. It is a data cache. So A, regardless of the cost, does not work for this use case."
      },
      {
        "index": 10,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer:B"
      }
    ]
  },
  {
    "id": 289,
    "source": "examtopics",
    "question": "You are hosting an application on bare-metal servers in your own data center. The application needs access to Cloud Storage. However, security policies prevent the servers hosting the application from having public IP addresses or access to the internet. You want to follow Google-recommended practices to provide the application with access to Cloud Storage. What should you do?",
    "options": {
      "A": "1. Use nslookup to get the IP address for storage.googleapis.com. 2. Negotiate with the security team to be able to give a public IP address to the servers. 3. Only allow egress traffic from those servers to the IP addresses for storage.googleapis.com.",
      "B": "1. Using Cloud VPN, create a VPN tunnel to a Virtual Private Cloud (VPC) in Google Cloud. 2. In this VPC, create a Compute Engine instance and install the Squid proxy server on this instance. 3. Configure your servers to use that instance as a proxy to access Cloud Storage.",
      "C": "1. Use Migrate for Compute Engine (formerly known as Velostrata) to migrate those servers to Compute Engine. 2. Create an internal load balancer (ILB) that uses storage.googleapis.com as backend. 3. Configure your new instances to use this ILB as proxy.",
      "D": "1. Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 1Â month ago\nD is the correct one as per Ref: https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid"
      },
      {
        "index": 2,
        "text": "Anonymous: obeythefist Highly Voted 3Â years, 4Â months ago\nWhat messy answers! I chose D and here is my reasoning per answer.\nA. It's bad practice to use nslookup to try find a permanent IP address because IPs can change. That's what DNS is for! Also, the security team aren't going to budge... this is just a silly answer.\nB. We're getting warmer. Any time a question mentions on-prem and cloud, Google wants you to think about Cloud VPN. This solution might even work, but installing Squid? This is a messy solution to a more simple problem.\nC. Talk about using a sledge hammer to swat a mosquito. I think this could work, but migrating servers to cloud to solve a simple networking problem?\nD. Once more Google's favorite Cloud VPN is in the answer. I'm not sure about the networking component of this question. obeythefist 3Â years, 4Â months ago\nEdit: Of course the reason D: is correct is because 199.36.153.4/30 is the network segment that you can direct traffic to if you want to use Google services \"internally\". So your on prem servers will resolve storage.googleapis.com to something in this 199.36.153.4/30 range. Then they will route using Cloud Router and your VPN tunnel into Google Cloud privately."
      },
      {
        "index": 3,
        "text": "Anonymous: Cloudmoh Most Recent 11Â months, 1Â week ago\nSelected Answer: D\nWell D is the right option : . Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com"
      },
      {
        "index": 4,
        "text": "Anonymous: subha.elumalai 1Â year, 1Â month ago\nC is the correct Answer"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is D"
      },
      {
        "index": 6,
        "text": "Anonymous: gsmasad 1Â year, 8Â months ago\nSelected Answer: D\nD as per google recommened pratcises"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD as per google recommened pratcises"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: D\nANSWER D is the recommended solution because it provides a secure and direct connection to Cloud Storage without requiring internet access or exposing the servers to public IP addresses.\n* By setting up a VPN or Interconnect tunnel, the on-premises servers can access Google\nCloud resources over a private and encrypted connection.\n* The custom route advertisement for 199.36.153.4/30 ensures that traffic is routed\ncorrectly between the on-premises network and Google Cloud.\n* Configuring the DNS server to resolve *.googleapis.com as a CNAME to\nrestricted.googleapis.com ensures that requests are directed to Google Cloud over the\nVPN or Interconnect tunnel."
      },
      {
        "index": 9,
        "text": "Anonymous: warrior9000 2Â years, 6Â months ago\nD but anyone wanna try to explain how the hell you can have a VPN connection without accessing the public internet? The only option for D should be using Interconnect for a direct private wire from your data center to GCP. VPN doesn't make any sense. ast3citos 2Â years, 4Â months ago\nThe machine hosting the application cannot access directly the public internet. So to go to Google Cloud it must go through a VPN. xaqanik 2Â years, 6Â months ago\nhttps://cloud.google.com/vpc/docs/configure-private-google-access-hybrid"
      },
      {
        "index": 10,
        "text": "Anonymous: cslince 2Â years, 7Â months ago\nSelected Answer: D\nD is the correct"
      }
    ]
  },
  {
    "id": 290,
    "source": "examtopics",
    "question": "You want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "1. Create a Cloud Function that uses a Cloud Pub/Sub trigger on that topic. 2. Call your application on Cloud Run from the Cloud Function for every message.",
      "B": "1. Grant the Pub/Sub Subscriber role to the service account used by Cloud Run. 2. Create a Cloud Pub/Sub subscription for that topic. 3. Make your application pull messages from that subscription.",
      "C": "1. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.",
      "D": "1. Deploy your application on Cloud Run on GKE with the connectivity set to Internal. 2. Create a Cloud Pub/Sub subscription for that topic. 3. In the same Google Kubernetes Engine cluster as your application, deploy a container that takes the messages and sends them to your application."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Meix Highly Voted 5Â years, 7Â months ago\nC looks right for me as per https://cloud.google.com/run/docs/tutorials/pubsub#integrating-pubsub ChrisBelt5 4Â years, 5Â months ago\ngreat doc, its' C"
      },
      {
        "index": 2,
        "text": "Anonymous: Bhagirathi Highly Voted 5Â years, 1Â month ago\nwhy c ?\nexplained>>\nYou can use Pub/Sub to push messages to the endpoint of your Cloud Run service, where the messages are subsequently delivered to containers as HTTP requests. You cannot use Pub/Sub pull subscriptions because Cloud Run only allocates CPU during the processing of a request."
      },
      {
        "index": 3,
        "text": "Anonymous: user263263 Most Recent 1Â year, 1Â month ago\nSelected Answer: C\nC follows the official description in \"Use Pub/Sub with Cloud Run tutorial\"\nB. you cannot pull (that means wait for messages) in Cloud Run code. It only runs in response to a request and is suspended less than a second later"
      },
      {
        "index": 4,
        "text": "Anonymous: xylene314 1Â year, 1Â month ago\nSelected Answer: C\nC is the right answer"
      },
      {
        "index": 5,
        "text": "Anonymous: varshitag 1Â year, 3Â months ago\nC is right Answer"
      },
      {
        "index": 6,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect answer is D"
      },
      {
        "index": 7,
        "text": "Anonymous: DWT33004 1Â year, 9Â months ago\nSelected Answer: B\nHere's why Option B might be preferred over Option C:\nService Account Permissions: Option C involves creating a separate service account and granting it the Cloud Run Invoker role.\nDirect Subscription Configuration: Option B allows for a direct configuration of a Cloud Pub/Sub subscription to pull messages from the topic eliminating the need to manage service accounts and roles separately.\nStandard Practice: Granting the Pub/Sub Subscriber role directly to the service account used by Cloud Run is a standard practice for allowing Cloud Run services to access Pub/Sub topics follows the principle of least privilege by granting only the necessary permissions to the service account.\nPush vs. Pull Model: Option C uses a push model where Cloud Pub/Sub sends messages directly to the Cloud Run service. While this model can work, it requires additional setup for configuring the push endpoint\nOverall, Option B provides a simpler and more direct approach to integrating Cloud Run with Cloud Pub/Sub, aligning well with Google-recommended practices"
      },
      {
        "index": 8,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nSelected Answer: C\nC looks the correct option."
      },
      {
        "index": 9,
        "text": "Anonymous: hanweiCN 2Â years, 8Â months ago\nSelected Answer: C\nit is explicated recommended use \" push\" :\nNote: Google recommends using push subscriptions to consume messages from a Pub/Sub topic on Cloud Run. Although it is possible to use Pub/Sub pull subscriptions, pull subscriptions require you to monitor message delivery latency and manually scale the number of instances to maintain a healthy delivery latency. If you want to use pull subscriptions, use the CPU always allocated setting along with a number of minimum instances.\nhttps://cloud.google.com/run/docs/triggering/pubsub-push"
      },
      {
        "index": 10,
        "text": "Anonymous: SMR123 2Â years, 9Â months ago\nwhat is the actual answer people who are voting or actual answer?"
      }
    ]
  },
  {
    "id": 291,
    "source": "examtopics",
    "question": "You need to deploy an application, which is packaged in a container image, in a new project. The application exposes an HTTP endpoint and receives very few requests per day. You want to minimize costs. What should you do?",
    "options": {
      "A": "Deploy the container on Cloud Run.",
      "B": "Deploy the container on Cloud Run on GKE.",
      "C": "Deploy the container on App Engine Flexible.",
      "D": "Deploy the container on GKE with cluster autoscaling and horizontal pod autoscaling enabled."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Gurnoor Highly Voted 5Â years, 7Â months ago\nA should be cheapest as no infra needed. spudleymcdudley 5Â years, 6Â months ago\nListen to this guy. Google says \"Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneouslyâ€”depending on traffic. Cloud Run only charges you for the exact resources you use.\""
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer should be A:\nCloud Run takes any container images and pairs great with the container ecosystem: Cloud Build, Artifact Registry, Docker. ... No infrastructure to manage: once deployed, Cloud Run manages your services so you can sleep well. Fast autoscaling. Cloud Run automatically scales up or down from zero to N depending on traffic.\nhttps://cloud.google.com/run"
      },
      {
        "index": 3,
        "text": "Anonymous: nubelukita45852 Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nCloud Run es una plataforma serverless que permite ejecutar contenedores de manera altamente escalable y a un costo muy bajo, ya que solo se paga por las solicitudes recibidas y el tiempo de ejecuciÃ³n. Dado que la aplicaciÃ³n recibe pocas solicitudes por dÃ­a, Cloud Run es la opciÃ³n mÃ¡s rentable, ya que no incurre en costos cuando no hay trÃ¡fico. AdemÃ¡s, es fÃ¡cil de implementar y mantiene la infraestructura al mÃ­nimo, lo que optimiza tanto costos como administraciÃ³n."
      },
      {
        "index": 4,
        "text": "Anonymous: Mohammed52 1Â year, 7Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: MUNHU 1Â year, 7Â months ago\nA is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: B"
      },
      {
        "index": 7,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 8,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 9,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA, as it does not include the infra services and its cheaper"
      },
      {
        "index": 10,
        "text": "Anonymous: sthapit 2Â years, 5Â months ago\nShould be A"
      }
    ]
  },
  {
    "id": 292,
    "source": "examtopics",
    "question": "Your company has an existing GCP organization with hundreds of projects and a billing account. Your company recently acquired another company that also has hundreds of projects and its own billing account. You would like to consolidate all GCP costs of both GCP organizations onto a single invoice. You would like to consolidate all costs as of tomorrow. What should you do?",
    "options": {
      "A": "Link the acquired company's projects to your company's billing account.",
      "B": "Configure the acquired company's billing account and your company's billing account to export the billing data into the same BigQuery dataset.",
      "C": "Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.",
      "D": "Create a new GCP organization and a new billing account. Migrate the acquired company's projects and your company's projects into the new GCP organization and link the projects to the new billing account."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: GunjGupta Highly Voted 5Â years, 7Â months ago\nTo me, A looks correct. projects are linked to another organization as well in the acquired company so migrating would need google cloud support. we can not do ourselves. however, we can link other company projects to an existing billing account to generate total cost.\nhttps://medium.com/google-cloud/google-cloud-platform-cross-org-billing-41c5db8fefa6 uganeshku 4Â years ago\nA is correct because linking all projects of the acquired organization to the main organizationâ€™s billing account will generate a single bill for all projects.\nD is incorrect because there is no need to create a new organization for this. spudleymcdudley 5Â years, 6Â months ago\nListen to this guy. It's 'A' as moving projects can take some time from Google. There's no need to create a new organisation and other options don't make any sense lxgywil 4Â years, 8Â months ago\nYou're saying it as if \"moving projects\" was a viable option. What about B? ryumada 3Â years, 5Â months ago\nI think B is not make sense. You don't want to do statistical analytic to the billing data. You want to consolidate all the costs as of tomorrow. So, the costs as of tomorrow should be billed in one billing account. That's what I've understand from the question."
      },
      {
        "index": 2,
        "text": "Anonymous: XRiddlerX Highly Voted 5Â years, 5Â months ago\nI could be missing something but where does it say in the question that the two orgs want to migrate projects? I believe the question and key points are \"consolidate all GCP costs\" and \"consolidate all costs as of tomorrow\". With that said, C and D would not be a 24 hour task and seems a bit cumbersome to perform for something simple as \"creating a single invoice\" AND that's a migration and not a consolidation of cost. With A, I can't find anywhere in GCP docs that this is a best practice, only a medium.com blog. IMHO, I won't go down this route because \"Just because you can do something, doesn't mean you should.\" and I would consult GCP support for best practices on A before I do something like that.\nThat leaves B which is to export both detailed billing to BigQuery and create a invoice/report. This would be a temporary solution until you migrate Organizations. IMHO\nI go with B. ashrafh 4Â years, 5Â months ago\nI also vote B,\nwhy?\nagree with this technical explanation and my finance team not gonna pay some newly acquired company bill by tomorrow :) Armne96X 4Â years, 1Â month ago\nAre you sure you can do all steps by tomorrow?\n(You would like to consolidate all costs as of tomorrow) ninjaasmoke 3Â years, 1Â month ago\nWhat does exporting data to BigQuery have to do with creating an Invoice? TAvenger 4Â years, 11Â months ago\nI am not sure that exporting some statistical data to BigQuery means anything for Google who creates the invoice.\nWith \"A\" you are right, that is not the best practice, but the key word \"for tomorrow\" allows this custom approach.\nSo the answer is \"A\" zaxma 3Â years, 9Â months ago\nI will go with A in the exam as well, but just wondering, they are two different organisations, how can you link all projects from org2 to org1's billing account without the help of GCP support?? eBooKz 2Â years, 12Â months ago\nCloud Billing accounts can be used across organization resources. However, organization resource moves often also include a requirement to move to a new billing account. To get the permissions that you need to change the project's billing account, ask your administrator to grant you the following IAM roles:\nBilling Account User (roles/billing.user) on the destination billing account\nProject billing manager (roles/billing.projectManager) on the project\nhttps://cloud.google.com/resource-manager/docs/project-migration#permissions-billing Load full discussion..."
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 4Â weeks ago\nSelected Answer: A\nA single invoice is a key word.\nIMHO, Single Invoice means Single Billing Account so i will go with A"
      },
      {
        "index": 4,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: C\nI will go for C"
      },
      {
        "index": 5,
        "text": "Anonymous: DWT33004 1Â year, 9Â months ago\nSelected Answer: A\nA. Link the acquired company's projects to your company's billing account.\nExplanation:\nBilling Account Linking: By linking the acquired company's projects to your company's billing account, you can consolidate all costs onto a single invoice. This allows for centralized billing management and easier tracking of expenses.\nImmediate Consolidation: This action can be implemented quickly and efficiently, allowing for cost consolidation as of tomorrow, as specified in the requirement.\nMinimal Disruption: Linking projects to a different billing account does not require significant changes to the existing project configurations or organizational structure. It allows both companies to maintain their separate GCP organizations and project structures while consolidating billing.\nCost Tracking: With all costs consolidated onto a single invoice, it becomes easier to track expenses and manage budgets effectively."
      },
      {
        "index": 6,
        "text": "Anonymous: abyacharya90 1Â year, 11Â months ago\nC. Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.\nHere's why:\nOption A: Linking projects to a different billing account doesn't consolidate costs onto a single invoice.\nOption B: Exporting data to a shared BigQuery dataset allows analysis but doesn't consolidate billing itself.\nOption C: This approach achieves your goal efficiently:\nMigration: Moving acquired company projects into your organization allows centralized management and cost consolidation.\nLinking to existing billing account: Ensures all project costs appear on your existing invoice starting from the day of migration.\nTiming: Given the urgency of same-day consolidation, this is the fastest option."
      },
      {
        "index": 7,
        "text": "Anonymous: PiperMe 1Â year, 11Â months ago\nThis question is outdated: As of October 26, 2023, Google Cloud Platform does not allow directly linking projects between separate organizations to a single billing account. Each organization must have its own billing account, and resource costs cannot be directly consolidated across distinct organizations."
      },
      {
        "index": 8,
        "text": "Anonymous: Shriyanka 2Â years ago\nC should be be correct as per me"
      },
      {
        "index": 9,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is D"
      },
      {
        "index": 10,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer"
      }
    ]
  },
  {
    "id": 293,
    "source": "examtopics",
    "question": "You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data.\nYou need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Add the support team group to the roles/monitoring.viewer role",
      "B": "Add the support team group to the roles/spanner.databaseUser role.",
      "C": "Add the support team group to the roles/spanner.databaseReader role.",
      "D": "Add the support team group to the roles/stackdriver.accounts.viewer role."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 7Â months ago\nits A, As you need to monitor only WindDriver 4Â years, 6Â months ago\nA, right, correct answer.\nB and C are incorrect because allow to read data.\nD also incorrect: Not for monitoring. roles/stackdriver.accounts.viewer Stackdriver Accounts Viewer:\nRead-only access to get and list information about Stackdriver account structure (resourcemanager.projects.get, resourcemanager.projects.list and stackdriver.projects.get) WindDriver 4Â years, 6Â months ago\nhttps://cloud.google.com/iam/docs/understanding-roles"
      },
      {
        "index": 2,
        "text": "Anonymous: Gurnoor Highly Voted 5Â years, 7Â months ago\nA is correct as user should not have any access to data, so B and C cant be used in this scenario."
      },
      {
        "index": 3,
        "text": "Anonymous: nish2288 Most Recent 1Â year, 6Â months ago\nIts D.\nStackdriver roles in GCP (Google Cloud Platform) are predefined sets of permissions that control access to monitoring and logging data within Stackdriver, a suite of tools for monitoring and logging applications and infrastructure in GCP.\nThese roles determine what users or groups can see and do within Stackdriver. They allow you to grant granular access levels, ensuring users have the necessary permissions to perform their tasks without exposing sensitive data or granting unnecessary control. peddyua 12Â months ago\nit does not grant access to Cloud Spanner metadata or any related Spanner-specific monitoring data."
      },
      {
        "index": 4,
        "text": "Anonymous: ekta25 2Â years, 3Â months ago\nA. Add the support team group to the roles/monitoring.viewer role"
      },
      {
        "index": 5,
        "text": "Anonymous: axantroff 2Â years, 3Â months ago\nSelected Answer: A\nMakes sense for me"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA as you only need the monitor access"
      },
      {
        "index": 7,
        "text": "Anonymous: sakdip66 2Â years, 9Â months ago\nthe goal of support team is to MONITOR the environment only. therefore roles/monitoring.viewer role is the best option we have\nhttps://cloud.google.com/spanner/docs/iam#roles"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: A\nAnswer A, adding the support team group to the roles/monitoring.viewer role, is the CORRECT answer. This role grants read-only access to monitoring data for all resources in a project, which allows the support team to monitor the environment but not access the table data.\nAnswer B, adding the support team group to the roles/spanner.databaseUser role, grants read and write access to all tables in the specified database, which is NOT required for the support team to monitor the environment.\nAnswer C, adding the support team group to the roles/spanner.databaseReader role, grants read-only access to all tables in the specified database, which would give the support team access to the table data.\nAnswer D, adding the support team group to the roles/stackdriver.accounts.viewer role, grants permissions to view Stackdriver data for all resources in a project, which is NOT directly related to monitoring the Cloud Spanner environment."
      },
      {
        "index": 9,
        "text": "Anonymous: cslince 3Â years, 1Â month ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: Zoze 3Â years, 2Â months ago\nSelected Answer: A\nA is correct, the team need to monitor the environment not read the data."
      }
    ]
  },
  {
    "id": 294,
    "source": "examtopics",
    "question": "For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Cloud Logging agent on all the instances. You want to minimize cost. What should you do?",
    "options": {
      "A": "1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.",
      "B": "1. In Cloud Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.",
      "C": "1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.",
      "D": "1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: sumanshu Highly Voted 4Â years, 4Â months ago\nvote for ''C\"\nhttps://cloud.google.com/logging/docs/export/configure_export_v2"
      },
      {
        "index": 2,
        "text": "Anonymous: vmart Highly Voted 4Â years, 1Â month ago\nI vote for C"
      },
      {
        "index": 3,
        "text": "Anonymous: Nitesh2000 Most Recent 1Â year, 5Â months ago\nOption C"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 5,
        "text": "Anonymous: axantroff 1Â year, 9Â months ago\nC, it's simple enough"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC , it minimizes the cost"
      },
      {
        "index": 7,
        "text": "Anonymous: jayjani66 2Â years ago\nAnswer: C. Option C allows you to create a log export from Cloud Logging to BigQuery with minimal setup and cost. By creating a filter to view only Compute Engine logs, you ensure that only the relevant logs are exported to BigQuery, reducing unnecessary data transfer and storage costs."
      },
      {
        "index": 8,
        "text": "Anonymous: Kyle1776 2Â years, 1Â month ago\nSelected Answer: B\nB looks like the most cost effective option since filtering out only the logs you need will reduce storage and data transfer costs. Kyle1776 2Â years, 1Â month ago\nCorrection - C"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: C\nThe most cost-effective and recommended solution to send logs from Compute Engine instances to BigQuery is to use the Cloud Logging agent with a sink that streams the logs to BigQuery.\nAnswer C is the most appropriate solution. In Cloud Logging, create a filter to view only Compute Engine logs. Click Create Export. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination. This will allow all Compute Engine instance logs to be exported to BigQuery with minimal complexity and cost."
      },
      {
        "index": 10,
        "text": "Anonymous: David_Esteban 2Â years, 7Â months ago\nSelected Answer: C\nMi vote is for \"c\""
      }
    ]
  },
  {
    "id": 295,
    "source": "examtopics",
    "question": "You are using Deployment Manager to create a Google Kubernetes Engine cluster. Using the same Deployment Manager deployment, you also want to create a\nDaemonSet in the kube-system namespace of the cluster. You want a solution that uses the fewest possible services. What should you do?",
    "options": {
      "A": "Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.",
      "B": "Use the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition.",
      "C": "With Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet.",
      "D": "In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 4Â months ago\nCorrect Answer is (A):\nAdding an API as a type provider\nThis page describes how to add an API to Google Cloud Deployment Manager as a type provider. To learn more about types and type providers, read the Types overview documentation.\nA type provider exposes all of the resources of a third-party API to Deployment Manager as base types that you can use in your configurations. These types must be directly served by a RESTful API that supports Create, Read, Update, and Delete (CRUD).\nIf you want to use an API that is not automatically provided by Google with Deployment Manager, you must add the API as a type provider.\nhttps://cloud.google.com/deployment-manager/docs/configuration/type-providers/creating-type-provider magistrum 4Â years ago\nvery good find, sounds like you hit the nail in the head"
      },
      {
        "index": 2,
        "text": "Anonymous: PR0704 Highly Voted 3Â years, 1Â month ago\ncouldn't be more confusing"
      },
      {
        "index": 3,
        "text": "Anonymous: skhan Most Recent 6Â months, 2Â weeks ago\nSelected Answer: A\nA is Correct"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is the correct , bcoz it help you contact directly to the gke cluster to create daemon"
      },
      {
        "index": 5,
        "text": "Anonymous: sthapit 1Â year, 5Â months ago\nShould have been D"
      },
      {
        "index": 6,
        "text": "Anonymous: sakdip66 1Â year, 9Â months ago\nSelected Answer: A\noption A is the right answer because it lets you directly interact with the Kubernetes API to create the Daemonset using the same deployment Manager Deployment"
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: A\nI would say both Answer A and Answer D are valid solutions, and it depends on your preference and requirements.\nAnswer A involves adding the cluster's API as a new Type Provider in Deployment Manager and using the new type to create the DaemonSet. This solution would allow you to create and manage the DaemonSet and the cluster in the same Deployment Manager deployment.\nAnswer D involves adding a metadata block to the Deployment Manager deployment of the cluster, which will create the DaemonSet in the kube-system namespace of the cluster. This solution would allow you to create the DaemonSet in a simple way and avoid the need to create a new Type of Provider.\nIn conclusion, I would choose Answer A to be considered the answer that uses the fewest possible services, as it only involves adding the cluster's API as a new Type Provider in Deployment Manager, which is a lightweight solution."
      },
      {
        "index": 8,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: D\nD. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value.\nThis approach involves adding the DaemonSet manifest directly as a metadata entry in the cluster's definition in Deployment Manager. When the cluster is created, the DaemonSet is automatically created in the kube-system namespace. This approach is the simplest and requires the fewest number of services. Option A is also a viable solution but requires more work to set up a Type Provider. Option B is not suitable because it involves a separate service (Runtime Configurator). Option C is also not recommended because it involves creating a Compute Engine instance and using kubectl to create the DaemonSet, which is more complicated and less efficient than the other options."
      },
      {
        "index": 9,
        "text": "Anonymous: vkamlesh0205 2Â years ago\nSelected Answer: A\nOption A is the right answer"
      },
      {
        "index": 10,
        "text": "Anonymous: RanjithK 2Â years, 6Â months ago\nSelected Answer: A\nAnswer is A."
      }
    ]
  },
  {
    "id": 296,
    "source": "examtopics",
    "question": "You are building an application that will run in your data center. The application will use Google Cloud Platform (GCP) services like AutoML. You created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your on-premises environment. What should you do?",
    "options": {
      "A": "Use service account credentials in your on-premises application.",
      "B": "Use gcloud to create a key file for the service account that has appropriate permissions.",
      "C": "Set up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications.",
      "D": "Go to the IAM & admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect answer should be (B):\nTo use a service account outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account. Public/private key pairs provide a secure way of accomplishing this goal.\nhttps://cloud.google.com/iam/docs/creating-managing-service-account-keys"
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 1Â year, 11Â months ago\nSelected Answer: B\nThe recommended approach for enabling authentication from an on-premises environment to Google Cloud Platform (GCP) services like AutoML is to use a service account and generate a JSON key file for the service account. This key file can then be used to authenticate and authorize API calls from your on-premises environment to GCP.\nTherefore, the correct answer is B. Use gcloud to create a key file for the service account that has appropriate permissions."
      },
      {
        "index": 3,
        "text": "Anonymous: 85c887f Most Recent 9Â months, 4Â weeks ago\nSelected Answer: A\nCorrect answer should be A. \"Use service account credentials\", so here \"credentials\" indicate that we will use JSON key file in our on-premises application, and it is a correct way to authenticate from on-premises to APIs. The B option just says how to create this file, and it misses the next step for what to do next to achieve an authentication. In option A and B we will have the same JSON key file, but only option A contains full way to accomplish the task."
      },
      {
        "index": 4,
        "text": "Anonymous: thewalker 1Â year, 1Â month ago\nSelected Answer: B\nB\nAs per the documentation: https://cloud.google.com/iam/docs/keys-create-delete#creating"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is B"
      },
      {
        "index": 6,
        "text": "Anonymous: drinkwater 1Â year, 3Â months ago\nA. Use service account credentials in your on-premises application.\nExplanation:\nService accounts are the recommended way to authenticate your application and authorize it to access GCP services.\nYou can create and use service account credentials to authenticate your application running in your on-premises environment and access GCP services like AutoML.\nOption B (using gcloud to create a key file for the service account) is a valid approach to generate credentials for a service account, but using those credentials in your application is essential, which aligns with option A.\nOptions C and D are not directly related to enabling authentication for on-premises applications using service account credentials. Setting up direct interconnect (option C) is about networking, and granting permissions to a user account (option D) is not the standard approach for authenticating an application running on-premises to GCP services"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB is the correct answer, as to access the out side the google cloud , you need the key"
      },
      {
        "index": 8,
        "text": "Anonymous: Bobbybash 1Â year, 11Â months ago\nSelected Answer: A\nA. Use service account credentials in your on-premises application.\nTo enable authentication to GCP services from your on-premises environment, you can use service account credentials in your on-premises application. This involves creating a service account that has appropriate access to the required GCP services, downloading the service account key file, and using the key file to authenticate the API requests in your on-premises application. This is a secure way to authenticate to GCP services as it does not require direct access to your GCP project or credentials from your on-premises environment. Buruguduystunstugudunstuy 1Â year, 11Â months ago\nCloud Security/Auditor doesn't like Answer \"A\". Using service account credentials in your on-premises application could be a security risk if the credentials are compromised. If the key file is stolen or leaked, an attacker could use it to access your GCP resources, potentially causing data breaches, service disruptions, or financial losses.\nI would select Answer \"B\". Use gcloud to create a key file for the service account that has appropriate permissions and let Security Auditor stay away from my back. Never-ending \"You cannot do this, you cannot do that\" on Answer A."
      },
      {
        "index": 9,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: B\nB it is."
      },
      {
        "index": 10,
        "text": "Anonymous: mvk2022 2Â years, 1Â month ago\nSelected Answer: B\nB it is."
      }
    ]
  },
  {
    "id": 297,
    "source": "examtopics",
    "question": "You are using Container Registry to centrally store your company's container images in a separate project. In another project, you want to create a Google\nKubernetes Engine (GKE) cluster. You want to ensure that Kubernetes can download images from Container Registry. What should you do?",
    "options": {
      "A": "In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.",
      "B": "When you create the GKE cluster, choose the Allow full access to all Cloud APIs option under 'Access scopes'.",
      "C": "Create a service account, and give it access to Cloud Storage. Create a P12 key for this service account and use it as an imagePullSecrets in Kubernetes.",
      "D": "Configure the ACLs on each image in Cloud Storage to give read-only access to the default Compute Engine service account."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer (A):\nIAM permissions\nIAM permissions determine who can access resources. All users, service accounts, and other identities that interact with Container Registry must have the appropriate Cloud Storage permissions.\nBy default, Google Cloud use default service accounts to interact with resources within the same project. For example, the Cloud Build service account can both push and pull images when Container Registry is in the same project.\nYou must configure or modify permissions yourself if:\nYou are using a service account in one project to access Container Registry in a different project\nYou are using a default service account with read-only access to storage, but you want to both pull and push images\nYou are using a custom service account to interact with Container Registry\nhttps://cloud.google.com/container-registry/docs/access-control"
      },
      {
        "index": 2,
        "text": "Anonymous: XRiddlerX Highly Voted 5Â years, 5Â months ago\nA is correct...\nContainer Registry uses Cloud Storage buckets as the underlying storage for container images. You control access to your images by granting appropriate Cloud Storage permissions to a user, group, service account, or other identity.\nIf the service account needs to access Container Registry in another project, you must grant the required permissions in the project with Container Registry.\nReference:\nhttps://cloud.google.com/container-registry/docs/access-control#permissions"
      },
      {
        "index": 3,
        "text": "Anonymous: gseva Most Recent 11Â months, 1Â week ago\nSelected Answer: A\nContainer Registry stores images in Cloud Storage buckets under gcr.io, us.gcr.io, etc.\nGKE nodes need permission to pull images from this registry."
      },
      {
        "index": 4,
        "text": "Anonymous: Enamfrancis 1Â year, 3Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: omunoz 1Â year, 8Â months ago\nAs per https://cloud.google.com/container-registry/docs/access-control.\nContainer Registry is deprecated and scheduled for shutdown. After May 15, 2024, Artifact Registry will host images for the gcr.io domain in Google Cloud projects without previous Container Registry usage. After March 18, 2025, Container Registry will be shut down.\nArtifact Registry is the recommended service for container image storage and management on Google Cloud."
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is the correct answer , as granting this role allow to download the image"
      },
      {
        "index": 7,
        "text": "Anonymous: sakdip66 2Â years, 9Â months ago\nSelected Answer: A\nGrating Storage Object Viewer IAM Role to the service account used by Kubernetes nodes allow the nodes to download the images from Container registry."
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: A\nAnswer A. In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.\nTo ensure that Kubernetes can download container images from Container Registry, you need to grant the necessary permissions to the service account used by the Kubernetes nodes. In this case, you would need to grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes in the project where the images are stored. This role allows the service account to read objects from Cloud Storage buckets, including the container images in Container Registry."
      },
      {
        "index": 9,
        "text": "Anonymous: jrisl1991 2Â years, 11Â months ago\nSelected Answer: A\nDefinitely A seems more practical and accurate."
      },
      {
        "index": 10,
        "text": "Anonymous: GaneshSurwase 3Â years, 3Â months ago\nCORRET ANS is A"
      }
    ]
  },
  {
    "id": 298,
    "source": "examtopics",
    "question": "You deployed a new application inside your Google Kubernetes Engine cluster using the YAML file specified below.\n\nYou check the status of the deployed pods and notice that one of them is still in PENDING status:\n\nYou want to find out why the pod is stuck in pending status. What should you do?",
    "options": {
      "A": "Review details of the myapp-service Service object and check for error messages.",
      "B": "Review details of the myapp-deployment Deployment object and check for error messages.",
      "C": "Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages.",
      "D": "View logs of the container in myapp-deployment-58ddbbb995-lp86m pod and check for warning messages."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: spudleymcdudley Highly Voted 4Â years, 6Â months ago\nIt's C - https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods"
      },
      {
        "index": 2,
        "text": "Anonymous: someoneinthecloud Highly Voted 4Â years, 6Â months ago\nAnswer is C - You can't view logs of a pod that isn't deployed, so D is incorrect.\nC allows you to check the pod deployment messages and look for errors sidharthwader 3Â years, 6Â months ago\nWhat u said is incorrect you can view pod's log even in pending state.\nkubectl logs <pon-name> -n <namespace>"
      },
      {
        "index": 3,
        "text": "Anonymous: Captain1212 Most Recent 1Â year, 4Â months ago\nSelected Answer: C\nC is correct, as its help you to check the error"
      },
      {
        "index": 4,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: C\nAnswer C. Review details of myapp-deployment-58ddbbb995-lp86m Pod and check for warning messages.\nTo find out why a pod is stuck in pending status, you can review the details of the pod and check for any warning messages. Answer C is the correct answer because it suggests reviewing the details of the specific pod that is stuck in pending status. You can use the kubectl describe pod <pod-name> command to view detailed information about the pod, including any warning messages that might indicate why the pod is not scheduled."
      },
      {
        "index": 5,
        "text": "Anonymous: AKSHAT09jain 2Â years ago\nD : we first check logs"
      },
      {
        "index": 6,
        "text": "Anonymous: Zoze 2Â years, 2Â months ago\nSelected Answer: C\nI vote C; because if we imagine that we will go to a main menu that display the errors of the all deployment object as hole, we will surely navigate thin to the pod menu ! so the C option will direct take us to the second menu."
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 2Â years, 3Â months ago\nC is correct,\nDebugging Pods\nThe first step in debugging a Pod is taking a look at it. Check the current state of the Pod and recent events with the following command:\nkubectl describe pods ${POD_NAME}"
      },
      {
        "index": 8,
        "text": "Anonymous: Letahrgicbeagle 2Â years, 4Â months ago\nSelected Answer: C\nDefinitely"
      },
      {
        "index": 9,
        "text": "Anonymous: Dheeraj1986 2Â years, 5Â months ago\nSelected Answer: B\nI guess it's B. its deployment that creates the pod and it has the information why it is not able to create. it shows the information if you describe the deployment ( kubectl describe deployment )"
      },
      {
        "index": 10,
        "text": "Anonymous: abirroy 2Â years, 5Â months ago\nSelected Answer: C\nAnswer is C"
      }
    ]
  },
  {
    "id": 299,
    "source": "examtopics",
    "question": "You are setting up a Windows VM on Compute Engine and want to make sure you can log in to the VM via RDP. What should you do?",
    "options": {
      "A": "After the VM has been created, use your Google Account credentials to log in into the VM.",
      "B": "After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.",
      "C": "When creating the VM, add metadata to the instance using 'windows-password' as the key and a password as the value.",
      "D": "After the VM has been created, download the JSON private key for the default Compute Engine service account. Use the credentials in the JSON file to log in to the VM."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: John_Iam Highly Voted 5Â years, 1Â month ago\nCorrect Answer is B.\nB. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.\nhttps://cloud.google.com/sdk/gcloud/reference/beta/compute/reset-windows-password voler 5Â years ago\nYes! \"If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned.\" dan80 5Â years, 1Â month ago\ndid you even look at the link you provide ? it clearly say gcloud beta compute reset-windows-password my-instance and not gcloud compute reset-windows-password. D is correct - https://cloud.google.com/iam/docs/creating-managing-service-account-keys dan80 5Â years, 1Â month ago\nnobody talk on reset the password but how to access the Windows - best way - Service Account KerolesKhalil 2Â years, 1Â month ago\nService accounts shouldn't be used for RDP , they are used to machine authentication with services. KerolesKhalil 2Â years, 1Â month ago\nAlso how you will RDP with Service account and private key\nYou need username and password , it is not ssh lxgywil 4Â years, 2Â months ago\nOh yes? Then what about this link (for non-beta command)?\nhttps://cloud.google.com/sdk/gcloud/reference/compute/reset-windows-password\n\"If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned.\"\nThe answer is obviously B. Just test it and it'll become very clear ragu123 4Â years, 11Â months ago\nCorrect answer is B.\ngcloud beta compute reset-windows-password allows a user to reset and retrieve a password for a Windows virtual machine instance. If the Windows account does not exist, this command will cause the account to be created and the password for that new account will be returned."
      },
      {
        "index": 2,
        "text": "Anonymous: Buruguduystunstugudunstuy Highly Voted 2Â years, 5Â months ago\nSelected Answer: B\nCORRECT:\nAnswer B. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.\nINCORRECT:\nAnswer A is not correct because Google Account credentials cannot be used to log in to Windows VMs.\nAnswer C is not correct because metadata can be used to specify some settings for a VM, but the 'windows-password' metadata key is not used for specifying the login password for a Windows VM.\nAnswer D is not correct because the JSON private key for the default Compute Engine service account is not used for logging in to a Windows VM."
      },
      {
        "index": 3,
        "text": "Anonymous: Sakshi1221 Most Recent 1Â year, 7Â months ago\nWhy this website has published all incorrect answers"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is B."
      },
      {
        "index": 5,
        "text": "Anonymous: dataSoftNinja 1Â year, 8Â months ago\nwhat is rdp meaning please ? lagx 1Â year, 4Â months ago\nremote desktop protocal used to connect and login into windows machine, similar to ssh protocal for linux machine"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nYes , B is the corrrect answer"
      },
      {
        "index": 7,
        "text": "Anonymous: sakdip66 2Â years, 3Â months ago\nSelected Answer: B\nafter creating Windows VM on COmpute Engine it has a local user account as well. This acct is used to login to the VM via RDP. If you forget the password you can use gcloud compute reset-windows-password to reset it. This command generates a new password and sets it for the user account on the VM."
      },
      {
        "index": 8,
        "text": "Anonymous: Partha117 2Â years, 3Â months ago\nSelected Answer: B\nCorrect option B"
      },
      {
        "index": 9,
        "text": "Anonymous: smanoj85 2Â years, 4Â months ago\nOption B is correct. After creating a Windows VM instance on Compute Engine, you need to use the gcloud compute reset-windows-password command to retrieve the login credentials for the VM. This command generates a new Windows password and displays it in the output of the command. You can then use this password to log in to the VM via RDP.\nOption A is incorrect because logging in to the VM using your Google Account credentials is not a supported method for Windows VM instances.\nOption C is also incorrect because 'windows-password' is not a recognized metadata key for Windows VM instances.\nOption D is incorrect because you cannot use the JSON private key for the default Compute Engine service account to log in to a Windows VM instance via RDP."
      },
      {
        "index": 10,
        "text": "Anonymous: cslince 2Â years, 7Â months ago\nSelected Answer: B\nCorrect Answer is B.\nB. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM"
      }
    ]
  },
  {
    "id": 300,
    "source": "examtopics",
    "question": "You want to configure an SSH connection to a single Compute Engine instance for users in the dev1 group. This instance is the only resource in this particular\nGoogle Cloud Platform project that the dev1 users should be able to connect to. What should you do?",
    "options": {
      "A": "Set metadata to enable-oslogin=true for the instance. Grant the dev1 group the compute.osLogin role. Direct them to use the Cloud Shell to ssh to that instance.",
      "B": "Set metadata to enable-oslogin=true for the instance. Set the service account to no service account for that instance. Direct them to use the Cloud Shell to ssh to that instance.",
      "C": "Enable block project wide keys for the instance. Generate an SSH key for each user in the dev1 group. Distribute the keys to dev1 users and direct them to use their third-party tools to connect.",
      "D": "Enable block project wide keys for the instance. Generate an SSH key and associate the key with that instance. Distribute the key to dev1 users and direct them to use their third-party tools to connect."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 1Â month ago\nA correct one nithinpb180 5Â years, 1Â month ago\nAgree with that spudleymcdudley 5Â years ago\nFor further evidence... https://cloud.google.com/compute/docs/instances/managing-instance-access"
      },
      {
        "index": 2,
        "text": "Anonymous: student002 Highly Voted 4Â years, 9Â months ago\nPure from logic thinking: A can't be right. If the group get access to that instance with enable-oslogin=true, then they could have access to every instance that has enable-oslogin=true. Or do I miss something? magistrum 4Â years, 6Â months ago\nI'm convinced with this logic bgallet 3Â years, 7Â months ago\nclearly, question say \"the only ressource they need to access in this project\"\nas you said, all ressources will be available if we set the role jrisl1991 2Â years, 5Â months ago\nThat's not necessarily true - https://cloud.google.com/compute/docs/oslogin/set-up-oslogin. The doc says \"If you want enable OS Login for all VMs in a project, set the metadata at the project-level. If you want to enable OS Login for a single VM, set the metadata at the instance-level.\"\nThat means you can do it at the instance level, so there shouldn't be a problem with following A. akshaychavan7 3Â years, 1Â month ago\nNote the sentence \"Set metadata to enable-oslogin=true for the instance.\" This means the metadata for oslogin has been set to that particular instance only, and not for all."
      },
      {
        "index": 3,
        "text": "Anonymous: Cynthia2023 Most Recent 1Â year, 6Â months ago\nSelected Answer: A\nOS Login Feature:\nOS Login is a feature in GCP that manages SSH access to your Compute Engine instances using IAM (Identity and Access Management) roles. When OS Login is enabled, it allows you to use IAM roles to grant or revoke SSH access to your instances, which can be more secure and manageable than traditional SSH key management.\nEnabling OS Login:\nSetting the instance metadata enable-oslogin=true enables the OS Login feature on that specific Compute Engine instance.\nWhen OS Login is enabled, traditional SSH keys defined in the project or instance metadata are ignored, and the instance instead relies on IAM roles for SSH access."
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is correct as , it gives the only specific access"
      },
      {
        "index": 5,
        "text": "Anonymous: KerolesKhalil 2Â years, 1Â month ago\nSelected Answer: A\nhttps://cloud.google.com/compute/docs/oslogin/set-up-oslogin"
      },
      {
        "index": 6,
        "text": "Anonymous: sakdip66 2Â years, 3Â months ago\nSelected Answer: A\nEnabling OSLogin allow user to login to Google Cloud credentials to authenticate to instance, instead of SSH key.\nGranting 'compute.osLogin' to the dev1 lets them login using OSLogin to the resourcee\nBD are incorrect because \"block project-wide SSH keys\" is an advance security features taht is used for high secured environment where more granular control over SSH us required\nC is hassle because it manually ditribute keys to each user in Dev1 which is time consuming"
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: A\nAnswer B is incorrect because setting the service account to no service account has no impact on SSH access to the VM instance.\nAnswer C is incorrect because generating an SSH key for each user in the dev1 group and distributing them is cumbersome and not scalable, especially if you have many users.\nAnswer D is incorrect because generating a single SSH key and distributing it to multiple users undermines security, as it means any of the users in possession of the key can access the VM instance."
      },
      {
        "index": 8,
        "text": "Anonymous: FeaRoX 2Â years, 5Â months ago\nIn my opinion A would be best, but they have to use this and only this 1 instance. You don't know if any other instances has this metadata set up - if they do, dev1 team has also access to this instances, what invalidates the answer. To make sure they are using only this 1 instance, I'd say D."
      },
      {
        "index": 9,
        "text": "Anonymous: jrisl1991 2Â years, 5Â months ago\nSelected Answer: A\nBased on https://cloud.google.com/compute/docs/oslogin/set-up-oslogin I'd go for A."
      },
      {
        "index": 10,
        "text": "Anonymous: alex000 2Â years, 6Â months ago\nSelected Answer: C\nThe dev1 users should be able to connect only to this VM instance"
      }
    ]
  },
  {
    "id": 301,
    "source": "examtopics",
    "question": "You need to produce a list of the enabled Google Cloud Platform APIs for a GCP project using the gcloud command line in the Cloud Shell. The project name is my-project. What should you do?",
    "options": {
      "A": "Run gcloud projects list to get the project ID, and then run gcloud services list --project <project ID>.",
      "B": "Run gcloud init to set the current project to my-project, and then run gcloud services list --available.",
      "C": "Run gcloud info to view the account value, and then run gcloud services list --account <Account>.",
      "D": "Run gcloud projects describe <project ID> to verify the project value, and then run gcloud services list --available."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 5Â years, 1Â month ago\nA is the correct answer, log to gcloud and run the commands, doesnt make sense to run cloud init and gcloud services list --available gives you the full services that are available. raffiq 5Â years, 1Â month ago\nYes, Answer A correct. it shows only enabled services of API"
      },
      {
        "index": 2,
        "text": "Anonymous: lxgywil Highly Voted 4Â years, 2Â months ago\n\"A\" is correct.\nFor those, who have doubts:\n`gcloud services list --available` returns not only the enabled services in the project but also services that CAN be enabled. Therefore, option B is incorrect.\nhttps://cloud.google.com/sdk/gcloud/reference/services/list#--available squishy_fishy 4Â years, 1Â month ago\nBest answer!"
      },
      {
        "index": 3,
        "text": "Anonymous: pzacariasf7 Most Recent 1Â year, 4Â months ago\nSelected Answer: A\nA is correct!"
      },
      {
        "index": 4,
        "text": "Anonymous: sabrinakloud 2Â years, 3Â months ago\nSelected Answer: A\nAnswer: A"
      },
      {
        "index": 5,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: A\nLet's do a side-by-side analysis of Answer A and Answer D to clear our doubts:\nAnswer A: Run gcloud projects list to get the project ID, and then run gcloud services list --project <project ID>.\nThis option first retrieves the project ID by running the gcloud projects list command.\nThen, it uses the gcloud services list command with the --project flag to list the enabled APIs for the specified project. Buruguduystunstugudunstuy 2Â years, 5Â months ago\nAnswer D: Run gcloud projects describe <project ID> to verify the project value, and then run gcloud services list --available.\nThis option uses the gcloud projects describe command with the project ID to retrieve information about the specified project, including the project ID.\nThen, it uses the gcloud services list command with the --available flag to list all available APIs, not just the ones that are enabled for the specified project.\nBased on the scenario presented in the question, we want to produce a list of the enabled APIs for a GCP project, NOT a list of all available APIs. Therefore, Answer A is more appropriate because it specifically lists the enabled APIs for the specified project.\nAnswer D lists all available APIs which may include APIs that are not enabled in the project, which could cause confusion or unnecessary information.\nTherefore, Answer A is the correct option in this case."
      },
      {
        "index": 6,
        "text": "Anonymous: Bobbybash 2Â years, 5Â months ago\nSelected Answer: D\nD. Run gcloud projects describe <project ID> to verify the project value, and then run gcloud services list --available.\nTo produce a list of enabled Google Cloud Platform APIs for a GCP project using the gcloud command line, you can first run gcloud projects describe <project ID> to verify the project ID for the project in question. Then, you can run gcloud services list --available to list all the available APIs and see which ones are enabled for the project. This command shows the full list of services and their status, including whether they are enabled, disabled, or ready for use. Option A is incorrect because it lists all the available services, regardless of whether they are enabled or not. Option B is incorrect because it lists only the available services, which might not be enabled in the project. Option C is incorrect because it shows account information and not service information."
      },
      {
        "index": 7,
        "text": "Anonymous: Charumathi 2Â years, 9Â months ago\nA is correct answer,\nRun the following command to list the enabled APIs and services in your current project:\ngcloud services list\nwhereas, Run the following command to list the APIs and services available to you in your current project:\ngcloud services list --available"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3Â years ago\nA is the correct answer"
      },
      {
        "index": 9,
        "text": "Anonymous: haroldbenites 3Â years, 1Â month ago\nGo for A"
      },
      {
        "index": 10,
        "text": "Anonymous: luciorifa 3Â years, 5Â months ago\nSelected Answer: A\nA is the correct answer"
      }
    ]
  },
  {
    "id": 302,
    "source": "examtopics",
    "question": "You are building a new version of an application hosted in an App Engine environment. You want to test the new version with 1% of users before you completely switch your application over to the new version. What should you do?",
    "options": {
      "A": "Deploy a new version of your application in Google Kubernetes Engine instead of App Engine and then use GCP Console to split traffic.",
      "B": "Deploy a new version of your application in a Compute Engine instance instead of App Engine and then use GCP Console to split traffic.",
      "C": "Deploy a new version as a separate app in App Engine. Then configure App Engine using GCP Console to split traffic between the two apps.",
      "D": "Deploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SIX Highly Voted 5Â years, 1Â month ago\nCorrect answer is D"
      },
      {
        "index": 2,
        "text": "Anonymous: mohdafiuddin Highly Voted 4Â years, 6Â months ago\nSplitting the question to the key requirements"
      },
      {
        "index": 1,
        "text": "new version of an application hosted in an App Engine environment."
      },
      {
        "index": 2,
        "text": "test the new version with 1% of users\nApp engine supports versioning and traffic splitting so no need to involve anything else\n(source - https://cloud.google.com/appengine#all-features)\nA. ....'Google Kubernetes Engine'.... - No need to involve GKE. Not the right option\nB. ....'Compute Engine instance'.... - No need to involve Compute Engine.\nC. ....'Separate app in App Engine'....- No need to deploy as a separate app. versioning is supported already. Not the right option.\nD. This is the right answer. akshaychavan7 3Â years, 1Â month ago\nJust to add, for option C you cannot have two applications deployed inside an app engine project. In order to do so, you need to create the application inside a new project.\nSo, we just eliminate option C."
      },
      {
        "index": 3,
        "text": "Anonymous: tmpcs Most Recent 1Â year, 6Â months ago\nthe correct answer is D"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is D."
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: D\nD is correct , because you cannot create a sepearte app in the same app engine"
      },
      {
        "index": 6,
        "text": "Anonymous: Neha_Pallavi 1Â year, 10Â months ago\nD. Deploy a new version of your application in App Engine. Then go to App Engine settings in GCP Console and split traffic between the current version and newly deployed versions accordingly"
      },
      {
        "index": 7,
        "text": "Anonymous: trainingexam 2Â years ago\nSelected Answer: D\nApp engine provides out of box functionality to split the traffic between multiple versions"
      },
      {
        "index": 8,
        "text": "Anonymous: sabrinakloud 2Â years, 3Â months ago\nSelected Answer: D\nAnswer D is the correct answer"
      }
    ]
  },
  {
    "id": 303,
    "source": "examtopics",
    "question": "You need to provide a cost estimate for a Kubernetes cluster using the GCP pricing calculator for Kubernetes. Your workload requires high IOPs, and you will also be using disk snapshots. You start by entering the number of nodes, average hours, and average days. What should you do next?",
    "options": {
      "A": "Fill in local SSD. Fill in persistent disk storage and snapshot storage.",
      "B": "Fill in local SSD. Add estimated cost for cluster management.",
      "C": "Select Add GPUs. Fill in persistent disk storage and snapshot storage.",
      "D": "Select Add GPUs. Add estimated cost for cluster management."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 5Â years, 1Â month ago\nThis one is Tricky, local SSD is require for High IOPS - https://cloud.google.com/compute/docs/disks/local-ssd , but it say using disk snapshots. A is correct."
      },
      {
        "index": 2,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 1Â month ago\nA is correct ."
      },
      {
        "index": 3,
        "text": "Anonymous: Gayathri29 Most Recent 1Â year, 6Â months ago\nA is correct"
      },
      {
        "index": 4,
        "text": "Anonymous: tmpcs 1Â year, 6Â months ago\nObviously the correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is correct , as ssd requires the High IOPS"
      },
      {
        "index": 7,
        "text": "Anonymous: trainingexam 2Â years ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: sakdip66 2Â years, 3Â months ago\nSelected Answer: A\nsince the requirement is high IOPs Local SSD is our best option. that makes:\nC and D as irrelevant.\nB Add estimated cost for cluster management is not related to storage requirement mentioned in the scenario"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: A\nTo provide a cost estimate for a Kubernetes cluster using the GCP pricing calculator for Kubernetes, after entering the number of nodes, average hours, and average days, you should fill in the required storage and snapshot details.\nGiven that your workload requires high IOPs and will also be using disk snapshots, the appropriate option would be;\nA. Fill in local SSD. Fill in persistent disk storage and snapshot storage. chikorita 2Â years, 3Â months ago\nmy lord! you're always right"
      },
      {
        "index": 10,
        "text": "Anonymous: NosFerazi 2Â years, 5Â months ago\nSelected Answer: B\nWhy do I need to fill out persistent disk storage and snapshot storage, it is already populated.\nfilling out local ssd should suffice. going with B Buruguduystunstugudunstuy 2Â years, 5Â months ago\nIf persistent disk storage and snapshot storage are already populated in the GCP pricing calculator, you do not need to fill them out again. In that case, selecting local SSD and adding an estimated cost for cluster management, as suggested in Answer B, would be sufficient.\nHowever, it is important to note that the cost estimate may not be accurate if any of the details in the GCP pricing calculator are incorrect or do not match your requirements. Therefore, it is always a good practice to review all the details and ensure that they are accurate and up to date before finalizing the cost estimate.\nIn summary, if persistent disk storage and snapshot storage are already populated, and you only require local SSD and an estimated cost for cluster management, then Answer B is a valid choice. Still, I go with Answer A my friend. :)"
      }
    ]
  },
  {
    "id": 304,
    "source": "examtopics",
    "question": "You are using Google Kubernetes Engine with autoscaling enabled to host a new application. You want to expose this new application to the public, using HTTPS on a public IP address. What should you do?",
    "options": {
      "A": "Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer.",
      "B": "Create a Kubernetes Service of type ClusterIP for your application. Configure the public DNS name of your application using the IP of this Service.",
      "C": "Create a Kubernetes Service of type NodePort to expose the application on port 443 of each node of the Kubernetes cluster. Configure the public DNS name of your application with the IP of every node of the cluster to achieve load-balancing.",
      "D": "Create a HAProxy pod in the cluster to load-balance the traffic to all the pods of the application. Forward the public traffic to HAProxy with an iptable rule. Configure the DNS name of your application using the public IP of the node HAProxy is running on."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: arsav Highly Voted 4Â years, 11Â months ago\nHAProxy is HTTP only, doesnt support HTTPS, so you can reject option D\nhttps://www.haproxy.org/#desc\nCluster IP - is an internal IP, you cannot expose public externally. reject option B\nout of option A and C\nC, port 443 is https but public DNS is not going to give you a load balancing\nA is the right choice,\nkubernets ingress exposes HTTPS\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/\nand cloud load balancer is the right choice which will help to expose the app to public NoniGeorge 4Â years ago\nPretty sure that option D works more from on premise then cloud because with cloud you pretty much don't have to configure your ip tables !"
      },
      {
        "index": 2,
        "text": "Anonymous: dan80 Highly Voted 5Â years, 7Â months ago\nA is correct. magistrum 5Â years ago\nSaw this which provides good context https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0 nitinz 4Â years, 11Â months ago\nyou nailed it."
      },
      {
        "index": 3,
        "text": "Anonymous: yehia2221 Most Recent 1Â year, 5Â months ago\noption A is correct, but do not use it in real deployments, it is a bad practice. I am wondering why they didn't mention Cluster IP and exposing it via an ingress or at least a service of type loadbalancer"
      },
      {
        "index": 4,
        "text": "Anonymous: Cynthia2023 2Â years ago\nSelected Answer: A\nIn Kubernetes, a Service of type NodePort is a way to expose your applications to external traffic. It's one of the several types of Services available in Kubernetes to control how external sources can access services running within the cluster. Here's what a NodePort service entails:\nExposing Services Outside the Cluster:\nA NodePort service makes your application accessible from outside the Kubernetes cluster by opening a specific port (the NodePort) on all the nodes (VMs) in your cluster. This port is randomly selected from a defined range (default: 30000-32767) unless you specify a particular port. Cynthia2023 2Â years ago\nWhen a NodePort service is created, each node in the cluster allocates the specified NodePort. External traffic can access the service by hitting any node's IP address at the NodePort, regardless of whether that node is actually running a pod for the service.\nKubernetes internally routes that traffic to the appropriate pods, even if they are running on different nodes."
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\noption A is correct as you need load balancing and in option c dns will not give you load balancing"
      },
      {
        "index": 7,
        "text": "Anonymous: frantishk 2Â years, 5Â months ago\nI didnt know, that ClusterIP is an internal IP and you cannot expose public externally..\nThanks !"
      },
      {
        "index": 8,
        "text": "Anonymous: trainingexam 2Â years, 6Â months ago\nSelected Answer: A\nA is very easy solution"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: A\nTo expose a new application hosted on Google Kubernetes Engine with autoscaling enabled to the public using HTTPS on a public IP address, the most appropriate option would be;\nA. Create a Kubernetes Service of type NodePort for your application, and a Kubernetes Ingress to expose this Service via a Cloud Load Balancer."
      },
      {
        "index": 10,
        "text": "Anonymous: GS300 2Â years, 12Â months ago\nSelected Answer: A\nA works and is correct, but service type should be ClusterIP"
      }
    ]
  },
  {
    "id": 305,
    "source": "examtopics",
    "question": "You need to enable traffic between multiple groups of Compute Engine instances that are currently running two different GCP projects. Each group of Compute\nEngine instances is running in its own VPC. What should you do?",
    "options": {
      "A": "Verify that both projects are in a GCP Organization. Create a new VPC and add all instances.",
      "B": "Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC.",
      "C": "Verify that you are the Project Administrator of both projects. Create two new VPCs and add all instances.",
      "D": "Verify that you are the Project Administrator of both projects. Create a new VPC and add all instances."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 4Â years, 7Â months ago\nB - https://cloud.google.com/vpc/docs/shared-vpc"
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 4Â years, 3Â months ago\nB. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "index": 3,
        "text": "Anonymous: trainingexam Most Recent 1Â year, 6Â months ago\nSelected Answer: B\nshared-vpc is right option"
      },
      {
        "index": 4,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: B\nUse shared VPC"
      },
      {
        "index": 5,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: B\nShared VPC is the correct choice here"
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: B\nTo enable traffic between multiple groups of Compute Engine instances running in different VPCs of different GCP projects, the best option would be;\nB. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC."
      },
      {
        "index": 7,
        "text": "Anonymous: hems4all 2Â years, 1Â month ago\nSelected Answer: B\nB correct"
      },
      {
        "index": 8,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: B\nB is correc"
      },
      {
        "index": 9,
        "text": "Anonymous: Zoze 2Â years, 2Â months ago\nSelected Answer: B\nB is correct because is the concept of the shared VPC."
      },
      {
        "index": 10,
        "text": "Anonymous: darcal95 2Â years, 4Â months ago\nok, is B, but that means that the VMs in the \"other project\" have to change their ip?"
      }
    ]
  },
  {
    "id": 306,
    "source": "examtopics",
    "question": "You want to add a new auditor to a Google Cloud Platform project. The auditor should be allowed to read, but not modify, all project items.\nHow should you configure the auditor's permissions?",
    "options": {
      "A": "Create a custom role with view-only project permissions. Add the user's account to the custom role.",
      "B": "Create a custom role with view-only service permissions. Add the user's account to the custom role.",
      "C": "Select the built-in IAM project Viewer role. Add the user's account to this role.",
      "D": "Select the built-in IAM service Viewer role. Add the user's account to this role."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cloudenthu01 Highly Voted 5Â years ago\nC is correct\nroles/Viewer role provides access to all resources under the projects but do not alter the state of these resources mav3r1ck 2Â years, 11Â months ago\nIt should be A.\nhttps://cloud.google.com/iam/docs/faq#when_would_i_use_basic_roles\nWhen would I use basic roles?\nYou can use basic roles in development and test environments, where it might be appropriate for some principals to have wide-ranging permissions. Avoid basic roles in production environments. mav3r1ck 2Â years, 11Â months ago\nPrinciple of least privilege creativenets 2Â years, 1Â month ago\ni disagree. jrisl1991 2Â years, 5Â months ago\nBut in this case we're not asked to follow any best practices. Besides, the help article says \"In production environments, do not grant basic roles unless there is no alternative.\", and in this case there's no alternative since we need to grant access to all resources."
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 4Â years, 9Â months ago\nC. Select the built-in IAM project Viewer role. Add the user's account to this role."
      },
      {
        "index": 3,
        "text": "Anonymous: DWT33004 Most Recent 1Â year, 3Â months ago\nSelected Answer: C\nC. Select the built-in IAM project Viewer role. Add the user's account to this role.\nExplanation:\nIAM Project Viewer Role: The IAM project Viewer role provides read-only access to all resources within a Google Cloud Platform project. This role allows the user to view project items, including resources and configurations, but does not grant permissions to modify them. This aligns with the requirement of allowing the auditor to read, but not modify, all project items.\nBuilt-in Role: The IAM project Viewer role is a built-in role provided by Google Cloud Platform. It is specifically designed for users who need read-only access to project resources.\nLeast Privilege: Selecting the IAM project Viewer role ensures that the auditor has the necessary permissions to perform their tasks without granting them unnecessary privileges. It follows the principle of least privilege, providing only the permissions required to fulfill their role."
      },
      {
        "index": 4,
        "text": "Anonymous: tmwf 1Â year, 4Â months ago\nSelected Answer: C\nI think C is more correct ."
      },
      {
        "index": 5,
        "text": "Anonymous: thewalker 1Â year, 8Â months ago\nSelected Answer: C\nC is better though it is a basic role, as the question says all the project items."
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 7,
        "text": "Anonymous: ArtistS 1Â year, 9Â months ago\nC is correct. Project viewer provide read-only permissions to all resources; no permission to change resources."
      },
      {
        "index": 8,
        "text": "Anonymous: drinkwater 1Â year, 9Â months ago\nTo grant an auditor read-only access to all project items on Google Cloud Platform, you should choose option A:\nA. Create a custom role with view-only project permissions. Add the user's account to the custom role.\nExplanation:\n- Creating a custom role allows you to define specific permissions tailored to your needs, in this case, view-only access to project items.\n- By selecting the necessary read-only project permissions for the custom role, you can provide the auditor with the appropriate level of access without allowing modifications.\n- Adding the user's account to this custom role will grant them the specified permissions.\nOption B refers to \"view-only service permissions,\" which may not provide the desired level of access to all project items.\nOptions C and D suggest using built-in roles, but they may have more permissions than needed for a read-only auditor role. Custom roles offer a more precise approach for achieving the specified permissions."
      },
      {
        "index": 9,
        "text": "Anonymous: jayjani66 1Â year, 12Â months ago\nAnswer is C. Select the built-in IAM project Viewer role. Add the user's account to this role.\nThe IAM project Viewer role is a built-in role in Google Cloud that provides read-only access to all resources within a project. This role allows users to view project items, configurations, and metadata but does not grant any permission to modify or make changes to the resources."
      },
      {
        "index": 10,
        "text": "Anonymous: trainingexam 2Â years ago\nSelected Answer: A\nwith principle of leastprivilege should be A\nAlso, question is asking to set permission on single project. Basic principles grants permissions on all project."
      }
    ]
  },
  {
    "id": 307,
    "source": "examtopics",
    "question": "You are operating a Google Kubernetes Engine (GKE) cluster for your company where different teams can run non-production workloads. Your Machine Learning\n(ML) team needs access to Nvidia Tesla P100 GPUs to train their models. You want to minimize effort and cost. What should you do?",
    "options": {
      "A": "Ask your ML team to add the ×’â‚¬accelerator: gpu×’â‚¬ annotation to their pod specification.",
      "B": "Recreate all the nodes of the GKE cluster to enable GPUs on all of them.",
      "C": "Create your own Kubernetes cluster on top of Compute Engine with nodes that have GPUs. Dedicate this cluster to your ML team.",
      "D": "Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: John_Iam Highly Voted 4Â years, 7Â months ago\nD is the correct answer.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/gpus tablet444 4Â years, 5Â months ago\nthe documentation states \"Limitations\nBefore using GPUs on GKE, keep in mind the following limitations:\nYou cannot add GPUs to existing node pools.\nGPU nodes cannot be live migrated during maintenance events.\" nightflyer 4Â years, 1Â month ago\nIn this case it is about adding a GPU enabled node pool not a GPU to an existing node-pool fragment137 2Â years, 1Â month ago\nYou're correct that D says that, except that the question also says to use the most cost-effective method. Two node-pools would be more expensive than rebuilding the current one with GPU enabled. Gulithor 2Â years ago\nIt also says to minimize effort, wouldn't recreating all the pools take way longer than just adding 1?"
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 4Â years, 3Â months ago\nD. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke -accelerator: nvidia-tesla-p100 nodeSelector to their pod specification."
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nThe correct answer is D"
      },
      {
        "index": 4,
        "text": "Anonymous: trainingexam 1Â year, 6Â months ago\nSelected Answer: D\nD is the correct answer."
      },
      {
        "index": 5,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: D\nOption D is good"
      },
      {
        "index": 6,
        "text": "Anonymous: sakdip66 1Â year, 9Â months ago\nSelected Answer: D\nCreating new node pool w/ GPU-enabled instances is cost - saving solution. This way ML team workload will GPU instance for their ML and other team workload will run smoothly"
      },
      {
        "index": 7,
        "text": "Anonymous: Prat25200607 1Â year, 9Â months ago\nSelected Answer: D\nD makes more cost effective"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: D\nAnswer D. Add a new, GPU-enabled, node pool to the GKE cluster. Ask your ML team to add the cloud.google.com/gke-accelerator: nvidia-tesla-p100 nodeSelector to their pod specification.\nAdding a new node pool with GPUs is the best option because it allows for a separate set of nodes that can be specifically allocated to workloads that require GPU acceleration, such as the Machine Learning (ML) team's workloads. This approach will not affect other workloads running on the original nodes, keeping the costs low and the overall cluster performance stable."
      },
      {
        "index": 9,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: D\nD is the correct answer."
      },
      {
        "index": 10,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nD is correct"
      }
    ]
  },
  {
    "id": 308,
    "source": "examtopics",
    "question": "Your VMs are running in a subnet that has a subnet mask of 255.255.255.240. The current subnet has no more free IP addresses and you require an additional\n10 IP addresses for new VMs. The existing and new VMs should all be able to reach each other without additional routes. What should you do?",
    "options": {
      "A": "Use gcloud to expand the IP range of the current subnet.",
      "B": "Delete the subnet, and recreate it using a wider range of IP addresses.",
      "C": "Create a new project. Use Shared VPC to share the current network with the new project.",
      "D": "Create a new subnet with the same starting IP but a wider range to overwrite the current subnet."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: JustLearning Highly Voted 5Â years, 1Â month ago\nA: Expand the existing subnet.\nhttps://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/expand-ip-range"
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 4Â years, 9Â months ago\nA. Use gcloud to expand the IP range of the current subnet."
      },
      {
        "index": 3,
        "text": "Anonymous: JB28 Most Recent 1Â year, 6Â months ago\nThe correct answer is A. Use gcloud to expand the IP range of the current subnet.\nExpanding the primary IPv4 address range of a subnet does not cause a break or gap in network connectivity2. DHCP leases are not broken. IP addresses of running VMs at the time of the expansion do not change. You cannot â€œun-expandâ€ or contract the range after itâ€™s expanded. Expansion is permanent."
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is correct , as you need to expand it"
      },
      {
        "index": 6,
        "text": "Anonymous: trainingexam 2Â years ago\nSelected Answer: A\ngcloud compute networks subnets expand-ip-range NAME --prefix-length=PREFIX_LENGTH [--region=REGION] [GCLOUD_WIDE_FLAG â€¦]"
      },
      {
        "index": 7,
        "text": "Anonymous: dasgcp 2Â years, 3Â months ago\nSelected Answer: C\nYou can't expand the subnet because the question states \"The current subnet has no more free IP addresses\", correct answer is C. KerolesKhalil 2Â years, 1Â month ago\nthat's exactly why you expand the subnet =D"
      },
      {
        "index": 8,
        "text": "Anonymous: Jimut 2Â years, 4Â months ago\nSelected Answer: A\nUse gcloud to expand the IP range of the current subnet."
      },
      {
        "index": 9,
        "text": "Anonymous: Mobin92 2Â years, 4Â months ago\nSelected Answer: A\nA. Use gcloud to expand the IP range of the current subnet."
      },
      {
        "index": 10,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: A\nAnswer A (Use gcloud to expand the IP range of the current subnet): This option is correct because it allows you to expand the primary IP range of the existing subnet to accommodate the additional 10 IP addresses required for the new VMs. This can be done without deleting or recreating the subnet, which saves time and avoids disrupting any existing resources that are using the subnet."
      }
    ]
  },
  {
    "id": 309,
    "source": "examtopics",
    "question": "Your organization uses G Suite for communication and collaboration. All users in your organization have a G Suite account. You want to grant some G Suite users access to your Cloud Platform project. What should you do?",
    "options": {
      "A": "Enable Cloud Identity in the GCP Console for your domain.",
      "B": "Grant them the required IAM roles using their G Suite email address.",
      "C": "Create a CSV sheet with all users' email addresses. Use the gcloud command line tool to convert them into Google Cloud Platform accounts.",
      "D": "In the G Suite console, add the users to a special group called cloud-console-users@yourdomain.com. Rely on the default behavior of the Cloud Platform to grant users access if they are members of this group."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: austinl Highly Voted 4Â years, 7Â months ago\nB is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: Ciumela Highly Voted 4Â years, 7Â months ago\nB is correct: To actively adopt the Organization resource, the G Suite or Cloud Identity super admins need to assign the Organization Administrator Cloud IAM role to a user or group"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nThe correct answer is B"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: B\nB seems more correct as per thhe google pratcices"
      },
      {
        "index": 5,
        "text": "Anonymous: trainingexam 1Â year, 6Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: zwwdplay 1Â year, 9Â months ago\nDear friends,\nGreat responses in question.\nCan someone with contributor access, send me the remaining questions to this email: zwwdplay@hotmail.com"
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: B\nAnswer B. Grant them the required IAM roles using their G Suite email address.\nTo grant G Suite users access to a Cloud Platform project, you should use their G Suite email addresses to grant them the required IAM roles.\nAnswer A is incorrect because enabling Cloud Identity is not necessary for granting G Suite users access to a Cloud Platform project. Cloud Identity provides a centralized identity management system for G Suite and Cloud Platform, but it is not required for this use case.\nAnswer C is incorrect because there is no need to convert G Suite email addresses into Google Cloud Platform accounts. G Suite users already have Google accounts and can be granted access to Cloud Platform using their G Suite email addresses.\nAnswer D is incorrect because there is no default behavior in the Cloud Platform to grant access to users who are members of a particular group. Access to Cloud Platform resources is granted based on IAM roles and policies, not group membership."
      },
      {
        "index": 8,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nSelected Answer: B\nGrant them the required IAM roles using their G Suite email address"
      },
      {
        "index": 9,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nB is right"
      },
      {
        "index": 10,
        "text": "Anonymous: haroldbenites 2Â years, 7Â months ago\nGo for B"
      }
    ]
  },
  {
    "id": 310,
    "source": "examtopics",
    "question": "You have a Google Cloud Platform account with access to both production and development projects. You need to create an automated process to list all compute instances in development and production projects on a daily basis. What should you do?",
    "options": {
      "A": "Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.",
      "B": "Create two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources.",
      "C": "Go to Cloud Shell and export this information to Cloud Storage on a daily basis.",
      "D": "Go to GCP Console and export this information to Cloud SQL on a daily basis."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: cloudenthu01 Highly Voted 4Â years, 6Â months ago\nA is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 4Â years, 3Â months ago\nA. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nA is the best"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is correct , first list , then acitvate it"
      },
      {
        "index": 5,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nA. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources."
      },
      {
        "index": 6,
        "text": "Anonymous: trainingexam 1Â year, 6Â months ago\nSelected Answer: A\nActivate each config and list the instances"
      },
      {
        "index": 7,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: A\nAnswer A is the correct answer.\nThe most straightforward way to list all compute instances in development and production projects is to use gcloud compute instances list command. However, since the account has access to both production and development projects, it's necessary to create two configurations with different project IDs.\nAnswer B is incorrect because gsutil is used for object storage operations and not compute instances. (DISTRACTOR)\nAnswers C and D are incorrect because they do not provide a straightforward solution for listing compute instances in multiple projects."
      },
      {
        "index": 9,
        "text": "Anonymous: Shwom 1Â year, 11Â months ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 10,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: A\nA is correct"
      }
    ]
  },
  {
    "id": 311,
    "source": "examtopics",
    "question": "You have a large 5-TB AVRO file stored in a Cloud Storage bucket. Your analysts are proficient only in SQL and need access to the data stored in this file. You want to find a cost-effective way to complete their request as soon as possible. What should you do?",
    "options": {
      "A": "Load data in Cloud Datastore and run a SQL query against it.",
      "B": "Create a BigQuery table and load data in BigQuery. Run a SQL query on this table and drop this table after you complete your request.",
      "C": "Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.",
      "D": "Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: mohdafiuddin Highly Voted 4Â years, 6Â months ago\nBreaking down the question into key points -"
      },
      {
        "index": 1,
        "text": "5-TB AVRO file stored in a Cloud Storage bucket."
      },
      {
        "index": 2,
        "text": "Analysts are proficient only in SQL"
      },
      {
        "index": 3,
        "text": "cost-effective way to complete their request as soon as possible\nA. ....Load data in Cloud Datastore... (Not Correct because Cloud Datastore is not a good option to run SQL Queries)\nB. ...Load data in BigQuery.... (Not Cost Effective because loading the data which is already present in the bucket into BigQuery again is expensive)\nC. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\n(This is the right answer as it meets all the requirements from the question)\nD. Create a Hadoop cluster and copy the AVRO file to NDFS by compressing it. Load the file in a hive table and provide access to your analysts so that they can run SQL queries.\n(Too roundabout and indirect. Not the right option) pondai 4Â years, 3Â months ago\nlistem this guy"
      },
      {
        "index": 2,
        "text": "Anonymous: Ciumela Highly Voted 5Â years ago\nC is correct: https://cloud.google.com/bigquery/external-data-sources"
      },
      {
        "index": 3,
        "text": "Anonymous: dck4893 Most Recent 1Â year, 3Â months ago\nD - The ONLY answer that provides data for the analysts is D. The question states that the analysts need access to the data and that they only know SQL (not that *you* only know SQL). The other 3 answers don't provide the data to analysts. You might have to fill in the blanks that you will pass it to them in a spreadsheet format, but that very likely won't satisfy their needs to query the data using SQL and at 5TB size, that isn't ideal. Therefore D is the only answer that satisfies the requirement."
      },
      {
        "index": 4,
        "text": "Anonymous: JB28 1Â year, 6Â months ago\nThe most cost-effective and efficient option would be Option C: Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request.\nThis approach allows you to query data directly from the AVRO file stored in the Cloud Storage bucket without having to load the data into BigQuery first. This saves both time and money as you are not charged for the storage of data within BigQuery. Plus, BigQuery is designed to be able to handle large datasets, making it a suitable choice for a 5-TB AVRO file. Your analysts, who are proficient in SQL, can easily work with BigQuery as it uses a SQL interface."
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC is the right answer , as"
      },
      {
        "index": 7,
        "text": "Anonymous: Neha_Pallavi 1Â year, 10Â months ago\nC. Create external tables in BigQuery that point to Cloud Storage buckets and run a SQL query on these external tables to complete your request."
      }
    ]
  },
  {
    "id": 312,
    "source": "examtopics",
    "question": "You need to verify that a Google Cloud Platform service account was created at a particular time. What should you do?",
    "options": {
      "A": "Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.",
      "B": "Filter the Activity log to view the Configuration category. Filter the Resource type to Google Project.",
      "C": "Filter the Activity log to view the Data Access category. Filter the Resource type to Service Account.",
      "D": "Filter the Activity log to view the Data Access category. Filter the Resource type to Google Project."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: John_Iam Highly Voted 5Â years, 1Â month ago\nCorrect Answer is A.\nFilter the Activity log to view the Configuration category. Filter the Resource type to Service Account. mlantonis 5Â years, 1Â month ago\nI agree A nmnm22 1Â year, 8Â months ago\nayyy its mlantonis pre-fame"
      },
      {
        "index": 2,
        "text": "Anonymous: shafiqeee1 Highly Voted 5Â years ago\nA - I reproduced in my project."
      },
      {
        "index": 3,
        "text": "Anonymous: peddyua Most Recent 12Â months ago\nSelected Answer: A\ngcloud iam service-accounts list --project=<PROJECT_ID> --format=\"value(email)\"\ngcloud iam service-accounts describe <SERVICE_ACCOUNT_EMAIL> --project=<PROJECT_ID> --format=\"value(createTime)\""
      },
      {
        "index": 4,
        "text": "Anonymous: pzacariasf7 1Â year, 4Â months ago\nSelected Answer: A\nA baby"
      },
      {
        "index": 5,
        "text": "Anonymous: kelliot 1Â year, 7Â months ago\nWhy is the correct answer almost always different from the community answers? PiperMe 1Â year, 5Â months ago\nFunskies! dck4893 1Â year, 3Â months ago\nIt could be a way for Google to identify people who have used this resource and invalidate their exam results. Best just not to click on the \"solution\" at all."
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is A"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nA is the correct answer, you can see it in configuration category"
      },
      {
        "index": 8,
        "text": "Anonymous: Neha_Pallavi 1Â year, 10Â months ago\nA. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account."
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: A\nAnswer A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.\nThe Activity log is the primary tool for viewing and analyzing activity within a Google Cloud project, including all Service Account-related activity. By filtering the Activity log to view the Configuration category and filtering the Resource type to Service Account, you can see when a Service Account was created, updated, or deleted, along with other related metadata such as the user who performed the action and the IP address from which the action was performed."
      },
      {
        "index": 10,
        "text": "Anonymous: nishant7290 2Â years, 6Â months ago\nSelected Answer: A\nCorrect Answer is A."
      }
    ]
  },
  {
    "id": 313,
    "source": "examtopics",
    "question": "You deployed an LDAP server on Compute Engine that is reachable via TLS through port 636 using UDP. You want to make sure it is reachable by clients over that port. What should you do?",
    "options": {
      "A": "Add the network tag allow-udp-636 to the VM instance running the LDAP server.",
      "B": "Create a route called allow-udp-636 and set the next hop to be the VM instance running the LDAP server.",
      "C": "Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag.",
      "D": "Add a network tag of your choice to the instance running the LDAP server. Create a firewall rule to allow egress on UDP port 636 for that network tag."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kishoredeena Highly Voted 5Â years, 1Â month ago\nOption C is the right one"
      },
      {
        "index": 2,
        "text": "Anonymous: cloudenthu01 Highly Voted 5Â years ago\nC is correct\nYou tag the instances ,then create ingress firewall rules to allow udp on desired port for target-tags name applied to instances"
      },
      {
        "index": 3,
        "text": "Anonymous: kelliot Most Recent 1Â year, 7Â months ago\nOption C, agree"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: C\nC is correct bcoz of ingress"
      },
      {
        "index": 6,
        "text": "Anonymous: Partha117 2Â years, 3Â months ago\nSelected Answer: C\nFirewall rule for ingress is correct"
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: C\nAnswer C is correct: Add a network tag of your choice to the instance. Create a firewall rule to allow ingress on UDP port 636 for that network tag.\nTo make sure that the LDAP server is reachable by clients over port 636 using UDP, you need to allow ingress traffic on that port. You can achieve this by adding a network tag to the instance running the LDAP server and then creating a firewall rule that allows ingress traffic on UDP port 636 for that network tag."
      },
      {
        "index": 8,
        "text": "Anonymous: ratnesh_uk01 2Â years, 6Â months ago\ncan anyone please suggest why D is not correct? thanks Buruguduystunstugudunstuy 2Â years, 5Â months ago\nAnswer D is incorrect because adding a network tag of your choice to the instance running the LDAP server and creating a firewall rule to allow egress traffic on UDP port 636 for that network tag would not allow incoming traffic on that port. You need to create a firewall rule that allows ingress traffic on that port. rs7745 1Â year, 3Â months ago\negress!"
      },
      {
        "index": 9,
        "text": "Anonymous: leogor 2Â years, 8Â months ago\nSelected Answer: C\nallow ingress"
      },
      {
        "index": 10,
        "text": "Anonymous: AzureDP900 3Â years ago\nC is right."
      }
    ]
  },
  {
    "id": 314,
    "source": "examtopics",
    "question": "You need to set a budget alert for use of Compute Engineer services on one of the three Google Cloud Platform projects that you manage. All three projects are linked to a single billing account. What should you do?",
    "options": {
      "A": "Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.",
      "B": "Verify that you are the project billing administrator. Select the associated billing account and create a budget and a custom alert.",
      "C": "Verify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.",
      "D": "Verify that you are project administrator. Select the associated billing account and create a budget and a custom alert."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: kishoredeena Highly Voted 4Â years, 7Â months ago\nI think the answer is A, You can rely on default alert. No need for custom alert alan9999 4Â years, 3Â months ago\nRight its not asking to set custom alert Eshkrkrkr 4Â years, 2Â months ago\nOne point - there is no such role as Project Billing Administrator - it should be Project Billing Manager but he can't create budgets, the only one who can - Billing Account Administrator. Nor Project Administrator exists. Very tricky question, maybe the option a wrong, hope smb will catch it on exam and pass some light on real variants.\nhttps://cloud.google.com/iam/docs/understanding-roles#billing-roles Wachy 4Â years ago\nEshkrkrkr read the question calmly. The role there is Billing Administrator. Not Project Billing Administrator.\nIt's more like: â€œVerify you are the project; billing administratorâ€ ryumada 2Â years, 5Â months ago\nmore like: \"â€œVerify you are the project's billing administratorâ€\" BobbyFlash 3Â years, 3Â months ago\nI agree. If I'm not wrong, project admin doesn't have billing permissions so C and D discarded. Between A and B, option B looks like it works but we would be creating a budget and alert receiving info about billing as a whole; so A delimits billing for the project you want to get info from."
      },
      {
        "index": 2,
        "text": "Anonymous: Ciumela Highly Voted 4Â years, 7Â months ago\nA is correct, as you can set a default alert also on a single project: https://cloud.google.com/billing/docs/how-to/budgets nickyshil 2Â years, 5Â months ago\nwhy nobody is talking about \"set a budget alert for use of Compute Engineer services\" only.. why not custom alert ?how default alert ? Romio2023 1Â year, 1Â month ago\nIs that not a typo? \"*Compute Engine\" mwwoodm 4Â years, 4Â months ago\nAgreed. Per the link included: \"To create a budget for your Cloud Billing account, you must be a Billing Account Administrator on the Cloud Billing account.\" So that eliminates C & D. Then no need for custom alert, eliminating B. The answer is A."
      },
      {
        "index": 3,
        "text": "Anonymous: AdelElagawany Most Recent 3Â months, 3Â weeks ago\nSelected Answer: A\nI will choose A since it is the closest however I believe none of the four options is correct.\nThe four options mention \"create a budget\" however as per [1], to create a budget for your Cloud Billing account, you need a role that includes the following permissions on the Cloud Billing account:"
      },
      {
        "index": 1,
        "text": "billing.budgets.create to create a new budget."
      },
      {
        "index": 3,
        "text": "billing.budgets.list\nTo gain these permissions using a predefined role, ask your administrator to grant you one of the following Cloud Billing IAM roles on your Cloud Billing account:\nBilling Account Administrator\nBilling Account Costs Manager\nI'm assuming in A and B, it meant by \"project billing administrator\", the \"billing account administrator\" role so I will go for (A).\n[1] https://cloud.google.com/billing/docs/how-to/budgets#create-budget"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: A\nA is correct because, there is default alert, no need oof custom alert"
      },
      {
        "index": 6,
        "text": "Anonymous: Neha_Pallavi 1Â year, 4Â months ago\nA. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project."
      },
      {
        "index": 7,
        "text": "Anonymous: jayjani66 1Â year, 6Â months ago\nAnswer is B.\nExplanation: In Google Cloud, budget alerts are associated with billing accounts, not individual projects. Since all three projects are linked to a single billing account, you need to be the billing administrator to set up a budget and alert for that billing account."
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: A\nAnswer A. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.\nIn this scenario, you need to create a budget alert for the use of Compute Engine services on a specific project. Since all three projects are linked to a single billing account, you need to make sure that you are the billing administrator for that account. Once verified, you can create a budget and alert for the specific project by selecting the associated billing account and setting the budget and alert for the appropriate project. Buruguduystunstugudunstuy 1Â year, 11Â months ago\nINCORRECT\nAnswer B is incorrect because a custom alert is not necessary for this scenario. A budget alert alone is sufficient to notify you when your spending reaches a certain threshold.\nAnswer C is incorrect because, while a project administrator can create a budget for the project, they cannot set a budget alert. Only a billing administrator has the necessary permissions to create a budget alert.\nAnswer D is incorrect because a project administrator cannot create a custom alert on the associated billing account. Custom alerts can only be created by billing administrators."
      }
    ]
  },
  {
    "id": 315,
    "source": "examtopics",
    "question": "You are migrating a production-critical on-premises application that requires 96 vCPUs to perform its task. You want to make sure the application runs in a similar environment on GCP. What should you do?",
    "options": {
      "A": "When creating the VM, use machine type n1-standard-96.",
      "B": "When creating the VM, use Intel Skylake as the CPU platform.",
      "C": "Create the VM using Compute Engine default settings. Use gcloud to modify the running instance to have 96 vCPUs.",
      "D": "Start the VM using Compute Engine default settings, and adjust as you go based on Rightsizing Recommendations."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 5Â years, 7Â months ago\nA is correct - https://cloud.google.com/compute/docs/machine-types Ahmed_Y 2Â years, 5Â months ago\nIndeed, there is a n1-standard-96 machine type in the machine types list here https://cloud.google.com/compute/docs/general-purpose-machines"
      },
      {
        "index": 2,
        "text": "Anonymous: glam Highly Voted 5Â years, 3Â months ago\nA. When creating the VM, use machine type n1-standard-96."
      },
      {
        "index": 3,
        "text": "Anonymous: Enamfrancis Most Recent 1Â year, 3Â months ago\nSelected Answer: A\nwhy B ?\nOption A is the most straightforward and reliable way to ensure your application runs in a similar environment with the required 96 vCPUs"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 5,
        "text": "Anonymous: Rajeshpaspi 2Â years, 3Â months ago\nA is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: A\nA is correct, use machine type n1-standard-96 while creating the VM"
      },
      {
        "index": 7,
        "text": "Anonymous: sakdip66 2Â years, 9Â months ago\nSelected Answer: A\nthe goal is to have an equivalent of this app in GCP. therefore A is the best shot we have"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 11Â months ago\nSelected Answer: A\nA. When creating the VM, use machine type n1-standard-96.\nAnswer A is the correct answer as it directly addresses the requirement to have 96 vCPUs by selecting the n1-standard-96 machine type. This machine type offers 96 vCPUs, 360 GB of memory, and up to 2,400 GB of local SSD storage.\nhttps://cloud.google.com/compute/docs/machine-resource\nAnswer B is incorrect because selecting a CPU platform alone will not guarantee the availability of the required number of vCPUs.\nAnswer C is incorrect because it is not possible to modify a running Compute Engine instance to add vCPUs. vCPUs can only be added or removed during instance creation or by stopping the instance first.\nAnswer D is incorrect because while Rightsizing Recommendations can help optimize compute resources, they will not guarantee that the application has the required 96 vCPUs to function properly."
      },
      {
        "index": 9,
        "text": "Anonymous: cslince 3Â years, 1Â month ago\nSelected Answer: A\nA is correct - https://cloud.google.com/compute/docs/machine-types"
      },
      {
        "index": 10,
        "text": "Anonymous: fragment137 3Â years, 1Â month ago\nthe instance name for 96 vcpu N1 is \"n1-highcpu-96\", not n1-standard-96.\nPossible that has been updated since this question came out?"
      }
    ]
  },
  {
    "id": 316,
    "source": "examtopics",
    "question": "You want to configure a solution for archiving data in a Cloud Storage bucket. The solution must be cost-effective. Data with multiple versions should be archived after 30 days. Previous versions are accessed once a month for reporting. This archive data is also occasionally updated at month-end. What should you do?",
    "options": {
      "A": "Add a bucket lifecycle rule that archives data with newer versions after 30 days to Coldline Storage.",
      "B": "Add a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage.",
      "C": "Add a bucket lifecycle rule that archives data from regional storage after 30 days to Coldline Storage.",
      "D": "Add a bucket lifecycle rule that archives data from regional storage after 30 days to Nearline Storage."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: neelesh88 Highly Voted 5Â years, 1Â month ago\nB is correct"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 10Â months ago\nCorrect Answer (B):\nNumberOfNewerVersions\nThe NumberOfNewerVersions condition is typically only used in conjunction with Object Versioning. If the value of this condition is set to N, an object version satisfies the condition when there are at least N versions (including the live version) newer than it. For a live object version, the number of newer versions is considered to be 0. For the most recent noncurrent version, the number of newer versions is 1 (or 0 if there is no live object version), and so on.\nImportant: When specifying this condition in a .json configuration file, you must use numNewerVersions instead of NumberOfNewerVersions.\nhttps://cloud.google.com/storage/docs/lifecycle#numberofnewerversions"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 8Â months ago\nThe correct answer is B"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: B\nB is the right answer, because of data is accessing infrequently and nearline storage is good for it"
      },
      {
        "index": 5,
        "text": "Anonymous: SanjeevKumar1983 1Â year, 10Â months ago\nSelected Answer: B\nB is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: jayjani66 1Â year, 12Â months ago\nCorrect ans is A.\nExplanation: In this scenario, you need to archive data after 30 days, which implies that the data with multiple versions is considered for archiving. Since you need to access previous versions once a month for reporting, using Coldline Storage is the most cost-effective option. omunoz 1Â year, 5Â months ago\nRetrieval fees is more expensive in Coldline.\nStandard storage Nearline storage Coldline storage Archive storage\n$0 per GB $0.01 per GB $0.02 per GB $0.05 per GB"
      },
      {
        "index": 7,
        "text": "Anonymous: Partha117 2Â years, 3Â months ago\nSelected Answer: B\nsince accessed frequently it will be nearline kelliot 1Â year, 7Â months ago\nthe logic is simple :) i agree with you"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 5Â months ago\nSelected Answer: B\nAnswer B, adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, is the correct answer for this scenario.\nNearline Storage is designed for data that is accessed less frequently, such as for backup and archival purposes. It has a minimum storage duration of 30 days, which makes it suitable for archiving data that needs to be kept for a long time but is accessed infrequently. Additionally, Nearline Storage has lower storage costs than Coldline Storage, making it more cost-effective for this use case.\nBy adding a bucket lifecycle rule that archives data with newer versions after 30 days to Nearline Storage, you can ensure that the data is automatically moved to a more cost-effective storage class while still being easily accessible for reporting purposes. dnur 2Â years, 3Â months ago\nYou're incorrect. Coldline storage has a lower costs that Nearline Storage. https://cloud.google.com/storage/docs/storage-classes. eaakgul 2Â years, 3Â months ago\nThe question tells us that the previous versions are accessed once a month for reporting. So, nearline makes more sense in this case. 'Buruguduystunstugudunstuy' has mentioned that nearline has lower storage costs for only this 'use case' chikorita 2Â years, 3Â months ago\njust FYI that my lord, @Buruguduystunstugudunstuy, is always right!!!!1"
      },
      {
        "index": 9,
        "text": "Anonymous: leogor 2Â years, 8Â months ago\nSelected Answer: B\narchives data with newer versions after 30 days to Nearline Storage."
      },
      {
        "index": 10,
        "text": "Anonymous: kadc 2Â years, 10Â months ago\nSelected Answer: B\nB should be correct:\nNearline has min storage of 30 days, while Coldline has 90 days.\nSince \"archive data is also occasionally updated at month-end\", updating object before min storage period is allowed but causes early deletion fees as if the object was stored for the min duration, so using Coldline will always charge for 90 days and not likely to save cost.\nhttps://cloud.google.com/storage/pricing#early-delete"
      }
    ]
  },
  {
    "id": 317,
    "source": "examtopics",
    "question": "Your company's infrastructure is on-premises, but all machines are running at maximum capacity. You want to burst to Google Cloud. The workloads on Google\nCloud must be able to directly communicate to the workloads on-premises using a private IP range. What should you do?",
    "options": {
      "A": "In Google Cloud, configure the VPC as a host for Shared VPC.",
      "B": "In Google Cloud, configure the VPC for VPC Network Peering.",
      "C": "Create bastion hosts both in your on-premises environment and on Google Cloud. Configure both as proxy servers using their public IP addresses.",
      "D": "Set up Cloud VPN between the infrastructure on-premises and Google Cloud."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SIX Highly Voted 4Â years, 7Â months ago\nI believe D is the right answer dan80 4Â years, 7Â months ago\nB is correct - https://cloud.google.com/solutions/best-practices-vpc-design . this answer also on all machines are running at maximum capacity. JustLearning 4Â years, 7Â months ago\nvpc network peering does not connect to on-prem. Cloud VPN is the correct solution. https://cloud.google.com/vpn/docs/concepts/overview mlantonis 4Â years, 7Â months ago\nYou need VPN, so D is the correct. VPC network peering is between VPCs. xharf 2Â years, 5Â months ago\n\"Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of whether they belong to the same project or the same organization.\"\nhttps://cloud.google.com/vpc/docs/vpc-peering\nwhile\n\"Cloud Interconnect provides low latency, high availability connections that enable you to reliably transfer data between your on-premises and Google Cloud Virtual Private Cloud (VPC) networks.\"\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview\nand\n\"HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network through an IPsec VPN connection in a single region.\"\nhttps://cloud.google.com/network-connectivity/docs/vpn/concepts/overview\nso, cloud vpn is the best answer for the question requirement"
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (D):\nAccess internal IPs directly\nYour VPC network's internal (RFC 1918) IP addresses are directly accessible from your on-premises network with peering, no NAT device or VPN tunnel required.\nHybrid made easy\nTodayâ€™s business climate demands flexibility. Connecting your on-premises resources to your cloud resources seamlessly, with minimum latency or interruption, is a business-critical requirement. The speed and reliability of Cloud Interconnect lets you extend your organizationâ€™s data center network into Google Cloud, simply and easily, while options such as Cloud VPN provide flexibility for all your workloads. This unlocks the potential of hybrid app development and all the benefits the cloud has to offer.\nIn the graphic below: What GCP Connection is right for you? shows clearly what is the method for extend your on premise network (IP Private communication).\nWhat GCP Connection is right for you?\nhttps://cloud.google.com/hybrid-connectivity"
      },
      {
        "index": 3,
        "text": "Anonymous: BAofBK Most Recent 1Â year, 2Â months ago\nThe correct answer is D"
      },
      {
        "index": 4,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: D\nD is the right answer as they need the private range and the machine are also on high working load"
      },
      {
        "index": 5,
        "text": "Anonymous: rosapersiani 1Â year, 7Â months ago\nSelected Answer: D\nd is right"
      },
      {
        "index": 6,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: D\nVPN to connect your on-premise network to the cloud"
      },
      {
        "index": 7,
        "text": "Anonymous: Partha117 1Â year, 10Â months ago\nSelected Answer: D\nVPN for on premise connection to GCP"
      },
      {
        "index": 8,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: D\nAnswer D. Set up Cloud VPN between the infrastructure on-premises and Google Cloud.\nTo burst into Google Cloud from the on-premises infrastructure, a VPN connection can be established between the on-premises network and Google Cloud. VPN provides a secure, private tunnel to transfer data between on-premises infrastructure and Google Cloud. Cloud VPN would allow workloads on Google Cloud to communicate with workloads on-premises over private IP addresses, making it a suitable option for this scenario.\nAnswer A (Shared VPC) and Answer B (VPC Network Peering) do not address the requirement of communicating over a private IP range between on-premises and Google Cloud.\nAnswer C (bastion hosts) involves the use of public IP addresses, which may not be suitable for a private, secure connection."
      },
      {
        "index": 9,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: D\nD is the right answer"
      },
      {
        "index": 10,
        "text": "Anonymous: leogor 2Â years, 2Â months ago\nSelected Answer: D\nCloud VPN"
      }
    ]
  },
  {
    "id": 318,
    "source": "examtopics",
    "question": "You want to select and configure a solution for storing and archiving data on Google Cloud Platform. You need to support compliance objectives for data from one geographic location. This data is archived after 30 days and needs to be accessed annually. What should you do?",
    "options": {
      "A": "Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.",
      "B": "Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
      "C": "Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage.",
      "D": "Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: SIX Highly Voted 4Â years, 7Â months ago\nD\nGoogle Cloud Coldline is a new cold-tier storage for archival data with access frequency of less than once per year. Unlike other cold storage options, Nearline has no delays prior to data access, so now it is the leading solution among competitors. JustLearning 4Â years, 7Â months ago\nD is correct. Coldline is a better choice. dan80 4Â years, 7Â months ago\nC is correct - This data is archived after 30 days - Nearline Storage 30 days , Coldline Storage 90 days https://cloud.google.com/storage/docs/storage-classes mlantonis 4Â years, 7Â months ago\ndan80 is right y2kniel 4Â years, 3Â months ago\nD, It is saying AFTER 30 days. We should use coldline storage Phat 9Â months, 4Â weeks ago\nnot really, the key word is: annual access gh999l 4Â years, 2Â months ago\nyou have misunderstood minimum storage period here, nearline storage class minimum you have to plan for 30 days Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: ESP_SAP Highly Voted 4Â years, 5Â months ago\nCorrect Answer is (D):\nhttps://cloud.google.com/storage/docs/storage-classes\nNearline Storage\nNearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs.\nNearline Storage is ideal for data you plan to read or modify on average once per month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline Storage is a great choice.\nNearline Storage is also appropriate for data backup, long-tail multimedia content, and data archiving. Note, however, that for data accessed less frequently than once a quarter, Coldline Storage or Archive Storage are more cost-effective, as they offer lower storage costs.\nhttps://cloud.google.com/storage/docs/storage-classes#nearline ESP_SAP 4Â years, 4Â months ago\nCORRECTION.\nCorrect Answer is (D):\nThe Real description is about Coldline storage Class:\nColdline Storage\nColdline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs.\nColdline Storage is ideal for data you plan to read or modify at most once a quarter. Note, however, that for data being kept entirely for backup or archiving purposes, Archive Storage is more cost-effective, as it offers the lowest storage costs.\nhttps://cloud.google.com/storage/docs/storage-classes#coldline"
      },
      {
        "index": 3,
        "text": "Anonymous: kelliot Most Recent 1Â year, 1Â month ago\nD is correct.\n\"from one geographic location\" clues the answer"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is D"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: D\nD is the correrct answer, as the data in access only once a year"
      },
      {
        "index": 6,
        "text": "Anonymous: sabrinakloud 1Â year, 9Â months ago\nSelected Answer: D\n\"This data is archived after 30 days and needs to be accessed annually\"\nideally archive; coldine is the closest."
      },
      {
        "index": 7,
        "text": "Anonymous: Elya 1Â year, 9Â months ago\nThe best option would be to select Regional Storage and add a bucket lifecycle rule that archives data after 30 days to Nearline Storage. Nearline Storage is designed for data that is accessed less frequently, but still needs to be readily available when accessed. It has a lower storage cost than Regional Storage, and retrieval costs are lower than those of Coldline Storage."
      },
      {
        "index": 8,
        "text": "Anonymous: inbalinbal 1Â year, 9Â months ago\nSelected Answer: D\nD is correct"
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 11Â months ago\nSelected Answer: D\nAnswer D is the CORRECT answer. The scenario mentioned in the question requires archiving data after 30 days and accessing it annually. As per the Cloud Storage documentation, Coldline storage is ideal for data that is accessed at most once a quarter. Hence, selecting regional storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage is the best solution to meet the compliance objectives and cost-effectiveness requirements. Buruguduystunstugudunstuy 1Â year, 11Â months ago\nINCORRECT:\nAnswer A, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Coldline Storage, is not a good fit for this scenario because Multi-Regional Storage is more expensive than Regional Storage and it does not provide a clear advantage for this use case.\nAnswer B, selecting Multi-Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is also not the best option because Nearline Storage is more appropriate for data that is accessed less than once a month, while in this scenario, the data needs to be accessed at least once a year.\nAnswer C, selecting Regional Storage and adding a bucket lifecycle rule that archives data after 30 days to Nearline Storage, is not ideal because Nearline Storage is more suitable for data that is accessed less than once a month. If the data is accessed only once a year, it might be more cost-effective to choose Coldline Storage instead."
      },
      {
        "index": 10,
        "text": "Anonymous: cslince 2Â years, 1Â month ago\nSelected Answer: D\nThe answer is D."
      }
    ]
  },
  {
    "id": 319,
    "source": "examtopics",
    "question": "Your company uses BigQuery for data warehousing. Over time, many different business units in your company have created 1000+ datasets across hundreds of projects. Your CIO wants you to examine all datasets to find tables that contain an employee_ssn column. You want to minimize effort in performing this task.\nWhat should you do?",
    "options": {
      "A": "Go to Data Catalog and search for employee_ssn in the search box.",
      "B": "Write a shell script that uses the bq command line tool to loop through all the projects in your organization.",
      "C": "Write a script that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column.",
      "D": "Write a Cloud Dataflow job that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find employee_ssn column."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 1Â month ago\nIts A."
      },
      {
        "index": 2,
        "text": "Anonymous: filco72 Highly Voted 4Â years, 12Â months ago\nCorrect is A.\nI tested on my account following this procedure: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui?authuser=4\nI created a data set and through Data Catalog I easily and effortlessly searched for the column name \"gender\""
      },
      {
        "index": 3,
        "text": "Anonymous: JB28 Most Recent 1Â year, 6Â months ago\nThe best option to minimize effort in performing this task is A. Go to Data Catalog and search for employee_ssn in the search box.\nGoogle Cloudâ€™s Data Catalog is a fully managed and scalable metadata management service that empowers organizations to quickly discover, manage, and understand all their data in Google Cloud. It offers a simple and easy-to-use search interface for data discovery. By searching for â€œemployee_ssnâ€ in the Data Catalog, you can quickly find all tables across all datasets and projects that contain this column. This approach is more efficient and requires less effort compared to writing and maintaining scripts or jobs.\nPlease note that access to Data Catalog and the visibility of datasets, tables, and columns are subject to permissions and roles in IAM policy. Make sure you have the necessary permissions to view the metadata."
      },
      {
        "index": 4,
        "text": "Anonymous: ogerber 1Â year, 7Â months ago\nSelected Answer: A\nData Catalog lets you search and tag entries such as BigQuery tables with metadata. Some examples of metadata that you can use for tagging include public and private tags, data stewards, and rich text overview.\nhttps://cloud.google.com/data-catalog/docs/tag-bigquery-dataset"
      },
      {
        "index": 5,
        "text": "Anonymous: Vik96 1Â year, 8Â months ago\nI am preparing for the GCP-ACE exam, I was able to access 96 questions only, if anyone has the entire questions please share them with my vittoriaprovenza@tiscali.it address. I have exam on next week,pls share Thanks in advance. I would be forever grateful."
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nI will go for A"
      },
      {
        "index": 7,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is A"
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 1Â year, 10Â months ago\nSelected Answer: A\nAnswer is A, as it requires the less effort and other options are more time consuming and error prone"
      },
      {
        "index": 9,
        "text": "Anonymous: deadsong 2Â years, 2Â months ago\nThe most efficient approach to identify tables that contain an employee_ssn column in BigQuery would be to query the INFORMATION_SCHEMA.COLUMNS view, which provides metadata about all columns in all tables in a given dataset. Therefore, options C and D are both possible solutions.\nOption A, searching for the column name in Data Catalog, may not be efficient if there are too many datasets to search through manually.\nOption B, writing a shell script to loop through all the projects in your organization, may work, but it would require more effort and time than options C and D. Also, it would be more error-prone since the script would need to handle authentication and authorization, handle exceptions and errors, and collect the results.\nTherefore, options C and D are better choices, but option D, using Cloud Dataflow, might be overkill for this specific task. Option C, looping through all projects and querying INFORMATION_SCHEMA.COLUMNS view, is the simplest and most effective solution to minimize effort."
      },
      {
        "index": 10,
        "text": "Anonymous: Neeyo 2Â years, 2Â months ago\nHi All, I have my GCP ACE exam scheduled for tomorrow. However, I am only being able to access 96 questions. Can anyone kindly share the entire list of questions as I have hardly anytime left before my exam. oniyi6@yahoo.com. Thank you all so much arnika98 2Â years, 2Â months ago\nDid you pass the exam? If so any questions from here came? Please let us know so that it will be helpful"
      }
    ]
  },
  {
    "id": 320,
    "source": "examtopics",
    "question": "You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status:\n\nWhat is the most likely cause?",
    "options": {
      "A": "The pending Pod's resource requests are too large to fit on a single node of the cluster.",
      "B": "Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod.",
      "C": "The node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod.",
      "D": "The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node."
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ESP_SAP Highly Voted 5Â years, 5Â months ago\nCorrect Answer is (B):\nReasons for a Pod Status Pending:\nTroubleshooting Reason #1: Not enough CPU\nTroubleshooting Reason #2: Not enough memory\nTroubleshooting Reason #3: Not enough CPU and memory\nhttps://managedkube.com/kubernetes/k8sbot/troubleshooting/pending/pod/2019/02/22/pending-pod.html SSPC 5Â years, 5Â months ago\nI agree with you. The correct answer is B Linus11 4Â years, 8Â months ago\nThe real crux of this question is the mention about \"Pre-emptible Node pool\". That need to take into consider while determining the answer. If we choose B, then the importance of \"Pre-emptible node pool\" is not there. Whether the node pool is pre-emptible or not, resource scarcity can lead to pending pods.\nWhen we consider the mention of \"Pre-emptible Node Poll\" , then the answer is obviously D. if a pre-meptible Node get pre-empted there will be a delay in cluster to sync it.\nAnswer is D. alexgrig 4Â years, 3Â months ago\nQuestions says \"Single Node\" at that case the second pod can't be in running state. brvinod 3Â years, 11Â months ago\nA node can have multiple pods. So that is not a problem. MidhunJose 3Â years, 12Â months ago\nIt says a single node pool, not a single node. Meaning there can be multiple nodes, right? brvinod 3Â years, 11Â months ago\nPre-emptible would have been an issue if the cluster had more than one node. The question clearly states that it is a single node cluster. That means if that single VM was pre-empted, neither of the pods should have been running. Since one pod is running, that means that (the only) VM is running. So, the reason the second pod is still pending because the VM is not having enough resources to run both the pods. Hence B. mplibunao 3Â years, 7Â months ago\nActually the question stated \"single preemptible node pool\" and not \"single node\" so it's possible that there are multiple nodes and one of the node on which the pod was scheduled on was preempted Finger41 4Â years, 7Â months ago\nThis is to throw you off, when there is insufficient resources for a Pod to stand up, then the status will equal pending : https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#:~:text=If%20a%20Pod%20is%20stuck,be%20scheduled%20onto%20a%20node.&text=You%20don't%20have%20enough,new%20nodes%20to%20your%20cluster. Load full discussion..."
      },
      {
        "index": 2,
        "text": "Anonymous: cloudenthu01 Highly Voted 5Â years, 6Â months ago\nD is correct as the node on which pod was scheduled to run was preempted & now this pod is scheduled to run on different preemtible node from the node-pool myuniquename 4Â years, 3Â months ago\nIncorrect. There is a single preemtible instance, if it was preempted then both pods would show as 'Pending'. B is correct. ashtonez 2Â years, 10Â months ago\nNo, because one of the pods may run on another node that its still up obeythefist 3Â years, 10Â months ago\n> There is a single preemtible instance\nWhere does it say that? It doesn't. Don't make things up. There's a single pre-emptible node pool. A single pool is not the same as a single node."
      },
      {
        "index": 3,
        "text": "Anonymous: guaose Most Recent 2Â months, 2Â weeks ago\nSelected Answer: D\nYouâ€™re using a single preemptible node pool. Preemptible VMs in GKE can be terminated at any time (usually after 24 hours or earlier if Google needs the resources).\nIf the node running a Pod is preempted, the Pod becomes Pending while Kubernetes tries to reschedule it.\nSince thereâ€™s only one node pool, and itâ€™s preemptible, there may be no available node at the moment to reschedule the Pod, hence it stays in Pending."
      },
      {
        "index": 4,
        "text": "Anonymous: 85c887f 9Â months, 4Â weeks ago\nSelected Answer: B\nCorrect Answer is B. As one of the indicators of incorrect D option is 0 in Restart section. If D was correct answer we would see in Restart number > 0."
      },
      {
        "index": 5,
        "text": "Anonymous: navel 10Â months, 3Â weeks ago\nSelected Answer: B\nB is correct as 0 restarts of the pod"
      },
      {
        "index": 6,
        "text": "Anonymous: swktopic 10Â months, 3Â weeks ago\nSelected Answer: B\nrestart is 0, so B is correct"
      },
      {
        "index": 7,
        "text": "Anonymous: peddyua 12Â months ago\nSelected Answer: D\nPreemptible nodes can be terminated at any time, and if there are not enough available preemptible nodes at the moment to accommodate the pod, it will stay in Pending."
      },
      {
        "index": 8,
        "text": "Anonymous: 1e62a4f 1Â year, 1Â month ago\nSelected Answer: B\npreemptible node can be stopped anytime. It can be not available as long as long resources are not available."
      },
      {
        "index": 9,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: D\nThe most likely cause is given in the question. We have a single preemptible node pool."
      },
      {
        "index": 10,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nSelected Answer: B\n100% - not enough CPU / memory."
      }
    ]
  },
  {
    "id": 321,
    "source": "examtopics",
    "question": "You want to find out when users were added to Cloud Spanner Identity Access Management (IAM) roles on your Google Cloud Platform (GCP) project. What should you do in the GCP Console?",
    "options": {
      "A": "Open the Cloud Spanner console to review configurations.",
      "B": "Open the IAM & admin console to review IAM policies for Cloud Spanner roles.",
      "C": "Go to the Stackdriver Monitoring console and review information for Cloud Spanner.",
      "D": "Go to the Stackdriver Logging console, review admin activity logs, and filter them for Cloud Spanner IAM roles."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: samvegas Highly Voted 5Â years, 6Â months ago\nAnswer = D, I have simple rule; if metrics then Monitoring, if Auditing then Logging."
      },
      {
        "index": 2,
        "text": "Anonymous: Meix Highly Voted 5Â years, 7Â months ago\nI think the answer is D Anand2608 5Â years, 2Â months ago\nAs per the Cloud Audit logs documentation."
      },
      {
        "index": 3,
        "text": "Anonymous: ccpmad Most Recent 1Â year, 7Â months ago\nD, but in 2024 there is not stackdriver Logging... yomi95 1Â year, 2Â months ago\nstackdriver logging = cloud logging\nstackdriver monitoring = cloud monitoring"
      },
      {
        "index": 4,
        "text": "Anonymous: Vik96 2Â years, 2Â months ago\nI am preparing for the GCP-ACE exam, I was able to access 96 questions only, if anyone has the entire questions please share them with my vittoriaprovenza@tiscali.it address. I have exam on next week,pls share Thanks in advance. I would be forever grateful."
      },
      {
        "index": 5,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nI will go with D"
      },
      {
        "index": 6,
        "text": "Anonymous: gsmasad 2Â years, 2Â months ago\nSelected Answer: D\nD is the only option that talks about the admin activity"
      },
      {
        "index": 7,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD , seems more correct as it shows you the history also"
      },
      {
        "index": 8,
        "text": "Anonymous: sana_sree 2Â years, 7Â months ago\noption D is correct\nrefer:\nhttps://www.exam-answer.com/google/ace/question95"
      },
      {
        "index": 9,
        "text": "Anonymous: sonia_mola 2Â years, 9Â months ago\nSelected Answer: D\nAnswer is D"
      },
      {
        "index": 10,
        "text": "Anonymous: Buruguduystunstugudunstuy 2Â years, 10Â months ago\nSelected Answer: D\nAnswer A is incorrect because the Cloud Spanner console only shows configurations related to Cloud Spanner instances and databases, but not IAM roles.\nAnswer B is partially correct in that the IAM & admin console is where IAM policies can be viewed and edited. However, it does not show a history of when users were added to Cloud Spanner IAM roles.\nAnswer C is incorrect because Stackdriver Monitoring is used to monitor the performance of Google Cloud resources and applications, and does not provide information about IAM role changes.\nOverall, the best answer is D, as Stackdriver Logging provides a comprehensive history of all administrative activity logs, including when users were added to Cloud Spanner IAM roles."
      }
    ]
  },
  {
    "id": 322,
    "source": "examtopics",
    "question": "You are building a product on top of Google Kubernetes Engine (GKE). You have a single GKE cluster. For each of your customers, a Pod is running in that cluster, and your customers can run arbitrary code inside their Pod. You want to maximize the isolation between your customers' Pods. What should you do?",
    "options": {
      "A": "Use Binary Authorization and whitelist only the container images used by your customers' Pods.",
      "B": "Use the Container Analysis API to detect vulnerabilities in the containers used by your customers' Pods.",
      "C": "Create a GKE node pool with a sandbox type configured to gvisor. Add the parameter runtimeClassName: gvisor to the specification of your customers' Pods.",
      "D": "Use the cos_containerd image for your GKE nodes. Add a nodeSelector with the value cloud.google.com/gke-os-distribution: cos_containerd to the specification of your customers' Pods."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: akshaychavan7 Highly Voted 3Â years, 1Â month ago\nLet me be honest, I did not have any clue to answer this question. However, I spotted the keyword, 'isolation', from the question and a keyword, 'sandbox' from the answers and guessed the answer which turned out to be correct.\nSo, yes it is C!"
      },
      {
        "index": 2,
        "text": "Anonymous: Sac3433 Highly Voted 3Â years, 2Â months ago\nCorrect answer is C: You can enable GKE Sandbox on your cluster to isolate untrusted workloads in sandboxes on the node. GKE Sandbox is built using gVisor, an open source project: https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview?hl=en#protecting_nodes_from_untrusted_workloads"
      },
      {
        "index": 3,
        "text": "Anonymous: Cynthia2023 Most Recent 1Â year, 6Â months ago\nSelected Answer: C\ngVisor is a sandboxing technology that provides an additional layer of isolation between running containers. It's particularly useful in scenarios where containers might be running untrusted or arbitrary code, as it helps in mitigating the risk of kernel exploits.\nBy configuring a node pool with gVisor and specifying runtimeClassName: gvisor in the Pod specifications, each Pod is run within this sandboxed environment, thereby enhancing isolation between the Pods. Cynthia2023 1Â year, 6Â months ago\nA. Binary Authorization: While Binary Authorization is a security control that ensures only trusted container images are deployed on GKE, it doesn't provide isolation between running Pods. It's more about image integrity and compliance.\nB. Container Analysis API: This API is used for scanning container images for vulnerabilities. While important for security, it doesn't directly contribute to runtime isolation between Pods.\nD. Using cos_containerd Image: The Container-Optimized OS with containerd (cos_containerd) is a secure choice for the node image in GKE. However, it doesn't provide the same level of isolation for arbitrary code execution in Pods as gVisor. The nodeSelector parameter is used to schedule Pods on specific nodes but doesn't enhance inter-Pod isolation. Cynthia2023 1Â year, 6Â months ago\nâ€¢ Implementing gVisor can impact the performance of the containers due to the additional layer of abstraction. However, for scenarios requiring high security and isolation, particularly when running arbitrary code, the trade-off can be justified."
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 8Â months ago\nThe correct answer is C"
      },
      {
        "index": 5,
        "text": "Anonymous: lov75 2Â years, 7Â months ago\nSelected Answer: C\nC is correct"
      },
      {
        "index": 6,
        "text": "Anonymous: mattcl 2Â years, 8Â months ago\nGKE Sandbox https://cloud.google.com/kubernetes-engine/docs/concepts/sandbox-pods"
      },
      {
        "index": 7,
        "text": "Anonymous: theBestStudent 2Â years, 11Â months ago\nSelected Answer: C\nAs it has been mentioned already: https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods?hl=en\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods?hl=en#working_with"
      },
      {
        "index": 8,
        "text": "Anonymous: AzureDP900 3Â years ago\ngVisor is the way to isolate. Those who already preparing for CKS can answer this question without even thinking further. C is right"
      },
      {
        "index": 9,
        "text": "Anonymous: haroldbenites 3Â years, 1Â month ago\nGo for C"
      },
      {
        "index": 10,
        "text": "Anonymous: PAUGURU 3Â years, 2Â months ago\nSelected Answer: C\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/security-overview?hl=en#protecting_nodes_from_untrusted_workloads"
      }
    ]
  },
  {
    "id": 323,
    "source": "examtopics",
    "question": "Your customer has implemented a solution that uses Cloud Spanner and notices some read latency-related performance issues on one table. This table is accessed only by their users using a primary key. The table schema is shown below.\n\nYou want to resolve the issue. What should you do?",
    "options": {
      "A": "Remove the profile_picture field from the table.",
      "B": "Add a secondary index on the person_id column.",
      "C": "Change the primary key to not have monotonically increasing values.",
      "D": "Create a secondary index using the following Data Definition Language (DDL):"
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: BenKenGo6 Highly Voted 3Â years, 4Â months ago\nSelected Answer: D\nCreate a secondary index using the following Data Definition.\nIf we watch the next video, he talks about a change to monotonically when we insert rows.\nFinally when we talk about read and we have a perdormance issues, we must create a index.\nhttps://www.youtube.com/watch?v=r6uj0HMNQNQ SilNilanjan 2Â years, 11Â months ago\nAdding index for faster retrieval is a basic DBMS concept but why do we need the index on firstname and lastname as per D? iooj 1Â year, 4Â months ago\nright, it says: This table is accessed only by a primary key. So B should be the answer"
      },
      {
        "index": 2,
        "text": "Anonymous: Cynthia2023 Highly Voted 2Â years ago\nSelected Answer: C\nâ€¢ Hotspotting Issue: Cloud Spanner, like many distributed databases, can experience issues with what is known as \"hotspotting.\" This happens when a large portion of read or write operations are concentrated on a specific part of the database. In this case, the sequential nature of the person_id as a primary key can lead to hotspotting because new records are continually added at the \"end\" of the key space, creating a hot node which can cause performance bottlenecks.\nâ€¢ Monotonically Increasing Values: Monotonically increasing values as primary keys can exacerbate the hotspotting effect because each new entry is placed after the last, creating a write hotspot on the last node that handles the upper bound of the key range. Over time, this can lead to unbalanced read/write loads across the nodes. Cynthia2023 2Â years ago\nâ€¢ Alternatives to Monotonic Keys: To mitigate this, you can use a primary key that distributes writes more evenly across the key space. This could be achieved by using a UUID (Universally Unique Identifier) or sharding the monotonically increasing identifier by combining it with another value that has a more random distribution."
      },
      {
        "index": 3,
        "text": "Anonymous: Roman1988 Most Recent 1Â year ago\nSelected Answer: C\nIf the primary key is monotonically increasing (e.g., an incrementing sequence), all new rows are written to the same split. This causes the split to become a hotspot, leading to increased read and write latency because all operations are concentrated on a single node."
      },
      {
        "index": 4,
        "text": "Anonymous: iooj 1Â year, 4Â months ago\nSelected Answer: B\nWhy B: To improve read performance, we should add a secondary index on the primary key. Since we know that this table is accessed only by the primary key, this would optimize read operations.\nWhy not C: Monotonically increasing values might create a hotspot, but this would primarily affect write operations. In fact, lookups would be easier with sequential values compared to a UUID. Additionally, it is not mentioned that users are only reading the last inserted data, which would create a read hotspot.\nWhy not D: This approach could work, but since the table is accessed only by the primary key, there's no need to add additional fields to the index.\nWhy not A: The profile_picture field should not significantly affect read performance. iooj 1Â year, 4Â months ago\noh, just found that primary key is automatically indexed by default... so I would vote for A then, hahaha"
      },
      {
        "index": 5,
        "text": "Anonymous: pzacariasf7 1Â year, 10Â months ago\nSelected Answer: D\nD is correct for me"
      },
      {
        "index": 6,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is C"
      },
      {
        "index": 7,
        "text": "Anonymous: ziomek666 2Â years, 3Â months ago\nD makes no sense. It's C"
      },
      {
        "index": 8,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: C\nC seems more correct"
      },
      {
        "index": 9,
        "text": "Anonymous: dasgcp 2Â years, 9Â months ago\nHow is this a GCP question? Mariuselul 2Â years, 9Â months ago\nSpanner and distribution of primary key"
      },
      {
        "index": 10,
        "text": "Anonymous: temple1305 2Â years, 10Â months ago\nSelected Answer: C\nPK already has index by default, so not B. D - index by 3 fields. but users use person_id for acces, so D is wrong.\nSo C - because monotonically increasing fields is not good candidate for PK(because index degradetion)"
      }
    ]
  },
  {
    "id": 324,
    "source": "examtopics",
    "question": "Your finance team wants to view the billing report for your projects. You want to make sure that the finance team does not get additional permissions to the project. What should you do?",
    "options": {
      "A": "Add the group for the finance team to roles/billing user role.",
      "B": "Add the group for the finance team to roles/billing admin role.",
      "C": "Add the group for the finance team to roles/billing viewer role.",
      "D": "Add the group for the finance team to roles/billing project/Manager role."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: miniso8153 Highly Voted 4Â years, 7Â months ago\nc\n\"Billing Account Viewer access would usually be granted to finance teams, it provides access to spend information, but does not confer the right to link or unlink projects or otherwise manage the properties of the billing account.\"\nhttps://cloud.google.com/billing/docs/how-to/billing-access"
      },
      {
        "index": 2,
        "text": "Anonymous: dan80 Highly Voted 4Â years, 7Â months ago\nAnswer is C - Billing Account Viewer access would usually be granted to finance teams, it provides access to spend information, but does not confer the right to link or unlink projects or otherwise manage the properties of the billing account. krsourav 3Â years, 11Â months ago\nHey, look at this ......:)"
      },
      {
        "index": 3,
        "text": "Anonymous: Jayz1992 Most Recent 1Â year ago\nSelected Answer: C\nBilling Account Viewer\n(roles/billing.viewer) View billing account cost information and transactions. Organization or billing account. Billing Account Viewer access is usually granted to finance teams, it provides access to spend information, but doesn't confer the right to link or unlink projects or otherwise manage the properties of the billing account."
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 1Â year, 2Â months ago\nThe correct answer is C"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 1Â year, 4Â months ago\nSelected Answer: C\nC is the correct answer"
      },
      {
        "index": 6,
        "text": "Anonymous: anujithn 1Â year, 5Â months ago\nSelected Answer: C\nall other will give additional permission"
      },
      {
        "index": 7,
        "text": "Anonymous: AzureDP900 2Â years, 7Â months ago\nIt is no brainer question, C is right"
      },
      {
        "index": 8,
        "text": "Anonymous: haroldbenites 2Â years, 7Â months ago\nGo for C"
      },
      {
        "index": 9,
        "text": "Anonymous: ElenaL 3Â years ago\nSelected Answer: C\nC - the only role appropriate answer to view and not change anything in the project is the billing viewer role."
      },
      {
        "index": 10,
        "text": "Anonymous: Vidyaji 3Â years, 1Â month ago\nC is perfect"
      }
    ]
  },
  {
    "id": 325,
    "source": "examtopics",
    "question": "Your organization has strict requirements to control access to Google Cloud projects. You need to enable your Site Reliability Engineers (SREs) to approve requests from the Google Cloud support team when an SRE opens a support case. You want to follow Google-recommended practices. What should you do?",
    "options": {
      "A": "Add your SREs to roles/iam.roleAdmin role.",
      "B": "Add your SREs to roles/accessapproval.approver role.",
      "C": "Add your SREs to a group and then add this group to roles/iam.roleAdmin.role.",
      "D": "Add your SREs to a group and then add this group to roles/accessapproval.approver role."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: reinocd21 Highly Voted 5Â years, 7Â months ago\nD. Add your SREs to a group and then add this group to roles/accessapproval approver role.\n-Google recommendation."
      },
      {
        "index": 2,
        "text": "Anonymous: ErenYeager Highly Voted 3Â years, 9Â months ago\nPassed my exams today. Not because just because of the questions I practiced here, but because of you guys, your knowledge and experience and breakdown of questions. Too bad this site can't go legit. It such an wholesome resource.\nSome final words... KEEP MOVING FORWARD UNTIL ALL THE QUESTIONS ARE DESTROYED TATAKAE!!!!!!"
      },
      {
        "index": 3,
        "text": "Anonymous: denno22 Most Recent 1Â year, 3Â months ago\nSelected Answer: D\nGrant the Access Approval Approver (roles/accessapproval.approver) IAM role on the project, folder, or organization to the principal who you want to be able to perform approvals. You can grant the Access Approval Approver IAM role to either an individual user, a service account, or a Google group.\nhttps://cloud.google.com/assured-workloads/access-approval/docs/approve-requests"
      },
      {
        "index": 4,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nD is the correct answer"
      },
      {
        "index": 5,
        "text": "Anonymous: Captain1212 2Â years, 4Â months ago\nSelected Answer: D\nD seems more correct"
      },
      {
        "index": 6,
        "text": "Anonymous: SanjeevKumar1983 2Â years, 4Â months ago\nD is correct"
      },
      {
        "index": 7,
        "text": "Anonymous: AzureDP900 3Â years, 7Â months ago\nD is right .."
      },
      {
        "index": 8,
        "text": "Anonymous: haroldbenites 3Â years, 7Â months ago\nGo for D"
      },
      {
        "index": 9,
        "text": "Anonymous: hiranfilho 3Â years, 8Â months ago\nSelected Answer: D\nAnswers C and D are correct, but it doesn't say if the SRE already has a group and as it is Google's recommendation to make a group to add users and privileges to the group, the right one is D"
      },
      {
        "index": 10,
        "text": "Anonymous: WTY 3Â years, 8Â months ago\nIt mentioned more than one SRE, so adding the user to group is most suitable approach, Answer is D."
      }
    ]
  },
  {
    "id": 326,
    "source": "examtopics",
    "question": "Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?",
    "options": {
      "A": "Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.",
      "B": "Ask each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.",
      "C": "Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the ×’â‚¬compute.osAdminLogin×’â‚¬ role to the Google group corresponding to this team.",
      "D": "Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: dan80 Highly Voted 1Â year, 4Â months ago\nC is correct - https://cloud.google.com/compute/docs/instances/managing-instance-access adedj99 1Â year, 4Â months ago\nWe recommend collecting users with the same responsibilities into groups and assigning IAM roles to the groups rather than to individual users. For example, you can create a \"data scientist\" group and assign appropriate roles to enable interaction with BigQuery and Cloud Storage. When a new data scientist joins your team, you can simply add them to the group and they will inherit the defined permissions. You can create and manage groups through the Admin Console."
      },
      {
        "index": 2,
        "text": "Anonymous: zakhili Highly Voted 5Â years, 7Â months ago\nSend private key to users is not safe, i think it's C"
      },
      {
        "index": 3,
        "text": "Anonymous: svij87 Most Recent 1Â month ago\nSelected Answer: C\nSharing Private key is security concern."
      },
      {
        "index": 4,
        "text": "Anonymous: ValidItexams_com 3Â months, 4Â weeks ago\nSelected Answer: C\nI can proudly say that I cleared the Google Associate-Cloud-Engineer Exam without any difficulty."
      },
      {
        "index": 5,
        "text": "Anonymous: BlairOnIce 5Â months, 3Â weeks ago\nSelected Answer: C\nWhen dealing with a large number of users it makes sense to use groups"
      },
      {
        "index": 6,
        "text": "Anonymous: Hismajesty 6Â months, 2Â weeks ago\nSelected Answer: C\nEach team member must generate a new SSH key pair and add the public key to their Google account. Grant the \"compute.osAdminLogin\" role to the corresponding Google group for this team."
      },
      {
        "index": 7,
        "text": "Anonymous: joaonunatings 6Â months, 4Â weeks ago\nSelected Answer: C\nC is correct"
      },
      {
        "index": 8,
        "text": "Anonymous: SayujM 7Â months, 2Â weeks ago\nSelected Answer: C\nSatisfies both the requirements a) providing Admin access & b) identifying who accessed the VM."
      },
      {
        "index": 9,
        "text": "Anonymous: emiliyasoftware 1Â year ago\nSelected Answer: C\nc is correct because you should never give your private keys"
      },
      {
        "index": 10,
        "text": "Anonymous: Jayz1992 1Â year ago\nSelected Answer: C\nI have taken GCP ACE exam today morning and also cleared exam.\nBelow are my suggestions and observations."
      }
    ]
  },
  {
    "id": 327,
    "source": "examtopics",
    "question": "You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use?",
    "options": {
      "A": "0.0.0.0/0",
      "B": "10.0.0.0/8",
      "C": "172.16.0.0/12",
      "D": "192.168.0.0/16"
    },
    "correct": "B",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Khaled_Rashwan Highly Voted 1Â year, 4Â months ago\nB is correct\nto calculate the range size for a network:"
      },
      {
        "index": 2,
        "text": "Anonymous: ovokpus Highly Voted 1Â year, 3Â months ago\nSelected Answer: B\nIn Google Cloud Platform (GCP), when creating a VPC network, you should use the IP ranges that are reserved for private networks as defined by the RFC 1918. Here are the private IP address ranges defined by RFC 1918:"
      },
      {
        "index": 255,
        "text": "It's the largest range among the options.\nC. 172.16.0.0/12: This is a private IP range, but it's smaller than 10.0.0.0/8.\nD. 192.168.0.0/16: This is also a private IP range, but it's smaller than both B and C.\nSo, if you want the subnet's range to be as large as possible:\nThe correct answer is B. 10.0.0.0/8."
      },
      {
        "index": 3,
        "text": "Anonymous: Hismajesty Most Recent 6Â months, 2Â weeks ago\nSelected Answer: B\nbecause the right answer is B"
      },
      {
        "index": 4,
        "text": "Anonymous: SayujM 7Â months, 2Â weeks ago\nSelected Answer: B\nkey here is to understand 0.0.0.0/0 is not a valid private IP range, which leaves us with option-b 10.0.0.0/8 which is in the valid private IP range & spans from 10.0.0.0 to 10.255.255.255 ( 2^24) IPs."
      },
      {
        "index": 5,
        "text": "Anonymous: Meera1986 7Â months, 3Â weeks ago\nSelected Answer: B\nThe correct answer is B"
      },
      {
        "index": 6,
        "text": "Anonymous: MANGANDA 1Â year ago\nSelected Answer: B\nThis CIDR notation provides the largest possible private subnet range. It allows for about 16 million IP addresses (from 10.0.0.0 to 10.255.255.255) and is a good choice for a VPC that may require a large IP address space."
      },
      {
        "index": 7,
        "text": "Anonymous: rxvybgfbhlswfllbxa 1Â year, 3Â months ago\nI will become an Associate Cloud Engineer"
      },
      {
        "index": 8,
        "text": "Anonymous: theanupmaurya 1Â year, 3Â months ago\nThe correct answer is B. 10.0.0.0/8.\nHere's why:\nClass A: 10.0.0.0/8 provides the largest subnet range with 16,777,214 possible IP addresses. This is because it uses only the first 8 bits for the network address, leaving the remaining 24 bits for host addresses.\nClass B: 172.16.0.0/12 provides a smaller range with 1,048,574 possible IP addresses.\nClass C: 192.168.0.0/16 provides an even smaller range with 65,534 possible IP addresses."
      },
      {
        "index": 9,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 3Â months ago\nSelected Answer: B\nOption B is the correct answer.\nTo create a custom VPC with a single subnet with the largest possible range, you should use the range 10.0.0.0/8. This range consists of 16,777,216 addresses, which is more than enough for most use cases."
      }
    ]
  },
  {
    "id": 328,
    "source": "examtopics",
    "question": "You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do?",
    "options": {
      "A": "Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.",
      "B": "Select Cloud SQL (MySQL). Select the create failover replicas option.",
      "C": "Select Cloud Spanner. Set up your instance with 2 nodes.",
      "D": "Select Cloud Spanner. Set up your instance as multi-regional."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: YashBindlish Highly Voted 1Â year, 4Â months ago\nA is Correct. You must enable binary logging to use point-in-time recovery. Enabling binary logging causes a slight reduction in write performance. https://cloud.google.com/sql/docs/mysql/backup-recovery/backups ryumada 1Â year, 4Â months ago\nIn this link below, the docs explains clearly that point-in-time recovery requires binary logging.\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#disk-usage"
      },
      {
        "index": 2,
        "text": "Anonymous: Bharathy Highly Voted 5Â years, 10Â months ago\nA is correct, as Binary Logging enables Point in Recovery in Cloud SQL"
      },
      {
        "index": 3,
        "text": "Anonymous: 4384d42 Most Recent 1Â week, 2Â days ago\nSelected Answer: A\nNow the option in GCP has changed the name to\"Enable point-in-time recovery\":\n[X] Enable point-in-time recovery\nAllows you to recover data from a specific point in time, down to a fraction of a second. Enables binary logs (required for replication)."
      },
      {
        "index": 4,
        "text": "Anonymous: BlairOnIce 5Â months, 3Â weeks ago\nSelected Answer: A\nPoint-in time = binary logging"
      },
      {
        "index": 5,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA. binary logging is uaed to setup point in time recovery. This basically records every single action to the db (crud) and stores on a binary log. So can be used to replicate. Logging like this means you can recover to any point in time rather than just eg 1 months agao when you last backed up - this way you can choose the day instead - ie before the issue occurred. MOre finer grained recovery. add a little latency to to sql response though."
      },
      {
        "index": 6,
        "text": "Anonymous: Akp965 9Â months, 3Â weeks ago\nSelected Answer: A\nA is correct .................\nyou need to enable binary logging. Cloud SQL for MySQL supports binary logging, which allows you to perform point-in-time recovery of the database."
      },
      {
        "index": 7,
        "text": "Anonymous: denno22 1Â year, 3Â months ago\nSelected Answer: A\nhttps://cloud.google.com/sql/docs/mysql/backup-recovery/restore#tips-pitr denno22 1Â year, 3Â months ago\nWhen you create a Cloud SQL instance in the Google Cloud console, PITR is enabled by default.\nPITR uses binary logging to archive logs."
      },
      {
        "index": 8,
        "text": "Anonymous: harsh5kalsait 1Â year, 4Â months ago\n**Best Choice: A**\nCloud SQL (MySQL) with binary logging enabled will provide point-in-time recovery capabilities, which meet your requirement for relational data in a single geographic location. It is also cost-effective for smaller datasets compared to Cloud Spanner."
      },
      {
        "index": 9,
        "text": "Anonymous: MUNHU 1Â year, 7Â months ago\nI agree with A"
      },
      {
        "index": 10,
        "text": "Anonymous: Ele24 1Â year, 11Â months ago\nSelected Answer: A\nA is Correct"
      }
    ]
  },
  {
    "id": 329,
    "source": "examtopics",
    "question": "You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps.\nYou need to configure re-creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?",
    "options": {
      "A": "Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP)",
      "B": "Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.",
      "C": "Create a managed instance group. Set the Autohealing health check to healthy (HTTP)",
      "D": "Create a managed instance group. Verify that the autoscaling setting is on."
    },
    "correct": "C",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: ReyBan Highly Voted 1Â year, 3Â months ago\nC, Agreed\nreference : https://cloud.google.com/compute/docs/tutorials/high-availability-autohealing\nPro Tip: Use separate health checks for load balancing and for autohealing. Health checks for load balancing detect unresponsive instances and direct traffic away from them. Health checks for autohealing detect and recreate failed instances, so they should be less aggressive than load balancing health checks. Using the same health check for these services would remove the distinction between unresponsive instances and failed instances, causing unnecessary latency and unavailability for your users. ashrafh 4Â years, 5Â months ago\nI also vote for C\ngo to gcp console create a httpa load balancer and in the health check settings take your mouse to question mark it says\n\"\"\"Ensures that requests are sent only to instances that are up and running\"\"\"\nso its not recreating, if the vm not working it redirect to one which work.\ngo to gpc console create MIG and check the questions mark of Autohealing health check settings it says\n\"\"\"Autohealing allows recreating VM instances when needed. You can use a health check to recreate a VM instance if the health check finds it unresponsive. If you don't select a health check, Compute Engine will recreate VM instances only when they're not running.\"\"\"\nhope this help :)"
      },
      {
        "index": 2,
        "text": "Anonymous: bryanchew Highly Voted 5Â years, 9Â months ago\nA. Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy(HTTP)\nThis is a possible answer. This answer assumes that the existing backend is configured correctly.\nB. Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.\nThis is a possible answer. This answer assumes that the existing backend is configured correctly. This answer adds an additional step over answer A.\nC. Create a managed instance group. Set the Autohealing health check to healthy(HTTP)\nThis is only a partial solution. The default configuration is auto scaling enabled. You still need to create the HTTP Load Balancer.\nD. Create a managed instance group. Verify that the auto scaling setting is on.\nThis is only a partial solution. Creating a Managed Instance Group with Auto Scaling is required, but you still need to create the HTTP Load Balancer.\nTherefore the best answer is A in my opinion. koniec 5Â years, 2Â months ago\nIt's A.\nManaged group already exists so create a LB with health checks.\nIf you go for C, you will have to create a LB anyway so it's more steps to achieve the goal tavva_prudhvi 4Â years, 9Â months ago\nhttps://www.youtube.com/watch?v=dT7xDEtALPQ&list=PLIivdWyY5sqIij_cgINUHZDMnGjVx3rxi&index=36\nstep-1: go to the instance group\nstep-2: click edit\nstep-3: scroll down you will see auto-healing off by default change to ON\nstep-4: create a health check saying 10 seconds as CHECK INTERVAL and UNHEALTHY THRESHOLD=3 DickDastardly 4Â years, 10Â months ago\nIt can't be A as a load balancer does not re-create unhealthy instances, as per the requirement.\nHas to be C\nhttps://cloud.google.com/compute/docs/instance-groups Ridhanya 4Â years, 1Â month ago\nit cannot be option A because as you said, load balancer with the health check is already present and now the problem is simply auto healing. so we need to focus only on recreation which can happen only if option C is correct THutch 1Â year, 4Â months ago\nIt can't be A or B. Question clearly states, \"using the fewest possible steps\" and setting up an HTTP load balancer is a long, drawn out process that requires quite a few steps and is never mentioned as part of the requirement. Load full discussion..."
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: C\nDefinately C. this is the only way to create such autohealing. you create an instance group and then you choose the option for autohealing. When you do you can set a url to send a health check to and set a check interval time and timout"
      },
      {
        "index": 4,
        "text": "Anonymous: toasty 8Â months, 3Â weeks ago\nSelected Answer: C\nC, agree with you"
      },
      {
        "index": 5,
        "text": "Anonymous: SaiSaiA 1Â year, 3Â months ago\nC is the only one with the AUTOHEALING option, but it is not really correct. Remember, the GIVEN information are \"a NETWORK load balancer and a group of Compute Engine Instances that run in multiple zones\" which gives us an idea that the existing configuration is a target pool-based network lb.\nIf we are to use the existing group of VMs, we need to choose UNMANAGED Instance Group, UNMANAGED Instance Group does not have Autohealing, only a health check. Health check only checks if VMs are responsive or not but does not re-create instances as what Autohealing and Autoscaling do.\nYou can also try re-creating the scenario or check this\nhttps://cloud.google.com/load-balancing/docs/network/transition-to-backend-services#console\nSo, if a MANAGED INSTANCE group is to be used, then you need to create an instance template and use it for your MIG. Ofc, you cannot use the existing VMs, then you create a new load balancer. Ofc, the existing group of VMs mentioned in the question will no longer be used but rather a new set of VMs based on the instance template will be created. The choices should be updated."
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 3Â months ago\nSelected Answer: C\nOption C is correct because creating a managed instance group allows you to use autohealing to automatically recreate VMs that are unresponsive after 3 attempts of 10 seconds each. You can set the Autohealing health check to healthy (HTTP) to specify the health check that determines whether the instances are considered healthy or not. If an instance becomes unresponsive, Autohealing will recreate the instance and attach it to the managed instance group.\nhttps://cloud.google.com/compute/docs/instance-groups/autohealing-instance-groups Shivangi30 2Â years, 7Â months ago\nThe link url is invalid"
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nThe correct answer is C.\nManaged instance groups are groups of homogeneous Compute Engine instances that are managed as a single entity. They can be used to distribute traffic across multiple instances and to provide high availability.\nAutohealing is a feature of managed instance groups that automatically replaces instances that fail health checks. You can configure autohealing to recreate instances if they are unresponsive after a certain number of attempts.\nTo configure autohealing for network load balancing, you need to create a managed instance group and set the Autohealing health check to healthy (HTTP). The health check will periodically probe the instances in the group to see if they are responding. If an instance fails the health check, autohealing will recreate it."
      },
      {
        "index": 8,
        "text": "Anonymous: harsh5kalsait 1Â year, 3Â months ago\nOption C correct C. Create a managed instance group. Set the Autohealing health check to healthy (HTTP)\nExplanation:\n* Managed Instance Groups (MIGs) are specifically designed for managing and scaling groups of instances. They offer features like autohealing, load balancing, and autoscaling.\n* Autohealing is a key feature of MIGs that allows you to automatically recreate unhealthy instances based on health checks.\nWhy other options are incorrect:\n* A and B involve creating an HTTP load balancer, which is not directly related to autohealing. Load balancers distribute traffic but do not handle instance health checks and recreation.\n* D only creates a managed instance group and verifies autoscaling, which is not sufficient for autohealing. Autohealing requires a health check configuration.\nBy choosing option C, you directly address the requirements of configuring autohealing for a group of Compute Engine instances with the fewest possible steps."
      },
      {
        "index": 9,
        "text": "Anonymous: errorfetch 1Â year, 4Â months ago\nSelected Answer: C\nhere we clearly need auto healing capability so C is correct."
      },
      {
        "index": 10,
        "text": "Anonymous: boydocarta 1Â year, 5Â months ago\nC, Agreed\nPro Tip: Use separate health checks for load balancing and for autohealing. Health checks for load balancing detect unresponsive instances and direct traffic away from them. Health checks for autohealing detect and recreate failed instances, so they should be less aggressive than load balancing health checks. Using the same health check for these services would remove the distinction between unresponsive instances and failed instances, causing unnecessary latency and unavailability for your users."
      }
    ]
  },
  {
    "id": 330,
    "source": "examtopics",
    "question": "You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps. What should you do?",
    "options": {
      "A": "Use gcloud config configurations describe to review the output.",
      "B": "Use gcloud config configurations activate and gcloud config list to review the output.",
      "C": "Use kubectl config get-contexts to review the output.",
      "D": "Use kubectl config use-context and kubectl config view to review the output."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: zukko78 Highly Voted 5Â years, 8Â months ago\nD is correct nhusain 4Â years, 8Â months ago\nhttps://medium.com/google-cloud/kubernetes-engine-kubectl-config-b6270d2b656c\nexplains it well"
      },
      {
        "index": 2,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 7Â months ago\nC is correct , Use kubectl config get-contexts to review the output : shows the clusters and the configurations and based on the output we can identify the inactive configurations Gurnoor 5Â years, 7Â months ago\nThis is wrong get-contexts does not show clusters it only shows contexts. jilly 5Â years, 6Â months ago\nTrue .\nWill give only below results\nkubectl config get-contexts\nCURRENT NAME CLUSTER AUTHINFO NAMESPACE\n* white white dazwilkin\nblack black dazwilkin fracila 3Â years, 2Â months ago\nkubectl config get-contexts displays a list of contexts as well as the clusters that use them. Here's a sample output."
      },
      {
        "index": 3,
        "text": "Anonymous: yves95 Most Recent 1Â week ago\nSelected Answer: A\nCorrect"
      },
      {
        "index": 4,
        "text": "Anonymous: Vismaya 3Â weeks ago\nSelected Answer: A\nA is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: SKSINDIAN 3Â weeks, 4Â days ago\nSelected Answer: A\nkubectl only shows the configuraiton after the gcloud container clusters get-credential, so we shoudl use the gcloud config command"
      },
      {
        "index": 6,
        "text": "Anonymous: Arshad1812 5Â months, 1Â week ago\nSelected Answer: A\nA is correct.\ngcloud config configurations describe\nThis command lets you inspect the details of any configuration (active or inactive) without switching."
      },
      {
        "index": 7,
        "text": "Anonymous: Hismajesty 6Â months, 2Â weeks ago\nSelected Answer: A\nThis command enables you to view the settings, including cluster details, of the inactive configuration without activating it. This approach is the most efficient and concise method for inspection. Keep in mind that all kubectl commands will only display Kubernetes contexts after switching to them, not Google Cloud configurations. To improve your understanding of gcloud, you can activate the interactive help mode by using the command `gcloud beta interactive`."
      },
      {
        "index": 8,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nA. As the kubectl just read you local .kubectl config file. the question was about gcloud configuration of a k8s cluster. As its not active must use describe."
      },
      {
        "index": 9,
        "text": "Anonymous: kewgard 7Â months, 3Â weeks ago\nSelected Answer: A\nThis command shows the settings (including the cluster) of the inactive configuration without needing to activate it. This is the most efficient and minimal way to inspect it. All the kubectl commands will only show k8s contexts (once you switch to it) not the gcloud configurations . turn on gcloud interactive help mode - gcloud beta interative - this will help you learn gcloud"
      },
      {
        "index": 10,
        "text": "Anonymous: MuhannadYW 8Â months ago\nSelected Answer: A\nWhy C and D are not ideal:\nThey assume that the inactive config already updated the kubectl context, which may not be true.\nkubectl config get-contexts only shows kubeconfig contexts, not gcloud config clusters.\nkubectl config use-context and kubectl config view affect your active kubeconfig, which can break workflows if misused.\nThey require the config to have been previously used and exported to kubeconfig (i.e., gcloud container clusters get-credentials must have been run)."
      }
    ]
  },
  {
    "id": 331,
    "source": "examtopics",
    "question": "Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use?",
    "options": {
      "A": "Multi-Regional Storage",
      "B": "Regional Storage",
      "C": "Nearline Storage",
      "D": "Coldline Storage"
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 7Â months ago\nBest Answer is \" Archive Storage \"\nhttps://cloud.google.com/storage/docs/storage-classes\nBut as per the given option next best solution is \" Coldline Storage\" Mutune 4Â years, 10Â months ago\nPerfectly stated toasty 8Â months, 3Â weeks ago\nArchive storage requires a lot of time to access files, and we are dealing with backups (we require fast acess times to restore service) Sami_27 1Â year, 3Â months ago\nNo, archive storage might not be the correct choice as we need to consider the access time. Coldline Storage provides relatively faster access times compared to Archive Storage, which is important if you need to recover data quickly in a disaster scenario.\nColdline Storage: Fits well with disaster recovery use cases where data is infrequently accessed but needs to be available relatively quickly if a disaster occurs."
      },
      {
        "index": 2,
        "text": "Anonymous: zukko78 Highly Voted 5Â years, 8Â months ago\nD is correct,\nColdline Storage COLDLINE 90 days"
      },
      {
        "index": 3,
        "text": "Anonymous: NetExpert Most Recent 2Â weeks ago\nSelected Answer: D\nColdline Storage: Fits well with disaster recovery use cases where data is infrequently accessed but needs to be available relatively quickly if a disaster occurs."
      },
      {
        "index": 4,
        "text": "Anonymous: sandipsmenon 4Â months ago\nSelected Answer: D\nThe Keyword here is backup here. Coldline is designed for disaster recovery and long-term archival."
      },
      {
        "index": 5,
        "text": "Anonymous: AgentR 5Â months, 3Â weeks ago\nSelected Answer: D\nBest Answer is Coldline. Please dont use to much of brain to answer this question. The question is very vague and assuming something is where most people got it wrong.\nQuestion is clear.\nDisaster recovery and best practices.\nOption A is eliminated since we need that kind of storage for frequent retrieval.\nOption D is best since data is retrived only when we have a DR scenario and it happen once in a blue moon.\nDO NOT PUT LOT OF THINKING. JUST READ THE QUESTION AND DONT ASSUME ANYTHING"
      },
      {
        "index": 6,
        "text": "Anonymous: ucanmanda 5Â months, 3Â weeks ago\nSelected Answer: A\nIt is Coldline Storage because question says that which Cloud Storage that you need. Not a Method that you have used."
      },
      {
        "index": 7,
        "text": "Anonymous: SayujM 7Â months, 1Â week ago\nSelected Answer: A\nWent of searching online and read that a full disaster recovery plan also considers:\n1) Recovery Point Objective (RPO): This is the maximum amount of data (measured in time) that you are willing to lose in a disaster. For backups, your RPO dictates how frequently you need to create them.\n2) Recovery Time Objective (RTO): This is the maximum amount of time that can pass after a disaster before your application or system must be available again. Your RTO influences how quickly you can restore your backups.\nhence we would want data to be highly available & also reasonably quick to retrieve when a disaster strikes. Considering that option of Multi-regional Standard Storage makes the most sense."
      },
      {
        "index": 8,
        "text": "Anonymous: Kessel 8Â months, 3Â weeks ago\nSelected Answer: A\nIt doesn't say that we want the most economical solution. Given that multi-regional provides instant failover and availability, I would go with A. Certainly not D, because there is too much of a delay in getting back your data for retrieval."
      },
      {
        "index": 9,
        "text": "Anonymous: AbsurdDragon 10Â months, 3Â weeks ago\nSelected Answer: A\nDR needs to be tested when you do pentesting\nDR needs to be sanity checked at some frequency depending on the importance of your app.\nDR needs to be kept up to date with your app changes - regular snapshots and writes.\nColdline is too expensive for regular writes and as needed reads.\nDR needs to be there when something catastrophic happens (or not too bad). If there's a hurricane in North Carolina that knocks google's DC out - and both your app and DR were hosted there - that sucks, ideally you should have had your DR in a couple of different regions.\nThus A - multi region is the answer"
      },
      {
        "index": 10,
        "text": "Anonymous: Joseph_Covaro 11Â months, 1Â week ago\nSelected Answer: D\nI believe Coldline Storage is the answer in absence of Archive Storage. Note how the bucket is responsible for disaster recovery, meaning it should already be in a different region from our operational data, therefore making Multi-Regional-Storage unnecessary."
      }
    ]
  },
  {
    "id": 332,
    "source": "examtopics",
    "question": "Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account. What should you do?",
    "options": {
      "A": "Contact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.",
      "B": "Create a ticket with Google Support and wait for their call to share your credit card details over the phone.",
      "C": "In the Google Platform Console, go to the Resource Manage and move all projects to the root Organizarion.",
      "D": "In the Google Cloud Platform Console, create a new billing account and set up a payment method."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: samcat84 Highly Voted 5Â years, 7Â months ago\nC is incomplete. Moving projects under an organisation doesn't change their linked billing project.\nhttps://cloud.google.com/resource-manager/docs/migrating-projects-billing\n----\nNote: The link between projects and billing accounts is preserved, irrespective of the hierarchy. When you move your existing projects into the organization they will continue to work and be billed as they used to before the migration, even if the corresponding billing account has not been migrated yet.\n----\nD is incomplete as well, after setting the billing account in the organisation you need to link the projects to the new billing account. sarahf 5Â years, 1Â month ago\nI agree that neither C or D is correct. I did the cert a month ago and this question was not on it. Although a similar question about how to change the payment method from your own card in your project to to the company's \"card\". So they might have removed this one. ehizren 4Â years, 11Â months ago\nWhat's was the answer your chose for your particular exam question? GokulVelusaamy 3Â years, 2Â months ago\nWe need to add a new payment method and need to set that as Primary, post that we need to remove the previous one\n\"If you want to remove a payment method, you should add a new payment method first.\"\nRefer : https://cloud.google.com/billing/docs/how-to/payment-methods Raz0r 3Â years, 11Â months ago\nThe given answers make D the only possible solution. C can not be right, you all need to look it up here: https://cloud.google.com/resource-manager/docs/project-migration#change_billing_account ryumada 3Â years, 5Â months ago\nThis link explains clearly that move a project won't affect billing.\nhttps://cloud.google.com/resource-manager/docs/project-migration#permissions-billing"
      },
      {
        "index": 2,
        "text": "Anonymous: poogcp Highly Voted 5Â years, 7Â months ago\nC is correct Answer. there will be 1 billing account for the organization and all projects under that organization are linked to single billing account. arathefu 3Â years, 10Â months ago\nhttps://cloud.google.com/resource-manager/docs/project-migration#change_billing_account\n\"Moving a project from one organization to another won't impact billing, and charges will continue against the old billing account. \" Neha_Pallavi 2Â years, 6Â months ago\nThe question is under the organization different projects are maintained the different cloud platforms.all the different project should single corporate bill account instead of the employee billing account. So try to update the corporate bill account details and mark it as primary for the all projects, post that employee account details need to removed. So suitable recomanded option is D"
      },
      {
        "index": 3,
        "text": "Anonymous: sandipsmenon Most Recent 4Â months ago\nSelected Answer: D\nC â†’ Partially correct (you may move projects under the org for management), but this doesnâ€™t set up billing. Thatâ€™s about resource hierarchy, not payment.\nD â†’ Correct. The recommended practice is to create a new billing account in the Google Cloud Console, attach a corporate payment method, and then link projects to this billing account."
      },
      {
        "index": 4,
        "text": "Anonymous: MuhannadYW 8Â months ago\nSelected Answer: D\nD. In the Google Cloud Platform Console, create a new billing account and set up a payment method\nExplanation:\nTo centralize billing across multiple Google Cloud projects, the company should:\nCreate a new corporate billing account in the Google Cloud Console.\nAdd a corporate payment method (like a company credit card or bank account).\nReassign each project to the new billing account via the Billing section in the console."
      },
      {
        "index": 5,
        "text": "Anonymous: psyll0n 1Â year, 2Â months ago\nD is the correct answer! psyll0n 1Â year, 2Â months ago\nConsolidate multiple Billing Accounts into your main Billing Accounts."
      },
      {
        "index": 1,
        "text": "First identify your main Billing Accounts and the projects you want to link to those billing accounts."
      },
      {
        "index": 2,
        "text": "Link or move existing projects onto your main Billing Accounts.\nReference: https://cloud.google.com/billing/docs/onboarding-checklist"
      },
      {
        "index": 6,
        "text": "Anonymous: Makar 1Â year, 2Â months ago\nAnswer is correct (D)"
      },
      {
        "index": 7,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 3Â months ago\nSelected Answer: D\nOption A is incorrect because you cannot request a corporate billing account by emailing cloud-billing@google.com. This email address is for general billing inquiries and support.\nOption B is incorrect because you cannot create a ticket with Google Support to share your credit card details over the phone. To set up a payment method for a billing account, you must do it through the Google Cloud Platform Console.\nOption C is incorrect because moving projects to the root organization will not create a new billing account. You must first create a new billing account and then move the projects to the root organization to ensure that they are all billed to the same billing account.\nTherefore, the correct answer is Option D.\nhttps://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account"
      },
      {
        "index": 8,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: D\nThe correct answer is D. You have to follow the complete steps to successfully:"
      }
    ]
  },
  {
    "id": 333,
    "source": "examtopics",
    "question": "You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do?",
    "options": {
      "A": "Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.",
      "B": "Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.",
      "C": "Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.",
      "D": "Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: Khaled_Rashwan Highly Voted 5Â years, 8Â months ago\nA\nIP 10.0.3.21 is internal by default, and to ensure that it will be static non-changing it should be selected as static internal ip address. riccamini 3Â years, 2Â months ago\nHow do you know it is internal by default? yeanlingmedal71 3Â years, 1Â month ago\nhttps://cloud.google.com/vpc/docs/subnets#valid-ranges"
      },
      {
        "index": 2,
        "text": "Anonymous: zukko78 Highly Voted 5Â years, 8Â months ago\nit's obvious, A"
      },
      {
        "index": 3,
        "text": "Anonymous: Buruguduystunstugudunstuy Most Recent 1Â year, 3Â months ago\nSelected Answer: A\nThe correct answer is Option A.\nTo deploy the licensing server on Compute Engine and ensure that the application can reach it, you should reserve the IP 10.0.3.21 as a static internal IP address and assign it to the licensing server.\nBy reserving IP 10.0.3.21 as a static internal IP address, you can ensure that the application can reach the licensing server at that IP address without changing the application's configuration.\nTo reserve the IP 10.0.3.21 as a static internal IP address and assign it to a Compute Engine instance using gcloud, you can use the following command:\ngcloud compute instances create [INSTANCE_NAME] --address [IP_ADDRESS] --no-address\nReplace [INSTANCE_NAME] with the name of the Compute Engine instance that you want to create, and [IP_ADDRESS] with the desired static internal IP address (in this case, 10.0.3.21).\nThe --no-address flag specifies that the instance should not be assigned a public IP address."
      },
      {
        "index": 4,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: A\nThe correct answer is A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.\nTo reserve a static internal IP address, you can use the gcloud command-line tool. For example, to reserve the IP address 10.0.3.21, you would run the following command:\ngcloud compute addresses reserve 10.0.3.21\nOnce you have reserved the static internal IP address, you can assign it to the licensing server by running the following command:\ngcloud compute instances set-address licensing-server --address 10.0.3.21\nOnce you have assigned the static internal IP address to the licensing server, the application will be able to reach it using that IP address."
      },
      {
        "index": 5,
        "text": "Anonymous: harsh5kalsait 1Â year, 5Â months ago\nCorrect A: Due to an Application licensing server on the Static IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server."
      },
      {
        "index": 6,
        "text": "Anonymous: sam422 1Â year, 7Â months ago\nD\nApplication Compatibility: It maintains the application's existing configuration looking for the IP 10.0.3.21 (assumed to be an internal address).\nStatic IP: Promoting the ephemeral IP to static ensures the licensing server retains the 10.0.3.21 address even after restarts."
      },
      {
        "index": 7,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer is A"
      },
      {
        "index": 8,
        "text": "Anonymous: vipuldhage 2Â years ago\nI agree that we should not expose the licensing server to the internet. But at the same time the in the question it is not mentioned that the application is deployed in the gcp environment."
      },
      {
        "index": 9,
        "text": "Anonymous: BAofBK 2Â years, 2Â months ago\nThe correct answer is A"
      },
      {
        "index": 10,
        "text": "Anonymous: Evan7557 2Â years, 3Â months ago\nA is Right Ans"
      }
    ]
  },
  {
    "id": 334,
    "source": "examtopics",
    "question": "You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?",
    "options": {
      "A": "Manual Scaling with 3 instances.",
      "B": "Basic Scaling with min_instances set to 3.",
      "C": "Basic Scaling with max_instances set to 3.",
      "D": "Automatic Scaling with min_idle_instances set to 3."
    },
    "correct": "D",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: zukko78 Highly Voted 5Â years, 8Â months ago\nD is correct.\nApp Engine supports the following scaling types, which controls how and when instances are created:\nAutomatic\nBasic\nManual\nYou specify the scaling type in your app's app.yaml.\nAutomatic scaling\nAutomatic scaling creates instances based on request rate, response latencies, and other application metrics. You can specify thresholds for each of these metrics, as well as a minimum number instances to keep running at all times. vincent2023 2Â years, 4Â months ago\nhttps://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed"
      },
      {
        "index": 2,
        "text": "Anonymous: Finger41 Highly Voted 4Â years, 8Â months ago\nD is correct : https://cloud.google.com/appengine/docs/standard/go/config/appref\n\"App Engine calculates the number of instances necessary to serve your current application traffic based on scaling settings such as target_cpu_utilization and target_throughput_utilization. Setting min_idle_instances specifies the number of instances to run in addition to this calculated number. For example, if App Engine calculates that 5 instances are necessary to serve traffic, and min_idle_instances is set to 2, App Engine will run 7 instances (5, calculated based on traffic, plus 2 additional per min_idle_instances).\""
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: D\nD. As setting min idle will provide you the min 3 unoccupied instances at all times. A : is manual so not recommeneded, setting min and max instances will not provide 'at least 3 unnoccupied inatances - as per the question requirement."
      },
      {
        "index": 4,
        "text": "Anonymous: djmanvaro 1Â year ago\nSelected Answer: D\nit is correct"
      },
      {
        "index": 5,
        "text": "Anonymous: Makar 1Â year, 2Â months ago\nAnswer D is correct\nmin_idle_instances set to 3: Setting the min_idle_instances to 3 ensures that there are always at least 3 instances ready and waiting to handle incoming requests. This setting helps in managing sudden spikes in traffic by reducing latency, as these instances do not require any spin-up time"
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 3Â months ago\nSelected Answer: D\nThe correct answer is Option D.\nTo scale the number of instances based on request rate and ensure that there are always at least 3 unoccupied instances, you should use Automatic Scaling with min_idle_instances set to 3.\nAutomatic Scaling automatically scales the number of instances based on request rate and other metrics, such as CPU and memory utilization. By setting min_idle_instances to 3, you can ensure that the instance group maintains at least 3 idle instances at all times, ready to handle incoming requests."
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: D\nThe correct answer is D. Automatic Scaling with min_idle_instances set to 3. By setting min_idle_instances to 3, you can ensure that there are always at least 3 instances available to handle new requests. The other options are not as good:\nA. Manual Scaling requires you to manually adjust the number of instances running your application.\nB. Basic Scaling is a simpler version of Automatic Scaling. It automatically scales the number of instances based on request rate, but it does not allow you to specify the minimum number of idle instances. This means that there is no guarantee that there will always be at least 3 instances available to handle new requests.\nC. The max_instances setting specifies the maximum number of instances to keep running. By setting max_instances to 3, you are limiting the number of instances that your application can scale to. This is not ideal, especially if your application experiences sudden spikes in traffic."
      },
      {
        "index": 8,
        "text": "Anonymous: JoniMONI 1Â year, 3Â months ago\nSelected Answer: D\nD. Automatic Scaling with min_idle_instances set to 3.\nAutomatic scaling adjusts the number of instances based on the request rate, while maintaining a minimum number of instances available. By setting min_idle_instances to 3, you ensure that at least 3 instances are running and available to handle requests, even when the request rate is low.\nManual scaling allows you to set a fixed number of instances, but does not automatically adjust based on the request rate. Basic scaling adjusts the number of instances based on the request rate, but does not allow you to set a minimum number of idle instances.\nIn order to keep at least 3 instances running and ready to handle requests, Automatic scaling with min_idle_instances set to 3 is the correct option."
      },
      {
        "index": 9,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: D\nReference:\nhttps://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed"
      },
      {
        "index": 10,
        "text": "Anonymous: thewalker 2Â years, 1Â month ago\nD.\nAs per, https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed#scaling_types"
      }
    ]
  },
  {
    "id": 335,
    "source": "examtopics",
    "question": "You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do?",
    "options": {
      "A": "Use gcloud iam roles copy and specify the production project as the destination project.",
      "B": "Use gcloud iam roles copy and specify your organization as the destination organization.",
      "C": "In the Google Cloud Platform Console, use the 'create role from role' functionality.",
      "D": "In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions."
    },
    "correct": "A",
    "discussions": [
      {
        "index": 1,
        "text": "Anonymous: coldpar Highly Voted 5Â years, 10Â months ago\nCorrect Answer is A not B"
      },
      {
        "index": 2,
        "text": "Anonymous: Agents89 Highly Voted 5Â years, 8Â months ago\nCorrect answer is A"
      },
      {
        "index": 3,
        "text": "Anonymous: kewgard Most Recent 7Â months, 3Â weeks ago\nSelected Answer: A\nA. as this command exists and its inputs the destination project are a parameter. Also B would copy to the whole org with is bad. C does not exist and D is a lot or work."
      },
      {
        "index": 4,
        "text": "Anonymous: MuhannadYW 8Â months ago\nSelected Answer: C\nWhy C is correct:\nThe \"create role from role\" feature in the GCP Console allows you to quickly create a new custom role by cloning an existing one, even across different projects.\nThis minimizes effort and prevents errors compared to manually selecting permissions again."
      },
      {
        "index": 5,
        "text": "Anonymous: ChristN 1Â year, 3Â months ago\nSelected Answer: A\nA could be possible if we were talking about organization in the question, But here, it's clearly specified \"*project*\"\n\"You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps.\"\nfrom google doc: https://cloud.google.com/sdk/gcloud/reference/iam/roles/copy\nEXAMPLES\nTo create a copy of an existing role spanner.databaseAdmin into an organization with 1234567, run:\ngcloud iam roles copy --source=\"roles/spanner.databaseAdmin\" --destination=CustomViewer --dest-organization=1234567\nTo create a copy of an existing role spanner.databaseAdmin into a project with PROJECT_ID, run:\ngcloud iam roles copy --source=\"roles/spanner.databaseAdmin\" --destination=CustomSpannerDbAdmin --dest-project=PROJECT_ID"
      },
      {
        "index": 6,
        "text": "Anonymous: Buruguduystunstugudunstuy 1Â year, 3Â months ago\nSelected Answer: A\nThe correct answer is Option A.\nTo create the same IAM roles in a production project as in a development project, using the fewest possible steps, you can use the gcloud iam roles copy command and specify the production project as the destination project.\nThe `gcloud iam roles copy` command allows you to copy IAM roles between projects or organizations. By specifying the production project as the destination project, you can copy the IAM roles from the development project to the production project.\nOption B is incorrect because specifying your organization as the destination organization will copy the IAM roles to all projects within the organization, which is not what you want."
      },
      {
        "index": 7,
        "text": "Anonymous: YourCloudGuru 1Â year, 3Â months ago\nSelected Answer: A\nThe correct answer is A. Use gcloud iam roles copy and specify the production project as the destination project.\nThe gcloud iam roles copy command copies a role from one project to another. To use this command, you will need to know the name of the role that you want to copy and the name of the destination project.\nFor example, to copy the role roles/compute.instanceAdmin from the project my-dev-project to the project my-prod-project, you would run the following command:\ngcloud iam roles copy roles/compute.instanceAdmin my-dev-project my-prod-project\nThis command will copy the role roles/compute.instanceAdmin to the project my-prod-project. The role will have the same permissions in the production project as it does in the development project."
      },
      {
        "index": 8,
        "text": "Anonymous: Amolbhombe 1Â year, 7Â months ago\nSelected Answer: A\nCorrect answer is A"
      },
      {
        "index": 9,
        "text": "Anonymous: sidobill 1Â year, 7Â months ago\nSelected Answer: A\nA is right. B will propagate for all projects, not desired as per case description"
      },
      {
        "index": 10,
        "text": "Anonymous: subha.elumalai 1Â year, 8Â months ago\nCorrect Answer: B\nReference:\nhttps://cloud.google.com/sdk/gcloud/reference/iam/roles/copy sidobill 1Â year, 7Â months ago\nthe exact same link shows A as right one. If you set dest org for it, it will inherint for all other projects."
      }
    ]
  }
]