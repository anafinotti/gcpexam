==============================
Page X — Question #61

Pergunta:
Your organization is a financial company that needs to store audit log files for 3 years. Your organization has hundreds of Google Cloud projects. You need to implement a cost-effective approach for log file retention. What should you do?

Alternativas:
- A. Create an export to the sink that saves logs from Cloud Audit to BigQuery.
- B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.
- C. Write a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.
- D. Export these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL.

Resposta correta:
B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.

Top 10 Discussões (sem replies):
1. Anonymous: yasu Highly Voted  5 years, 9 months ago
Why not B? cost effective
   upvoted 58 times
 lxs 4 years, 2 months ago
BigQuery data after 90 days has the same cost for storage as Cloud Storage Nearline. Storing it in Cloud Storage adds more costs for data retrival if the class is i.e archival
   upvoted 1 times
 KerolesKhalil 2 years, 7 months ago
the options have cold-line storage not nearline.
so B is the cheapest option.
   upvoted 2 times
 _Sande 2 years, 9 months ago
That seems to be the one...
   upvoted 1 times
 uganeshku 4 years ago
B is correct because Coldline Storage is the perfect service to store audit logs from all the projects and is very cost-efficient as well. Coldline Storage is a very low-cost, highly durable storage service for storing infrequently accessed data.
   upvoted 6 times

2. Anonymous: Gini Highly Voted  5 years, 9 months ago
if it is all about cost, B is the best. However, speaking of "audit" you probably need to access the data once in a while, which Coldline storage might not be ideal for this case I guess? I would go for A in the exam though.
   upvoted 21 times
 Ale1973 5 years, 4 months ago
Be strong!!! If B is the best, go for B!!!
   upvoted 15 times
 pas77 4 years, 5 months ago
The question is clearly saying cost effect. BQ is one of the most expensive services in GCP.
   upvoted 8 times
 boof 4 years, 3 months ago
I would play it safe and interpret the question literally, implying that they will only store the audit logs and not be accessing them a lot.
   upvoted 4 times

3. Anonymous: pragneshpandya Most Recent  1 year, 4 months ago
Selected Answer: B
as its logs for 3 years , coldline is the correct
   upvoted 1 times

4. Anonymous: jungkook_1 1 year, 8 months ago
But the retention period of Coldline storage is 90 days, it's not meeting the requirement mentioned in ques to store for 3 years.
   upvoted 1 times

5. Anonymous: jungkook_1 1 year, 8 months ago
But retention period of coldline storage is 90 days only, in ques they've mentioned for 3 years?
   upvoted 1 times

6. Anonymous: LSB56757 1 year, 9 months ago
Selected Answer: B
Using Google Gemini, it suggests Option B.
   upvoted 3 times

7. Anonymous: zameerb 1 year, 12 months ago
Both options (exporting to BigQuery and exporting to Coldline Storage) have their merits, and the choice depends on specific use cases, access patterns, and organizational preferences. If your organization values a more structured and analyzable format with SQL-like querying capabilities, BigQuery might be preferable. If the priority is on long-term, infrequent access with cost optimization, then Coldline Storage could be a suitable choice.
   upvoted 1 times

8. Anonymous: BAofBK 2 years, 2 months ago
The correct answer is B.
   upvoted 1 times

9. Anonymous: gsmasad 2 years, 2 months ago
Selected Answer: B
B is correct. coldline storage it is cost-effective and for long-term storage
   upvoted 1 times

10. Anonymous: elviskimutai 2 years, 4 months ago
B is correct. coldline storage it is cost-effective and for long-term storage
   upvoted 1 times
==============================

==============================
Page X — Question #62

Pergunta:
You want to run a single caching HTTP reverse proxy on GCP for a latency-sensitive website. This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost. How should you run this reverse proxy?

Alternativas:
- A. Create a Cloud Memorystore for Redis instance with 32-GB capacity.
- B. Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.
- C. Package it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.
- D. Run it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB.

Resposta correta:
A. Create a Cloud Memorystore for Redis instance with 32-GB capacity.

Top 10 Discussões (sem replies):
1. Anonymous: jzh Highly Voted  5 years, 5 months ago
Go to cloud console and create instance
select Memorystore with Basic tier, select us-central1 and us-central1-a, and capacity 32GB, the cost estimate is $0.023/GB/hr
select VM instance with custom machine type with 6 vCPUs and 32 GB memory, the same region and zone as Memorystore setting, the cost estimate is $0.239/hr
Option B will definitely cost more as it adds on CPU usage cost even it uses little in this scenario, but still charge you. So answer is A from real practice example.
   upvoted 57 times
 SSPC 5 years, 5 months ago
I agree with you
   upvoted 1 times
 Rothmansua 4 years, 3 months ago
and what about HTTP, how are you supporting that with Redis?
   upvoted 2 times
 obeythefist 3 years, 10 months ago
A quick Bing search shows a number of solutions for caching HTTP services with Redis.
   upvoted 2 times
 smarty_arse 3 years, 6 months ago
Who uses Bing at this present day and age?
   upvoted 13 times
 RNSS 3 years, 2 months ago
believe me it is very good and clean. When I was doing my research I have used both google and bing. and find bing as more trusted and complete answer.
   upvoted 2 times
 mexblood1 5 years, 4 months ago
Using pricing calculator matching 730 hrs per month for both.. Memorystore is 537.28 per month and vm (6 cpus 32 gb memory) is 174.41 per month. So vm is still cheaper even with 6 cpus.
   upvoted 8 times
 FenixRa73 5 years ago
$0.023 * 32 = $0.736
is it cheaper?
   upvoted 4 times

2. Anonymous: ESP_SAP Highly Voted  5 years, 5 months ago
Correct Answer should be A:
The question mention "You want to have a 30-GB in-memory cache, and
need an additional 2 GB of memory for the rest of the processes"
What is Google Cloud Memorystore?
Overview. Cloud Memorystore for Redis is a fully managed Redis service for Google Cloud Platform. Applications running on Google Cloud Platform can achieve extreme performance by leveraging the highly scalable, highly available, and secure Redis service without the burden of managing complex Redis deployments.
   upvoted 35 times
 ESP_SAP 5 years, 4 months ago
Just to complement the answer:
We are looking for "latency-sensitive website"
What it's good for
Memorystore for Redis provides a fast, in-memory store for use cases that require fast, real-time processing of data. From simple caching use cases to real time analytics, Memorystore for Redis provides the performance you need.
Caching: Cache is an integral part of modern application architectures. Memorystore for Redis provides low latency access and high throughput for heavily accessed data, compared to accessing the data from a disk based backend store. Session management, frequently accessed queries, scripts, and pages are common examples of caching.
https://cloud.google.com/memorystore/docs/redis/redis-overview#what_its_good_for
   upvoted 21 times

3. Anonymous: vaclavbenes1 Most Recent  11 months, 2 weeks ago
Selected Answer: B
Both Gemini and Claude.ai wotes for B. You need to run a reverse proxy server in this can you can eliminate A.
   upvoted 1 times

4. Anonymous: Sekar_1992 1 year ago
Selected Answer: B
You need 30 GB of in-memory cache plus an additional 2 GB for other processes.
The solution should minimize cost while fulfilling the memory requirements.
Option Analysis:
A. Create a Cloud Memorystore for Redis instance with 32-GB capacity:
Incorrect: While Memorystore is designed for caching, it does not meet the requirement to host an HTTP reverse proxy. Memorystore is only a caching layer, not a compute platform for running processes.
B. Run it on Compute Engine with a custom instance type:
Correct: Compute Engine allows you to specify a custom machine type tailored to your exact resource requirements (e.g., 6 vCPUs and 32 GB of memory). This minimizes cost compared to using a predefined machine type.
The reverse proxy can run directly on the Compute Engine instance, and the memory requirements are fulfilled.
   upvoted 3 times

5. Anonymous: mnasruul 1 year, 1 month ago
Selected Answer: B
Im choice B because the question is `This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes`, they need additional 2 GB of memory for the rest of the processes and maybe cannot running at Redis.
   upvoted 2 times

6. Anonymous: Noni_11 1 year, 1 month ago
Selected Answer: D
Las razones por las que cada opción es o no adecuada:
A: INCORRECTA. Cloud Memorystore es un servicio de caché administrado, no puede ejecutar un proxy HTTP.
B: INCORRECTA. 6 vCPUs es excesivo ya que se especifica que consume casi nada de CPU. Sería un desperdicio de recursos y dinero.
C: INCORRECTA. GKE sería una sobrecarga innecesaria para una única instancia y n1-standard-32 es extremadamente sobredimensionado.
D: CORRECTA. Una n1-standard-1 (1 vCPU, 3.75GB RAM) con un disco SSD de 32GB es la opción más económica que cumple los requisitos mínimos necesarios para el proxy.
La D es la correcta porque es la opción más económica que proporciona los recursos necesarios. Un SSD persistente puede usarse para swap si se necesita más memoria, aunque no será tan rápido como la memoria RAM.
   upvoted 1 times

7. Anonymous: user263263 1 year, 1 month ago
Selected Answer: B
A. Redis isn't used as storage for caching proxies - it is a kind of key-value store.
B. fulfills the requirements, e.g. take nginx use a RAM disk for caching
C. wrong machine type (32 vCPU), don't need k8s
D. technically possible, but does not fulfill "in memory" requirement
   upvoted 1 times

8. Anonymous: nubelukita45852 1 year, 4 months ago
Selected Answer: D
La n1-standard-1 es una instancia de bajo costo con 1 vCPU y 3.75 GB de memoria, suficiente para los procesos adicionales del proxy. Dado que el proxy inverso prácticamente no consume CPU, no es necesario optar por una instancia más grande. El disco persistente SSD de 32 GB puede actuar como almacenamiento para la caché en lugar de usar costosas soluciones en memoria, lo que ayuda a minimizar costos, mientras proporciona un almacenamiento rápido, suficiente para el sitio sensible a la latencia.
   upvoted 2 times

9. Anonymous: spatters 1 year, 5 months ago
A might be a fine answer, except that Redis is not an http reverse proxy. It is a data cache. So A, regardless of the cost, does not work for this use case.
   upvoted 1 times

10. Anonymous: subha.elumalai 1 year, 8 months ago
Correct Answer:B
   upvoted 1 times
==============================

==============================
Page X — Question #63

Pergunta:
You are hosting an application on bare-metal servers in your own data center. The application needs access to Cloud Storage. However, security policies prevent the servers hosting the application from having public IP addresses or access to the internet. You want to follow Google-recommended practices to provide the application with access to Cloud Storage. What should you do?

Alternativas:
- A. 1. Use nslookup to get the IP address for storage.googleapis.com. 2. Negotiate with the security team to be able to give a public IP address to the servers. 3. Only allow egress traffic from those servers to the IP addresses for storage.googleapis.com.
- B. 1. Using Cloud VPN, create a VPN tunnel to a Virtual Private Cloud (VPC) in Google Cloud. 2. In this VPC, create a Compute Engine instance and install the Squid proxy server on this instance. 3. Configure your servers to use that instance as a proxy to access Cloud Storage.
- C. 1. Use Migrate for Compute Engine (formerly known as Velostrata) to migrate those servers to Compute Engine. 2. Create an internal load balancer (ILB) that uses storage.googleapis.com as backend. 3. Configure your new instances to use this ILB as proxy.
- D. 1. Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com.

Resposta correta:
D. 1. Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com.

Top 10 Discussões (sem replies):
1. Anonymous: poogcp Highly Voted  5 years, 1 month ago
D is the correct one as per Ref: https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid
   upvoted 55 times

2. Anonymous: obeythefist Highly Voted  3 years, 4 months ago
What messy answers! I chose D and here is my reasoning per answer.
A. It's bad practice to use nslookup to try find a permanent IP address because IPs can change. That's what DNS is for! Also, the security team aren't going to budge... this is just a silly answer.
B. We're getting warmer. Any time a question mentions on-prem and cloud, Google wants you to think about Cloud VPN. This solution might even work, but installing Squid? This is a messy solution to a more simple problem.
C. Talk about using a sledge hammer to swat a mosquito. I think this could work, but migrating servers to cloud to solve a simple networking problem?
D. Once more Google's favorite Cloud VPN is in the answer. I'm not sure about the networking component of this question.
   upvoted 27 times
 obeythefist 3 years, 4 months ago
Edit: Of course the reason D: is correct is because 199.36.153.4/30 is the network segment that you can direct traffic to if you want to use Google services "internally". So your on prem servers will resolve storage.googleapis.com to something in this 199.36.153.4/30 range. Then they will route using Cloud Router and your VPN tunnel into Google Cloud privately.
   upvoted 15 times

3. Anonymous: Cloudmoh Most Recent  11 months, 1 week ago
Selected Answer: D
Well D is the right option : . Using Cloud VPN or Interconnect, create a tunnel to a VPC in Google Cloud. 2. Use Cloud Router to create a custom route advertisement for 199.36.153.4/30. Announce that network to your on-premises network through the VPN tunnel. 3. In your on-premises network, configure your DNS server to resolve *.googleapis.com as a CNAME to restricted.googleapis.com
   upvoted 1 times

4. Anonymous: subha.elumalai 1 year, 1 month ago
C is the correct Answer
   upvoted 1 times

5. Anonymous: BAofBK 1 year, 8 months ago
The correct answer is D
   upvoted 2 times

6. Anonymous: gsmasad 1 year, 8 months ago
Selected Answer: D
D as per google recommened pratcises
   upvoted 1 times

7. Anonymous: Captain1212 1 year, 10 months ago
Selected Answer: D
D as per google recommened pratcises
   upvoted 1 times

8. Anonymous: Buruguduystunstugudunstuy 2 years, 5 months ago
Selected Answer: D
ANSWER D is the recommended solution because it provides a secure and direct connection to Cloud Storage without requiring internet access or exposing the servers to public IP addresses.
* By setting up a VPN or Interconnect tunnel, the on-premises servers can access Google
Cloud resources over a private and encrypted connection.
* The custom route advertisement for 199.36.153.4/30 ensures that traffic is routed
correctly between the on-premises network and Google Cloud.
* Configuring the DNS server to resolve *.googleapis.com as a CNAME to
restricted.googleapis.com ensures that requests are directed to Google Cloud over the
VPN or Interconnect tunnel.
   upvoted 9 times

9. Anonymous: warrior9000 2 years, 6 months ago
D but anyone wanna try to explain how the hell you can have a VPN connection without accessing the public internet? The only option for D should be using Interconnect for a direct private wire from your data center to GCP. VPN doesn't make any sense.
   upvoted 6 times
 ast3citos 2 years, 4 months ago
The machine hosting the application cannot access directly the public internet. So to go to Google Cloud it must go through a VPN.
   upvoted 1 times
 xaqanik 2 years, 6 months ago
https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid
   upvoted 3 times

10. Anonymous: cslince 2 years, 7 months ago
Selected Answer: D
D is the correct
   upvoted 1 times
==============================

==============================
Page X — Question #64

Pergunta:
You want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic. You want to follow Google-recommended practices. What should you do?

Alternativas:
- A. 1. Create a Cloud Function that uses a Cloud Pub/Sub trigger on that topic. 2. Call your application on Cloud Run from the Cloud Function for every message.
- B. 1. Grant the Pub/Sub Subscriber role to the service account used by Cloud Run. 2. Create a Cloud Pub/Sub subscription for that topic. 3. Make your application pull messages from that subscription.
- C. 1. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.
- D. 1. Deploy your application on Cloud Run on GKE with the connectivity set to Internal. 2. Create a Cloud Pub/Sub subscription for that topic. 3. In the same Google Kubernetes Engine cluster as your application, deploy a container that takes the messages and sends them to your application.

Resposta correta:
C. 1. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.

Top 10 Discussões (sem replies):
1. Anonymous: Meix Highly Voted  5 years, 7 months ago
C looks right for me as per https://cloud.google.com/run/docs/tutorials/pubsub#integrating-pubsub
   upvoted 58 times
 ChrisBelt5 4 years, 5 months ago
great doc, its' C
   upvoted 7 times

2. Anonymous: Bhagirathi Highly Voted  5 years, 1 month ago
why c ?
explained>>
You can use Pub/Sub to push messages to the endpoint of your Cloud Run service, where the messages are subsequently delivered to containers as HTTP requests. You cannot use Pub/Sub pull subscriptions because Cloud Run only allocates CPU during the processing of a request.
   upvoted 42 times

3. Anonymous: user263263 Most Recent  1 year, 1 month ago
Selected Answer: C
C follows the official description in "Use Pub/Sub with Cloud Run tutorial"
B. you cannot pull (that means wait for messages) in Cloud Run code. It only runs in response to a request and is suspended less than a second later
   upvoted 2 times

4. Anonymous: xylene314 1 year, 1 month ago
Selected Answer: C
C is the right answer
   upvoted 1 times

5. Anonymous: varshitag 1 year, 3 months ago
C is right Answer
   upvoted 1 times

6. Anonymous: subha.elumalai 1 year, 8 months ago
Correct answer is D
   upvoted 1 times

7. Anonymous: DWT33004 1 year, 9 months ago
Selected Answer: B
Here's why Option B might be preferred over Option C:
Service Account Permissions: Option C involves creating a separate service account and granting it the Cloud Run Invoker role.
Direct Subscription Configuration: Option B allows for a direct configuration of a Cloud Pub/Sub subscription to pull messages from the topic eliminating the need to manage service accounts and roles separately.
Standard Practice: Granting the Pub/Sub Subscriber role directly to the service account used by Cloud Run is a standard practice for allowing Cloud Run services to access Pub/Sub topics follows the principle of least privilege by granting only the necessary permissions to the service account.
Push vs. Pull Model: Option C uses a push model where Cloud Pub/Sub sends messages directly to the Cloud Run service. While this model can work, it requires additional setup for configuring the push endpoint
Overall, Option B provides a simpler and more direct approach to integrating Cloud Run with Cloud Pub/Sub, aligning well with Google-recommended practices
   upvoted 4 times

8. Anonymous: thewalker 2 years, 1 month ago
Selected Answer: C
C looks the correct option.
   upvoted 1 times

9. Anonymous: hanweiCN 2 years, 8 months ago
Selected Answer: C
it is explicated recommended use " push" :
Note: Google recommends using push subscriptions to consume messages from a Pub/Sub topic on Cloud Run. Although it is possible to use Pub/Sub pull subscriptions, pull subscriptions require you to monitor message delivery latency and manually scale the number of instances to maintain a healthy delivery latency. If you want to use pull subscriptions, use the CPU always allocated setting along with a number of minimum instances.
https://cloud.google.com/run/docs/triggering/pubsub-push
   upvoted 3 times

10. Anonymous: SMR123 2 years, 9 months ago
what is the actual answer people who are voting or actual answer?
   upvoted 1 times
==============================

==============================
Page X — Question #65

Pergunta:
You need to deploy an application, which is packaged in a container image, in a new project. The application exposes an HTTP endpoint and receives very few requests per day. You want to minimize costs. What should you do?

Alternativas:
- A. Deploy the container on Cloud Run.
- B. Deploy the container on Cloud Run on GKE.
- C. Deploy the container on App Engine Flexible.
- D. Deploy the container on GKE with cluster autoscaling and horizontal pod autoscaling enabled.

Resposta correta:
A. Deploy the container on Cloud Run.

Top 10 Discussões (sem replies):
1. Anonymous: Gurnoor Highly Voted  5 years, 7 months ago
A should be cheapest as no infra needed.
   upvoted 73 times
 spudleymcdudley 5 years, 6 months ago
Listen to this guy. Google says "Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously—depending on traffic. Cloud Run only charges you for the exact resources you use."
   upvoted 34 times

2. Anonymous: ESP_SAP Highly Voted  5 years, 5 months ago
Correct Answer should be A:
Cloud Run takes any container images and pairs great with the container ecosystem: Cloud Build, Artifact Registry, Docker. ... No infrastructure to manage: once deployed, Cloud Run manages your services so you can sleep well. Fast autoscaling. Cloud Run automatically scales up or down from zero to N depending on traffic.
https://cloud.google.com/run
   upvoted 28 times

3. Anonymous: nubelukita45852 Most Recent  1 year, 4 months ago
Selected Answer: A
Cloud Run es una plataforma serverless que permite ejecutar contenedores de manera altamente escalable y a un costo muy bajo, ya que solo se paga por las solicitudes recibidas y el tiempo de ejecución. Dado que la aplicación recibe pocas solicitudes por día, Cloud Run es la opción más rentable, ya que no incurre en costos cuando no hay tráfico. Además, es fácil de implementar y mantiene la infraestructura al mínimo, lo que optimiza tanto costos como administración.
   upvoted 1 times

4. Anonymous: Mohammed52 1 year, 7 months ago
Selected Answer: A
A is correct
   upvoted 1 times

5. Anonymous: MUNHU 1 year, 7 months ago
A is the correct answer
   upvoted 1 times

6. Anonymous: subha.elumalai 1 year, 8 months ago
Correct Answer: B
   upvoted 1 times

7. Anonymous: BAofBK 2 years, 2 months ago
The correct answer is A
   upvoted 1 times

8. Anonymous: BAofBK 2 years, 2 months ago
The correct answer is A
   upvoted 1 times

9. Anonymous: Captain1212 2 years, 4 months ago
Selected Answer: A
A, as it does not include the infra services and its cheaper
   upvoted 1 times

10. Anonymous: sthapit 2 years, 5 months ago
Should be A
   upvoted 1 times
==============================

==============================
Page X — Question #66

Pergunta:
Your company has an existing GCP organization with hundreds of projects and a billing account. Your company recently acquired another company that also has hundreds of projects and its own billing account. You would like to consolidate all GCP costs of both GCP organizations onto a single invoice. You would like to consolidate all costs as of tomorrow. What should you do?

Alternativas:
- A. Link the acquired company's projects to your company's billing account.
- B. Configure the acquired company's billing account and your company's billing account to export the billing data into the same BigQuery dataset.
- C. Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.
- D. Create a new GCP organization and a new billing account. Migrate the acquired company's projects and your company's projects into the new GCP organization and link the projects to the new billing account.

Resposta correta:
A. Link the acquired company's projects to your company's billing account.

Top 10 Discussões (sem replies):
1. Anonymous: GunjGupta Highly Voted  5 years, 7 months ago
To me, A looks correct. projects are linked to another organization as well in the acquired company so migrating would need google cloud support. we can not do ourselves. however, we can link other company projects to an existing billing account to generate total cost.
https://medium.com/google-cloud/google-cloud-platform-cross-org-billing-41c5db8fefa6
   upvoted 58 times
 uganeshku 4 years ago
A is correct because linking all projects of the acquired organization to the main organization’s billing account will generate a single bill for all projects.
D is incorrect because there is no need to create a new organization for this.
   upvoted 6 times
 spudleymcdudley 5 years, 6 months ago
Listen to this guy. It's 'A' as moving projects can take some time from Google. There's no need to create a new organisation and other options don't make any sense
   upvoted 11 times
 lxgywil 4 years, 8 months ago
You're saying it as if "moving projects" was a viable option. What about B?
   upvoted 2 times
 ryumada 3 years, 5 months ago
I think B is not make sense. You don't want to do statistical analytic to the billing data. You want to consolidate all the costs as of tomorrow. So, the costs as of tomorrow should be billed in one billing account. That's what I've understand from the question.
   upvoted 3 times

2. Anonymous: XRiddlerX Highly Voted  5 years, 5 months ago
I could be missing something but where does it say in the question that the two orgs want to migrate projects? I believe the question and key points are "consolidate all GCP costs" and "consolidate all costs as of tomorrow". With that said, C and D would not be a 24 hour task and seems a bit cumbersome to perform for something simple as "creating a single invoice" AND that's a migration and not a consolidation of cost. With A, I can't find anywhere in GCP docs that this is a best practice, only a medium.com blog. IMHO, I won't go down this route because "Just because you can do something, doesn't mean you should." and I would consult GCP support for best practices on A before I do something like that.
That leaves B which is to export both detailed billing to BigQuery and create a invoice/report. This would be a temporary solution until you migrate Organizations. IMHO
I go with B.
   upvoted 47 times
 ashrafh 4 years, 5 months ago
I also vote B,
why?
agree with this technical explanation and my finance team not gonna pay some newly acquired company bill by tomorrow :)
   upvoted 5 times
 Armne96X 4 years, 1 month ago
Are you sure you can do all steps by tomorrow?
(You would like to consolidate all costs as of tomorrow)
   upvoted 1 times
 ninjaasmoke 3 years, 1 month ago
What does exporting data to BigQuery have to do with creating an Invoice?
   upvoted 4 times
 TAvenger 4 years, 11 months ago
I am not sure that exporting some statistical data to BigQuery means anything for Google who creates the invoice.
With "A" you are right, that is not the best practice, but the key word "for tomorrow" allows this custom approach.
So the answer is "A"
   upvoted 18 times
 zaxma 3 years, 9 months ago
I will go with A in the exam as well, but just wondering, they are two different organisations, how can you link all projects from org2 to org1's billing account without the help of GCP support??
   upvoted 1 times
 eBooKz 2 years, 12 months ago
Cloud Billing accounts can be used across organization resources. However, organization resource moves often also include a requirement to move to a new billing account. To get the permissions that you need to change the project's billing account, ask your administrator to grant you the following IAM roles:
Billing Account User (roles/billing.user) on the destination billing account
Project billing manager (roles/billing.projectManager) on the project
https://cloud.google.com/resource-manager/docs/project-migration#permissions-billing
   upvoted 1 times
Load full discussion...

3. Anonymous: AdelElagawany Most Recent  3 months, 4 weeks ago
Selected Answer: A
A single invoice is a key word.
IMHO, Single Invoice means Single Billing Account so i will go with A
   upvoted 1 times

4. Anonymous: Enamfrancis 1 year, 3 months ago
Selected Answer: C
I will go for C
   upvoted 1 times

5. Anonymous: DWT33004 1 year, 9 months ago
Selected Answer: A
A. Link the acquired company's projects to your company's billing account.
Explanation:
Billing Account Linking: By linking the acquired company's projects to your company's billing account, you can consolidate all costs onto a single invoice. This allows for centralized billing management and easier tracking of expenses.
Immediate Consolidation: This action can be implemented quickly and efficiently, allowing for cost consolidation as of tomorrow, as specified in the requirement.
Minimal Disruption: Linking projects to a different billing account does not require significant changes to the existing project configurations or organizational structure. It allows both companies to maintain their separate GCP organizations and project structures while consolidating billing.
Cost Tracking: With all costs consolidated onto a single invoice, it becomes easier to track expenses and manage budgets effectively.
   upvoted 1 times

6. Anonymous: abyacharya90 1 year, 11 months ago
C. Migrate the acquired company's projects into your company's GCP organization. Link the migrated projects to your company's billing account.
Here's why:
Option A: Linking projects to a different billing account doesn't consolidate costs onto a single invoice.
Option B: Exporting data to a shared BigQuery dataset allows analysis but doesn't consolidate billing itself.
Option C: This approach achieves your goal efficiently:
Migration: Moving acquired company projects into your organization allows centralized management and cost consolidation.
Linking to existing billing account: Ensures all project costs appear on your existing invoice starting from the day of migration.
Timing: Given the urgency of same-day consolidation, this is the fastest option.
   upvoted 1 times

7. Anonymous: PiperMe 1 year, 11 months ago
This question is outdated: As of October 26, 2023, Google Cloud Platform does not allow directly linking projects between separate organizations to a single billing account. Each organization must have its own billing account, and resource costs cannot be directly consolidated across distinct organizations.
   upvoted 4 times

8. Anonymous: Shriyanka 2 years ago
C should be be correct as per me
   upvoted 1 times

9. Anonymous: BAofBK 2 years, 2 months ago
The correct answer is D
   upvoted 1 times

10. Anonymous: Captain1212 2 years, 4 months ago
Selected Answer: A
A is the correct answer
   upvoted 1 times
==============================

==============================
Page X — Question #67

Pergunta:
You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data.
You need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do?

Alternativas:
- A. Add the support team group to the roles/monitoring.viewer role
- B. Add the support team group to the roles/spanner.databaseUser role.
- C. Add the support team group to the roles/spanner.databaseReader role.
- D. Add the support team group to the roles/stackdriver.accounts.viewer role.

Resposta correta:
A. Add the support team group to the roles/monitoring.viewer role

Top 10 Discussões (sem replies):
1. Anonymous: poogcp Highly Voted  5 years, 7 months ago
its A, As you need to monitor only
   upvoted 47 times
 WindDriver 4 years, 6 months ago
A, right, correct answer.
B and C are incorrect because allow to read data.
D also incorrect: Not for monitoring. roles/stackdriver.accounts.viewer Stackdriver Accounts Viewer:
Read-only access to get and list information about Stackdriver account structure (resourcemanager.projects.get, resourcemanager.projects.list and stackdriver.projects.get)
   upvoted 16 times
 WindDriver 4 years, 6 months ago
https://cloud.google.com/iam/docs/understanding-roles
   upvoted 4 times

2. Anonymous: Gurnoor Highly Voted  5 years, 7 months ago
A is correct as user should not have any access to data, so B and C cant be used in this scenario.
   upvoted 20 times

3. Anonymous: nish2288 Most Recent  1 year, 6 months ago
Its D.
Stackdriver roles in GCP (Google Cloud Platform) are predefined sets of permissions that control access to monitoring and logging data within Stackdriver, a suite of tools for monitoring and logging applications and infrastructure in GCP.
These roles determine what users or groups can see and do within Stackdriver. They allow you to grant granular access levels, ensuring users have the necessary permissions to perform their tasks without exposing sensitive data or granting unnecessary control.
   upvoted 1 times
 peddyua 12 months ago
it does not grant access to Cloud Spanner metadata or any related Spanner-specific monitoring data.
   upvoted 1 times

4. Anonymous: ekta25 2 years, 3 months ago
A. Add the support team group to the roles/monitoring.viewer role
   upvoted 2 times

5. Anonymous: axantroff 2 years, 3 months ago
Selected Answer: A
Makes sense for me
   upvoted 1 times

6. Anonymous: Captain1212 2 years, 4 months ago
Selected Answer: A
A as you only need the monitor access
   upvoted 1 times

7. Anonymous: sakdip66 2 years, 9 months ago
the goal of support team is to MONITOR the environment only. therefore roles/monitoring.viewer role is the best option we have
https://cloud.google.com/spanner/docs/iam#roles
   upvoted 1 times

8. Anonymous: Buruguduystunstugudunstuy 2 years, 11 months ago
Selected Answer: A
Answer A, adding the support team group to the roles/monitoring.viewer role, is the CORRECT answer. This role grants read-only access to monitoring data for all resources in a project, which allows the support team to monitor the environment but not access the table data.
Answer B, adding the support team group to the roles/spanner.databaseUser role, grants read and write access to all tables in the specified database, which is NOT required for the support team to monitor the environment.
Answer C, adding the support team group to the roles/spanner.databaseReader role, grants read-only access to all tables in the specified database, which would give the support team access to the table data.
Answer D, adding the support team group to the roles/stackdriver.accounts.viewer role, grants permissions to view Stackdriver data for all resources in a project, which is NOT directly related to monitoring the Cloud Spanner environment.
   upvoted 8 times

9. Anonymous: cslince 3 years, 1 month ago
Selected Answer: A
A is correct
   upvoted 1 times

10. Anonymous: Zoze 3 years, 2 months ago
Selected Answer: A
A is correct, the team need to monitor the environment not read the data.
   upvoted 1 times
==============================

==============================
Page X — Question #68

Pergunta:
For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Cloud Logging agent on all the instances. You want to minimize cost. What should you do?

Alternativas:
- A. 1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.
- B. 1. In Cloud Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.
- C. 1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.
- D. 1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day.

Resposta correta:
C. 1. In Cloud Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.

Top 10 Discussões (sem replies):
1. Anonymous: sumanshu Highly Voted  4 years, 4 months ago
vote for ''C"
https://cloud.google.com/logging/docs/export/configure_export_v2
   upvoted 23 times

2. Anonymous: vmart Highly Voted  4 years, 1 month ago
I vote for C
   upvoted 8 times

3. Anonymous: Nitesh2000 Most Recent  1 year, 5 months ago
Option C
   upvoted 2 times

4. Anonymous: BAofBK 1 year, 8 months ago
The correct answer is C
   upvoted 1 times

5. Anonymous: axantroff 1 year, 9 months ago
C, it's simple enough
   upvoted 2 times

6. Anonymous: Captain1212 1 year, 10 months ago
Selected Answer: C
C , it minimizes the cost
   upvoted 1 times

7. Anonymous: jayjani66 2 years ago
Answer: C. Option C allows you to create a log export from Cloud Logging to BigQuery with minimal setup and cost. By creating a filter to view only Compute Engine logs, you ensure that only the relevant logs are exported to BigQuery, reducing unnecessary data transfer and storage costs.
   upvoted 4 times

8. Anonymous: Kyle1776 2 years, 1 month ago
Selected Answer: B
B looks like the most cost effective option since filtering out only the logs you need will reduce storage and data transfer costs.
   upvoted 1 times
 Kyle1776 2 years, 1 month ago
Correction - C
   upvoted 1 times

9. Anonymous: Buruguduystunstugudunstuy 2 years, 5 months ago
Selected Answer: C
The most cost-effective and recommended solution to send logs from Compute Engine instances to BigQuery is to use the Cloud Logging agent with a sink that streams the logs to BigQuery.
Answer C is the most appropriate solution. In Cloud Logging, create a filter to view only Compute Engine logs. Click Create Export. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination. This will allow all Compute Engine instance logs to be exported to BigQuery with minimal complexity and cost.
   upvoted 3 times

10. Anonymous: David_Esteban 2 years, 7 months ago
Selected Answer: C
Mi vote is for "c"
   upvoted 1 times
==============================

==============================
Page X — Question #69

Pergunta:
You are using Deployment Manager to create a Google Kubernetes Engine cluster. Using the same Deployment Manager deployment, you also want to create a
DaemonSet in the kube-system namespace of the cluster. You want a solution that uses the fewest possible services. What should you do?

Alternativas:
- A. Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.
- B. Use the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition.
- C. With Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet.
- D. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value.

Resposta correta:
A. Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.

Top 10 Discussões (sem replies):
1. Anonymous: ESP_SAP Highly Voted  4 years, 4 months ago
Correct Answer is (A):
Adding an API as a type provider
This page describes how to add an API to Google Cloud Deployment Manager as a type provider. To learn more about types and type providers, read the Types overview documentation.
A type provider exposes all of the resources of a third-party API to Deployment Manager as base types that you can use in your configurations. These types must be directly served by a RESTful API that supports Create, Read, Update, and Delete (CRUD).
If you want to use an API that is not automatically provided by Google with Deployment Manager, you must add the API as a type provider.
https://cloud.google.com/deployment-manager/docs/configuration/type-providers/creating-type-provider
   upvoted 77 times
 magistrum 4 years ago
very good find, sounds like you hit the nail in the head
   upvoted 7 times

2. Anonymous: PR0704 Highly Voted  3 years, 1 month ago
couldn't be more confusing
   upvoted 11 times

3. Anonymous: skhan Most Recent  6 months, 2 weeks ago
Selected Answer: A
A is Correct
   upvoted 1 times

4. Anonymous: Captain1212 1 year, 4 months ago
Selected Answer: A
A is the correct , bcoz it help you contact directly to the gke cluster to create daemon
   upvoted 3 times

5. Anonymous: sthapit 1 year, 5 months ago
Should have been D
   upvoted 1 times

6. Anonymous: sakdip66 1 year, 9 months ago
Selected Answer: A
option A is the right answer because it lets you directly interact with the Kubernetes API to create the Daemonset using the same deployment Manager Deployment
   upvoted 1 times

7. Anonymous: Buruguduystunstugudunstuy 1 year, 11 months ago
Selected Answer: A
I would say both Answer A and Answer D are valid solutions, and it depends on your preference and requirements.
Answer A involves adding the cluster's API as a new Type Provider in Deployment Manager and using the new type to create the DaemonSet. This solution would allow you to create and manage the DaemonSet and the cluster in the same Deployment Manager deployment.
Answer D involves adding a metadata block to the Deployment Manager deployment of the cluster, which will create the DaemonSet in the kube-system namespace of the cluster. This solution would allow you to create the DaemonSet in a simple way and avoid the need to create a new Type of Provider.
In conclusion, I would choose Answer A to be considered the answer that uses the fewest possible services, as it only involves adding the cluster's API as a new Type Provider in Deployment Manager, which is a lightweight solution.
   upvoted 7 times

8. Anonymous: Bobbybash 1 year, 11 months ago
Selected Answer: D
D. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value.
This approach involves adding the DaemonSet manifest directly as a metadata entry in the cluster's definition in Deployment Manager. When the cluster is created, the DaemonSet is automatically created in the kube-system namespace. This approach is the simplest and requires the fewest number of services. Option A is also a viable solution but requires more work to set up a Type Provider. Option B is not suitable because it involves a separate service (Runtime Configurator). Option C is also not recommended because it involves creating a Compute Engine instance and using kubectl to create the DaemonSet, which is more complicated and less efficient than the other options.
   upvoted 3 times

9. Anonymous: vkamlesh0205 2 years ago
Selected Answer: A
Option A is the right answer
   upvoted 1 times

10. Anonymous: RanjithK 2 years, 6 months ago
Selected Answer: A
Answer is A.
   upvoted 1 times
==============================

==============================
Page X — Question #70

Pergunta:
You are building an application that will run in your data center. The application will use Google Cloud Platform (GCP) services like AutoML. You created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your on-premises environment. What should you do?

Alternativas:
- A. Use service account credentials in your on-premises application.
- B. Use gcloud to create a key file for the service account that has appropriate permissions.
- C. Set up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications.
- D. Go to the IAM & admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center.

Resposta correta:
B. Use gcloud to create a key file for the service account that has appropriate permissions.

Top 10 Discussões (sem replies):
1. Anonymous: ESP_SAP Highly Voted  4 years, 5 months ago
Correct answer should be (B):
To use a service account outside of Google Cloud, such as on other platforms or on-premises, you must first establish the identity of the service account. Public/private key pairs provide a secure way of accomplishing this goal.
https://cloud.google.com/iam/docs/creating-managing-service-account-keys
   upvoted 55 times

2. Anonymous: Buruguduystunstugudunstuy Highly Voted  1 year, 11 months ago
Selected Answer: B
The recommended approach for enabling authentication from an on-premises environment to Google Cloud Platform (GCP) services like AutoML is to use a service account and generate a JSON key file for the service account. This key file can then be used to authenticate and authorize API calls from your on-premises environment to GCP.
Therefore, the correct answer is B. Use gcloud to create a key file for the service account that has appropriate permissions.
   upvoted 7 times

3. Anonymous: 85c887f Most Recent  9 months, 4 weeks ago
Selected Answer: A
Correct answer should be A. "Use service account credentials", so here "credentials" indicate that we will use JSON key file in our on-premises application, and it is a correct way to authenticate from on-premises to APIs. The B option just says how to create this file, and it misses the next step for what to do next to achieve an authentication. In option A and B we will have the same JSON key file, but only option A contains full way to accomplish the task.
   upvoted 3 times

4. Anonymous: thewalker 1 year, 1 month ago
Selected Answer: B
B
As per the documentation: https://cloud.google.com/iam/docs/keys-create-delete#creating
   upvoted 4 times

5. Anonymous: BAofBK 1 year, 2 months ago
The correct answer is B
   upvoted 1 times

6. Anonymous: drinkwater 1 year, 3 months ago
A. Use service account credentials in your on-premises application.
Explanation:
Service accounts are the recommended way to authenticate your application and authorize it to access GCP services.
You can create and use service account credentials to authenticate your application running in your on-premises environment and access GCP services like AutoML.
Option B (using gcloud to create a key file for the service account) is a valid approach to generate credentials for a service account, but using those credentials in your application is essential, which aligns with option A.
Options C and D are not directly related to enabling authentication for on-premises applications using service account credentials. Setting up direct interconnect (option C) is about networking, and granting permissions to a user account (option D) is not the standard approach for authenticating an application running on-premises to GCP services
   upvoted 1 times

7. Anonymous: Captain1212 1 year, 4 months ago
Selected Answer: B
B is the correct answer, as to access the out side the google cloud , you need the key
   upvoted 1 times

8. Anonymous: Bobbybash 1 year, 11 months ago
Selected Answer: A
A. Use service account credentials in your on-premises application.
To enable authentication to GCP services from your on-premises environment, you can use service account credentials in your on-premises application. This involves creating a service account that has appropriate access to the required GCP services, downloading the service account key file, and using the key file to authenticate the API requests in your on-premises application. This is a secure way to authenticate to GCP services as it does not require direct access to your GCP project or credentials from your on-premises environment.
   upvoted 2 times
 Buruguduystunstugudunstuy 1 year, 11 months ago
Cloud Security/Auditor doesn't like Answer "A". Using service account credentials in your on-premises application could be a security risk if the credentials are compromised. If the key file is stolen or leaked, an attacker could use it to access your GCP resources, potentially causing data breaches, service disruptions, or financial losses.
I would select Answer "B". Use gcloud to create a key file for the service account that has appropriate permissions and let Security Auditor stay away from my back. Never-ending "You cannot do this, you cannot do that" on Answer A.
   upvoted 3 times

9. Anonymous: cslince 2 years, 1 month ago
Selected Answer: B
B it is.
   upvoted 1 times

10. Anonymous: mvk2022 2 years, 1 month ago
Selected Answer: B
B it is.
   upvoted 1 times
==============================
