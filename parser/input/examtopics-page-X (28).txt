==============================
Page X — Question #281

Pergunta:
You need to deploy a third-party software application onto a single Compute Engine VM instance. The application requires the highest speed read and write disk access for the internal database. You need to ensure the instance will recover on failure. What should you do?

Alternativas:
- A. Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateful managed instance group.
- B. Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateless managed instance group.
- C. Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateful managed instance group.
- D. Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateless managed instance group.

Resposta correta:
C. Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateful managed instance group.

Top 10 Discussões (sem replies):
1. Anonymous: samyakaparna 1 year, 1 month ago
Selected Answer: C
https://cloud.google.com/compute/docs/disks/hyperdisks
Hyperdisk volumes feature substantially better performance than Persistent Disk. With Hyperdisk, you get dedicated IOPS and throughput with each volume, as compared to Persistent Disk where performance is shared between volumes of the same type.
   upvoted 1 times

2. Anonymous: 26b39bb 1 year, 1 month ago
Selected Answer: C
For performance-critical applications, use Hyperdisk Extreme if Extreme Persistent Disk isn't supported or doesn't provide enough performance. Hyperdisk Extreme disks feature higher maximum IOPS and throughput along with low sub-millisecond latencies, and offer high performance for the most demanding workloads, such as high performance databases.
https://cloud.google.com/compute/docs/disks/hyperdisks#:~:text=Hyperdisk%20Extreme%20disks%20feature%20higher,such%20as%20high%20performance%20databases.
   upvoted 1 times

3. Anonymous: Moin23 1 year, 1 month ago
Selected Answer: C
Hyperdisk Extreme disks feature higher maximum IOPS and throughput along with low sub-millisecond latencies, and offer high performance for the most demanding workloads, such as high performance databases.
   upvoted 1 times

4. Anonymous: Ciupaz 1 year, 1 month ago
Selected Answer: A
Hyperdisk Extreme does not exists in GCP.
   upvoted 1 times
 halifax 1 year ago
What is Hyperdisk Extreme?
Hyperdisk Extreme is a high-performance block storage service that's part of Google Cloud's Compute Engine platform. It's designed for workloads that require high performance, such as real-time analytics and machine learning model training.
   upvoted 1 times
==============================

==============================
Page X — Question #282

Pergunta:
You have a VM instance running in a VPC with single-stack subnets. You need to ensure that the VM instance has a fixed IP address so that other services hosted in the same VPC can communicate with the VM. You want to follow Google-recommended practices while minimizing cost. What should you do?

Alternativas:
- A. Promote the existing IP address of the VM to become a static external IP address.
- B. Promote the existing IP address of the VM to become a static internal IP address.
- C. Reserve a new static external IPv6 address and assign the new IP address to the VM.
- D. Reserve a new static external IP address and assign the new IP address to the VM.

Resposta correta:
B. Promote the existing IP address of the VM to become a static internal IP address.

Top 10 Discussões (sem replies):
1. Anonymous: sahuprashant123 1 year, 1 month ago
Selected Answer: B
Fixed IP Address for Communication Within the Same VPC:
Since the services need to communicate within the same VPC and you want a fixed IP address, the best practice is to use a static internal IP address. This ensures that the VM will always have the same internal IP, which can be used for communication between services within the same VPC.
To minimize cost while ensuring that the VM instance has a fixed IP for internal communication within the VPC, promote the existing internal IP address to be static. This follows Google-recommended practices and ensures cost-effectiveness.
   upvoted 2 times

2. Anonymous: 26b39bb 1 year, 1 month ago
Selected Answer: B
A. Promote the existing IP address of the VM to become a static external IP address.-> Incorrect you do not need an external IP because all the devices are within the same natework.
B. Promote the existing IP address of the VM to become a static internal IP address.-> Correct Internal Static IP allows communication with all the devices within the same VPC while maintaining the same IP.
C. Reserve a new static external IPv6 address and assign the new IP address to the VM.-> Incorrect. You do not need to define IPv6 IP
D. Reserve a new static external IP address and assign the new IP address to the VM -> Incorrect you do not need an external IP
   upvoted 2 times

3. Anonymous: Ciupaz 1 year, 1 month ago
Selected Answer: B
B is the correct answer.
   upvoted 2 times
==============================

==============================
Page X — Question #283

Pergunta:
Your preview application, deployed on a single-zone Google Kubernetes Engine (GKE) cluster in us-central1, has gained popularity. You are now ready to make the application generally available. You need to deploy the application to production while ensuring high availability and resilience. You also want to follow Google-recommended practices. What should you do?

Alternativas:
- A. Use the gcloud container clusters create command with the options --enable-multi-networking and --enable-autoscaling to create an autoscaling zonal cluster and deploy the application to it.
- B. Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it.
- C. Use the gcloud container clusters update command with the option --region us-central1 to update the cluster and deploy the application to it.
- D. Use the gcloud container clusters update command with the option --node-locations us-central1-a,us-central1-b to update the cluster and deploy the application to the nodes.

Resposta correta:
B. Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it.

Top 10 Discussões (sem replies):
1. Anonymous: SkyZeroZx Highly Voted  1 year ago
Selected Answer: B
If you got this far, you have the will.
Good luck on your exam
   upvoted 9 times

2. Anonymous: dead1407 Most Recent  4 months, 2 weeks ago
Selected Answer: D
For high availability and resilience, Google-recommended practice is to run your GKE cluster across multiple zones within a region. Using --node-locations allows you to specify multiple zones (e.g., us-central1-a,us-central1-b), ensuring your application remains available even if one zone fails.
   upvoted 1 times

3. Anonymous: sahuprashant123 1 year, 1 month ago
Selected Answer: D
Why B is not correct -
B. Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it: Autopilot clusters manage infrastructure automatically, but it does not specifically address the need for multi-zone deployment. Autopilot clusters are typically easier to manage but may not provide as much control over specific zone selection.
   upvoted 1 times
 26b39bb 1 year, 1 month ago
Still the autopilot will provide at least regional scalability by default. With option B you only covering two regions which are not even all the zones in that region
   upvoted 1 times

4. Anonymous: 26b39bb 1 year, 1 month ago
Selected Answer: B
Autopilot cluster will ensure global scalability: https://cloud.google.com/kubernetes-engine/docs/concepts/configuration-overview
create-auto: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster
   upvoted 2 times

5. Anonymous: Ciupaz 1 year, 1 month ago
Selected Answer: B
B is the correct answer.
   upvoted 1 times
==============================

==============================
Page X — Question #284

Pergunta:
You are developing an application that will be deployed on Google Cloud. The application will use a service account to retrieve data from BigQuery. Before you deploy your application, you want to test the permissions of this service account from your local machine to ensure there will be no authentication issues. You want to ensure that you use the most secure method while following Google-recommended practices. What should you do?

Alternativas:
- A. Generate a service account key, and configure the gcloud CLI to use this key. Issue a relevant BigQuery request through the gdoud CLI to test the access.
- B. Grant the service account the BigQuery Administrator IAM role to ensure the service account has all required access.
- C. Configure the gcloud CLI to use service account impersonation. Issue a relevant BigQuery request through the gcloud CLI to test the access.
- D. Configure the gcloud CLI with Application Default Credentials using your user account. Issue a relevant BigQuery request through the gcloud CLI to test the access.

Resposta correta:
C. Configure the gcloud CLI to use service account impersonation. Issue a relevant BigQuery request through the gcloud CLI to test the access.

Top 10 Discussões (sem replies):
1. Anonymous: AdelElagawany 3 months ago
Selected Answer: C
1. Get the key of the SA: gcloud iam service-accounts keys create <type-any-name-for-the-key> --iam-account=<Your SA>
2. Impersonate the SA: gcloud auth activate-service-account <Your SA> --key-file <Your SA Key File Name>
3. Run a query on the SA behalf: bq query --use_legacy_sql=false 'SELECT * FROM mytable limit1;'
   upvoted 1 times

2. Anonymous: AdelElagawany 3 months ago
Selected Answer: A
1. Get the key of the SA: gcloud iam service-accounts keys create <type-any-name-for-the-key> --iam-account=<Your SA>
2. Impersonate the SA: gcloud auth activate-service-account <Your SA> --key-file <Your SA Key File Name>
3. Run a query on the SA behalf: bq query --use_legacy_sql=false 'SELECT * FROM mytable limit1;'
   upvoted 1 times

3. Anonymous: Mika_Ro 10 months, 3 weeks ago
Selected Answer: C
C is the best way to securely test the permissions of the service account from your local machine
   upvoted 1 times

4. Anonymous: Esteban08 10 months, 3 weeks ago
Selected Answer: C
Google-recommended practices advise against distributing or storing long-lived service account keys because they can be a security risk. Instead, service account impersonation allows you to use your own credentials to "borrow" the identity of the service account without needing to download a key file. This method is more secure.
   upvoted 2 times
==============================

==============================
Page X — Question #285

Pergunta:
Your organization is migrating to Google Cloud. You want only users with company-issued Google accounts to access your Google Cloud environment. You must ensure that users of the same department can only access resources within their own department. You want to minimize operational costs while following Google-recommended practices. What should you do?

Alternativas:
- A. Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Periodically identify and remove non-company issued Google accounts.
- B. Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Use organization policies to block non-company issued emails.
- C. Create a folder for each department in Resource Manager. Grant the users of each department the Folder Admin role on the folder of their department.
- D. Create a folder for each department in Resource Manager. Grant all company users the Folder Admin role on the organization level.

Resposta correta:
B. Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Use organization policies to block non-company issued emails.

Top 10 Discussões (sem replies):
1. Anonymous: psou7 2 weeks, 1 day ago
Selected Answer: B
B is definetely the right answer.
   upvoted 1 times

2. Anonymous: SajadAhm 3 weeks ago
Selected Answer: B
Google groups and the use of company policies are aligned with best practices in google and prevent manual overhead
   upvoted 1 times

3. Anonymous: MohannadSamir 8 months ago
Selected Answer: B
Why B is the best answer:
a) Using Google Groups and IAM roles:
Google Groups provides efficient management of user permissions
IAM roles allow granular control over resource access
This is a Google-recommended practice for managing permissions at scale
b) Organization policies to block non-company emails:
Organization policies provide a centralized way to enforce security controls
Can specifically block non-company email domains
Prevents unauthorized access attempts at the organization level
Automated enforcement reduces operational overhead
   upvoted 2 times
==============================

==============================
Page X — Question #286

Pergunta:
You are deploying an application to Cloud Run. Your application requires the use of an API that runs on Google Kubernetes Engine (GKE). You need to ensure that your Cloud Run service can privately reach the API on GKE, and you want to follow Google-recommended practices. What should you do?

Alternativas:
- A. Deploy an ingress resource on the GKE cluster to expose the API to the internet. Use Cloud Armor to filter for IP addresses that can connect to the API. On the Cloud Run service, configure the application to fetch its public IP address and update the Cloud Armor policy on startup to allow this IP address to call the API on ports 80 and 443.
- B. Create an ingress firewall rule on the VPC to allow connections from 0.0.0.0/0 on ports 80 and 443.
- C. Create an egress firewall rule on the VPC to allow connections to 0.0.0.0/ on ports 80 and 443.
- D. Deploy an internal Application Load Balancer to expose the API on GKE to the VPC. Configure Cloud DNS with the IP address of the internal Application Load Balancer. Deploy a Serverless VPC Access connector to allow the Cloud Run service to call the API through the FQDN on Cloud DNS.

Resposta correta:
D. Deploy an internal Application Load Balancer to expose the API on GKE to the VPC. Configure Cloud DNS with the IP address of the internal Application Load Balancer. Deploy a Serverless VPC Access connector to allow the Cloud Run service to call the API through the FQDN on Cloud DNS.

Top 10 Discussões (sem replies):
1. Anonymous: psou7 2 weeks, 1 day ago
Selected Answer: D
Cloud Run is a serverless product and does not live "inside" your VPC by default. A Serverless VPC Access connector (or the more modern Direct VPC Egress) is the required component that allows Cloud Run to send outbound (egress) traffic into your VPC network.
   upvoted 1 times

2. Anonymous: SajadAhm 3 weeks ago
Selected Answer: D
If you want to grant permissions to a Cloud Run service to access resources inside a VPC, always use "Serverless VPC Access connector"
   upvoted 1 times

3. Anonymous: MohannadSamir 8 months ago
Selected Answer: D
why this D is the best solution:
Security Best Practices:
This solution maintains private connectivity without exposing the API to the internet
It follows the principle of least privilege
It uses internal networking rather than public IP addresses
Components and their roles:
Internal Application Load Balancer: Exposes the GKE API internally within the VPC
Cloud DNS: Provides DNS resolution for the internal service
Serverless VPC Access connector: Enables Cloud Run to access VPC resources
   upvoted 2 times
==============================

==============================
Page X — Question #287

Pergunta:
Your company uses a multi-cloud strategy that includes Google Cloud. You want to centralize application logs in a third-party software-as-a-service (SaaS) tool from all environments. You need to integrate logs originating from Cloud Logging, and you want to ensure the export occurs with the least amount of delay possible. What should you do?

Alternativas:
- A. Create a Cloud Logging sink and configure BigQuery as the destination. Configure the SaaS tool to query BigQuery to retrieve the logs.
- B. Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs.
- C. Create a Cloud Logging sink and configure Cloud Storage as the destination. Configure the SaaS tool to read the Cloud Storage bucket to retrieve the logs.
- D. Use a Cloud Scheduler cron job to trigger a Cloud Function that queries Cloud Logging and sends the logs to the SaaS tool.

Resposta correta:
B. Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs.

Top 10 Discussões (sem replies):
1. Anonymous: psou7 2 weeks, 1 day ago
Selected Answer: B
SaaS tool can subscribe to the Pub/Sub topic and receive logs as they arrive
   upvoted 1 times

2. Anonymous: MohannadSamir 8 months ago
Selected Answer: B
The correct answer is B: Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs.
Here's why:
Requirement for minimal delay:
Pub/Sub provides near real-time messaging capabilities
Messages are delivered as soon as they are published
Supports push and pull subscription models
Integration capabilities:
Pub/Sub is designed for event-driven architectures and real-time data streaming
Many SaaS tools have built-in support for Pub/Sub integration
Provides reliable message delivery with at-least-once semantics
   upvoted 3 times
==============================

==============================
Page X — Question #288

Pergunta:
You are planning to migrate a database and a backend application to a Standard Google Kubernetes Engine (GKE) cluster. You need to prevent data loss and make sure there are enough nodes available for your backend application based on the demands of your workloads. You want to follow Google-recommended practices and minimize the amount of manual work required. What should you do?

Alternativas:
- A. Run your database as a StatefulSet. Configure cluster autoscaling to handle changes in the demands of your workloads.
- B. Run your database as a single Pod. Run the resize command when you notice changes in the demands of your workloads.
- C. Run your database as a DaemonSet. Run the resize command when you notice changes in the demands of your workloads.
- D. Run your database as a Deployment. Configure cluster autoscaling to handle changes in the demands of your workloads.

Resposta correta:
A. Run your database as a StatefulSet. Configure cluster autoscaling to handle changes in the demands of your workloads.

Top 10 Discussões (sem replies):
1. Anonymous: psou7 2 weeks, 1 day ago
Selected Answer: A
Databases are stateful -> StatefulSet
   upvoted 1 times

2. Anonymous: MohannadSamir 8 months ago
Selected Answer: A
Prevent data loss --> StatefulSet
   upvoted 3 times
==============================

==============================
Page X — Question #289

Pergunta:
You are the Organization Administrator for your company's Google Cloud resources. Your company has strict compliance rules that require you to be notified about any modifications to files and documents hosted on Cloud Storage. In a recent incident, one of your team members was able to modify files and you did not receive any notifications, causing other production jobs to fail. You must ensure that you receive notifications for all changes to files and documents in Cloud Storage while minimizing management overhead. What should you do?

Alternativas:
- A. View Cloud Audit logs for all Cloud Storage files in Logs Explorer. Filter by Admin Activity logs.
- B. Enable Cloud Storage object versioning on your bucket. Configure Pub/Sub notifications for your Cloud Storage buckets.
- C. Enable versioning on the Cloud Storage bucket. Set up a custom script that scans versions of Cloud Storage objects being modified and alert the admin by using the script.
- D. Configure Object change notifications on the Cloud Storage buckets. Send the events to Pub/Sub.

Resposta correta:
B. Enable Cloud Storage object versioning on your bucket. Configure Pub/Sub notifications for your Cloud Storage buckets.

Top 10 Discussões (sem replies):
1. Anonymous: helloitsme123 1 month ago
Selected Answer: D
https://docs.cloud.google.com/storage/docs/pubsub-notifications
   upvoted 1 times

2. Anonymous: guaose 2 months, 1 week ago
Selected Answer: D
Object change notifications allow you to receive real-time alerts when objects in a Cloud Storage bucket are created, updated, or deleted.
By sending these events to Pub/Sub, you can easily integrate with alerting systems, logging platforms, or custom workflows.
This approach is automated, scalable, and minimizes manual overhead, aligning with Google Cloud's best practices for monitoring and compliance.
   upvoted 3 times

3. Anonymous: PythonPL 8 months ago
Selected Answer: B
Option B is recommended by Google.
   upvoted 1 times

4. Anonymous: 85c887f 9 months, 3 weeks ago
Selected Answer: D
Options B and D both will work. Probably option D will be correct answer here in terms of just notifications without additional cost on versioning.
   upvoted 1 times

5. Anonymous: Mika_Ro 10 months, 3 weeks ago
Selected Answer: B
Object change notification is an older method and deprecated in favor of Pub/Sub notifications
   upvoted 2 times

6. Anonymous: Ahamza 11 months ago
Selected Answer: B
Option B follows Google-recommended best practices to ensure you receive notifications for all modifications to files and documents in Cloud Storage while minimizing management overhead:
Enable Cloud Storage Object Versioning → Ensures that previous versions of objects are retained, preventing data loss in case of accidental modifications.
Configure Pub/Sub Notifications for Cloud Storage → Enables real-time alerts whenever a file is modified, deleted, or created.
Minimizes manual effort → Pub/Sub automates notifications without requiring manual log checking or custom scripts.
This setup ensures you are notified immediately when files are changed while also keeping old versions for compliance and recovery.
   upvoted 2 times
==============================

==============================
Page X — Question #290

Pergunta:
Your company would like to store invoices and other financial documents in Google Cloud. You need to identify a Google-managed solution to store this information for your company. You must ensure that the documents are kept for a duration of three years. Your company’s analysts need frequent access to invoices from the past six months. After six months, invoices should be archived for audit purposes only. You want to minimize costs and follow Google-recommended practices. What should you do?

Alternativas:
- A. Use Cloud Storage with Object Lifecycle Management to change the object storage class to Coldline after six months.
- B. Use Cloud Storage with Object Lifecycle Management to change the object storage class to Standard after six months.
- C. Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Coldline after six months.
- D. Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Standard after six months.

Resposta correta:
A. Use Cloud Storage with Object Lifecycle Management to change the object storage class to Coldline after six months.

Top 10 Discussões (sem replies):
1. Anonymous: psou7 2 weeks, 1 day ago
Selected Answer: A
Cloud Storage is fully managed and the standard recommendation for document/object storage. Achival after 6 months then Coldline or Archive
   upvoted 1 times

2. Anonymous: MohannadSamir 8 months ago
Selected Answer: A
Cost optimization --> Cloud Storage
Archiving after six month --> Coldline
   upvoted 2 times
==============================
